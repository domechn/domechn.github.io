<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Not Bad</title>
  
  <subtitle>独立思考，坚持写作</subtitle>
  <link href="https://domc.me/atom.xml" rel="self"/>
  
  <link href="https://domc.me/"/>
  <updated>2025-04-11T02:56:57.521Z</updated>
  <id>https://domc.me/</id>
  
  <author>
    <name>Dom Chan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ArgoCD Monorepo 性能优化之路（二）</title>
    <link href="https://domc.me/2024/09/02/argocd_mono_repo_performance_optimization_second/"/>
    <id>https://domc.me/2024/09/02/argocd_mono_repo_performance_optimization_second/</id>
    <published>2024-09-02T12:50:24.000Z</published>
    <updated>2025-04-11T02:56:57.521Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文中 ArgoCD 为 2.8.x 版本。</p></blockquote><p>在 <a href="/2024/08/03/argocd_mono_repo_performance_optimization/">argocd-monorepo性能优化之路</a> 中，我们介绍了 ArgoCD Monorepo 性能优化的第一部分，主要是从优化 Git Repo 以及关闭 Application Auto Sync 改成通过 Github Action Workflow 触发应用 sync，从而降低 ArgoCD 负载。</p><p>虽然经过上面的优化后，argoCD 能在日常使用时，99% 的情况下不出现性能问题，然而还是存在一些极端场景下的性能问题，导致 sync 时耗时过长（比如正常情况下 sync app 耗时 25s (p90)，但是有时候会耗时 3min(p99)）。</p><p>本篇将继续介绍其他优化策略，优化 ArgoCD sync 的速度和稳定性和降低 sync 时使用的资源，从而增强 ArgoCD 并发 sync 多个 Application 的能力。</p><p>以下是导致 argoCD sync 突然很慢的原因以及优化的策略：</p><h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><ol><li>Sync App 时，argoCD application-controller 中 k8s cluster 缓存已经过期，argoCD 需要先刷新 cluster 缓存，然后再 sync app，如果 cluster 中 resources 数量过多，会导致 cluster 缓存刷新时间过长，从而导致 sync app 耗时过长。</li><li>如果 Application 使用了 sidecar plugin，那么在 sync app 时，argoCD repo-server 会把整个 repo 压缩成 tarball，然后传输给 sidecar plugin，如果 repo 过大，会导致压缩和解压缩耗时过长，以及在这过程中会占用大量的 CPU 和内存资源，导致 argoCD repo-server 没有足够的资源同时处理多个 sync 请求。</li><li>如果 repo server 长时间没有接收到请求，它内部的 git 代码可能落后太久，导致接受到 sync 请求时，会花费较长的时间 git fetch 代码（这取决于 repo 更新的频繁程度，如果是 monorepo 可能一天会有大量的 git commit），增加 argoCD sync 的耗时。</li></ol><h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><h3 id="1-优化-argoCD-application-controller-中-cluster-的缓存策略"><a href="#1-优化-argoCD-application-controller-中-cluster-的缓存策略" class="headerlink" title="1. 优化 argoCD application-controller 中 cluster 的缓存策略"></a>1. 优化 argoCD application-controller 中 cluster 的缓存策略</h3><p>argoCD application-controller 提供了环境变量 <code>ARGOCD_CLUSTER_CACHE_RESYNC_DURATION</code> 来控制 cluster 缓存的过期时间，默认是 12h 将所有 cluster 的缓存都过期，然后直到接受到新的 sync 或者 refresh 请求，再刷新这个 application 所在的 cluster 的缓存（会拉取 k8s cluster 中所有的资源，以及集群的 metadata 等）。同时这个过程是同步的，也就是说在 cluster 缓存刷新过程中，所有接受到的 app sync 请求，都会一直等，直到 cluster 缓存刷新完成。</p><blockquote><p>如果集群中资源过多，比如集群中有 200k 个资源，那么刷新 cluster 缓存可能会耗时 3～5min，</p></blockquote><p>如果集群中资源过多，可以适当调大 <code>ARGOCD_CLUSTER_CACHE_RESYNC_DURATION</code> 的值，减少 cluster 缓存刷新的频率（或者将它设置成 0，不过期 cluster 缓存），从而减少 sync app 时等待 cluster 缓存刷新的情况。</p><h3 id="2-优化-argoCD-repo-server-中-sidecar-plugin-的压缩策略"><a href="#2-优化-argoCD-repo-server-中-sidecar-plugin-的压缩策略" class="headerlink" title="2. 优化 argoCD repo-server 中 sidecar plugin 的压缩策略"></a>2. 优化 argoCD repo-server 中 sidecar plugin 的压缩策略</h3><p>这其实是影响 argoCD 性能的主要原因。如果 repo-server 数量不多，导致多个 sync 请求打到同一个 repo-server pod 上，pod 的 CPU 很快就会被占满（这取决于 git repo 大小，如果 repo 本身并不大，这个问题其实也并不明显。如之前的文章中提到的，目前我们 repo 大小为 150Mi，每次 tarball 需要 3s，在处理单个请求时，cpu 都会被占满（4vCPU））。</p><p>当然，这可以通过增加 repo-server 数量来缓解，但是仍然有可能出现负载不均衡的情况。而且增加 repo-server 数量，也无发解决业务侧批量 sync 整个集群所有服务的情况。sync 的 app 数量过多时，所有 repo-server pod 必然会被占满，导致后续的 sync 请求被阻塞。</p><p>然而不幸的是，argoCD 并没有提供优化 sidecar plugin 的压缩策略的环境变量，所以我们只能通过修改 argoCD 源码来实现。</p><p>早期在 argoCD 中。提供了 <code>argocd.argoproj.io/manifest-generate-paths</code> 配置，可以设置在 argoCD Application 的 Annotation 中。这原本是用来处理 webhook 请求，如果 argoCD 接入了 github webhook，那么在用户 push code 后，github 会发送 push event 到 argoCD server，argoCD 再判断变动的文件是否在 <code>argocd.argoproj.io/manifest-generate-paths</code> 路径中，如果是，argoCD 会自动触发 sync，否则就忽略。</p><p>这个值往往就对应了渲染这个 Application 中所有 l8s resources 时所需要用到的文件。</p><p>所以我们可以复用这个配置，修改 argoCD 代码，让它一旦发现 Application 中有 <code>argocd.argoproj.io/manifest-generate-paths</code> 配置，就不再压缩整个 repo，而是只压缩并传输 <code>argocd.argoproj.io/manifest-generate-paths</code> 配置中的文件。</p><p>在优化完成后，我们的 argoCD repo-server 的 CPU 使用率明显下降 95%，tarball 耗时从 3s -&gt; 400ms，同时 argoCD sync 的速度也有了明显的提升（20s -&gt; 3s）。</p><h3 id="3-优化-argoCD-repo-server-中-git-fetch-的策略"><a href="#3-优化-argoCD-repo-server-中-git-fetch-的策略" class="headerlink" title="3. 优化 argoCD repo-server 中 git fetch 的策略"></a>3. 优化 argoCD repo-server 中 git fetch 的策略</h3><p>通过观察 argoCD 的 grafana dashboard，我们发现 git fetch 的 metrics 偶尔会出现异常，拉取代码会超过 20s（repo 变动频发，且有 450k+ 的 commit），而正常情况下耗时都在 1s 以内。</p><p>因此我们猜想可能是因为请求被打到的 repo-server 长时间没有接收到请求，导致 git 代码落后太久，导致接受到 sync 请求时，会花费较长的时间 git fetch 代码。</p><p>所以我们对 argoCD repo-server 代码也做了一些优化。</p><p>当 repo-server 中的 git repo 长时间（超过 5min）没有被更新时，主动触发 git fetch，从而保证 repo-server 中的代码是最新的，减少之后 git fetch 的耗时。</p><h3 id="优化-argoCD-repo-server-中-git-gc-的策略"><a href="#优化-argoCD-repo-server-中-git-gc-的策略" class="headerlink" title="优化 argoCD repo-server 中 git gc 的策略"></a>优化 argoCD repo-server 中 git gc 的策略</h3><p>因为 monorepo 中变动频发，，可能导致 repo-server 中的 git 目录里有很多松散对象或者包文件，这可能会触发 git 的 auto gc，我们不希望在正常的使用过程中出现 git gc，导致 repo-server 不可用。因此我们重新 build 了 argoCD repo-server 镜像，在其中添加了 <code>.gitconfig</code> 文件，禁用了 git gc。</p><p>然后再修改 argoCD 代码，增加了一个 grpc function，通过调用这个 function 主动触发 git gc。在 git gc 时，repo server 会将自己的 readinessProbe 主动失败，等这个 pod 被移除出 k8s service 后，再执行 git gc，gc 完后再加入 k8s service。</p><p>最后再通过 k8s cronjob 每天定时执行 git gc 任务。执行时，会获取所有 repo-server pod，然后通过 grpc 挨个请求 pod ip 触发 git gc。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的优化策略，我们进一步优化了 argoCD 处理大量并发 sync app 的性能，并提高了 sync app 的速度，使其能够满足我们的性能和稳定性要求。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文中 ArgoCD 为 2.8.x 版本。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 &lt;a href=&quot;/2024/08/03/argocd_mono_repo_performance_optimization/&quot;&gt;argocd-monore</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="argocd" scheme="https://domc.me/tags/argocd/"/>
    
    <category term="gitops" scheme="https://domc.me/tags/gitops/"/>
    
    <category term="monorepo" scheme="https://domc.me/tags/monorepo/"/>
    
  </entry>
  
  <entry>
    <title>ArgoCD Monorepo 性能优化之路</title>
    <link href="https://domc.me/2024/08/03/argocd_mono_repo_performance_optimization/"/>
    <id>https://domc.me/2024/08/03/argocd_mono_repo_performance_optimization/</id>
    <published>2024-08-03T23:47:13.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文中 ArgoCD 为 2.8.x 版本。</p></blockquote><p>在这篇博客中，我将分享我们在使用 ArgoCD 和 Monorepo 的过程中遇到的性能问题以及我们是如何解决这些问题的，最终实现在 ArgoCD 中使用一个 Monorepo 稳定部署超过 100k+ 应用的。</p><h2 id="为什么使用-Monorepo"><a href="#为什么使用-Monorepo" class="headerlink" title="为什么使用 Monorepo"></a>为什么使用 Monorepo</h2><h3 id="通过目录结构划分环境和集群，方便权限控制"><a href="#通过目录结构划分环境和集群，方便权限控制" class="headerlink" title="通过目录结构划分环境和集群，方便权限控制"></a>通过目录结构划分环境和集群，方便权限控制</h3><p>通过 Prow 和 OWNERS 文件，只有特定的人或 team 能 <code>/lgtm</code> 合并特定 <code>业务/环境/集群/应用</code> 的变更 PR。</p><h3 id="方便批量变更"><a href="#方便批量变更" class="headerlink" title="方便批量变更"></a>方便批量变更</h3><p>因为所有部署文件都在一个仓库中，当有新的 feature 需要 enable 时，可以通过脚本批量更新所有部署文件。</p><h3 id="方便新环境，新集群上线"><a href="#方便新环境，新集群上线" class="headerlink" title="方便新环境，新集群上线"></a>方便新环境，新集群上线</h3><p>有新环境或新集群需要上线时，批量拷贝文件夹并替换全局变量即可同步所有 infra 组件&#x2F;应用 到新集群。</p><h3 id="当前用法"><a href="#当前用法" class="headerlink" title="当前用法"></a>当前用法</h3><p>因业务需要，我们会在每个业务的账号中部署一套 ArgoCD，因此有多个 ArgoCD 集群。不过 98% 以上的应用使用同一个 Monorepo。</p><p>以下是常用的几个 ArgoCD 集群以及用量。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/argocd_monorepo/argo-cd-1.png" >        </sapn>      <br>        <span class="lazyload-img-span">        <img              data-src="/images/argocd_monorepo/argo-cd-2.png" >        </sapn>      <br>        <span class="lazyload-img-span">        <img              data-src="/images/argocd_monorepo/argo-cd-3.png" >        </sapn>      </p><h4 id="然后通过-Apps-of-App-模式部署应用"><a href="#然后通过-Apps-of-App-模式部署应用" class="headerlink" title="然后通过 Apps of App 模式部署应用"></a>然后通过 Apps of App 模式部署应用</h4><p>在 ArgoCD 中用一个 Global Application 去创建各个控制部署资源的 Application，再由这些 Application 去创建部署资源（Deployment，Service 等）。</p><h4 id="目前我们只使用了-Application"><a href="#目前我们只使用了-Application" class="headerlink" title="目前我们只使用了 Application"></a>目前我们只使用了 Application</h4><p>虽然我们也需要将一份配置同时部署到多个集群，但我们没有使用 ApplicationSet。如果是镜像集群，我们会修改镜像集群中 Application 的 source，直接指向主集群的 gitops 目录。</p><p>如果是灾备集群，它会有单独的配置，在上游的发布系统中，发布某个服务的主集群时，会一同修改这个集群里的配置。</p><hr><p>但是众所周知，ArgoCD 对 Monorepo 的支持非常差。我们做了很多努力来优化它的性能，使其能够满足我们的性能和稳定性要求。</p><h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><p>我们通过以下几个方面来优化 ArgoCD 的性能。</p><h3 id="Monorepo-层面"><a href="#Monorepo-层面" class="headerlink" title="Monorepo 层面"></a>Monorepo 层面</h3><ul><li><strong>减少 Monorepo 体积</strong>：只存放部署文件。</li><li><strong>定时清理 Git 的 commit 记录</strong>：每次发版会创建一个 commit，当 commit 数量太多时，会严重增大 repo 的大小，导致 repo server 拉取速度变慢。我们会在 repo commits 数量超过 1M 时，备份 repo，然后清空 commit 记录。</li></ul><h3 id="ArgoCD-层面"><a href="#ArgoCD-层面" class="headerlink" title="ArgoCD 层面"></a>ArgoCD 层面</h3><ul><li><p><strong>增加 repo server 节点，关闭 repo server 的 HPA</strong>：</p><p>每次 ArgoCD <code>refresh</code> 或 <code>sync</code> 应用时都会请求 repo server 获取 <code>helm/kustomize</code> 渲染后的 K8s YAML 文件。由于每个 repo server pod 对同一个 repo 一次只能处理一个请求，而我们使用的是 mono repo（只有一个 repo），因此 repo server 的数量限制了 ArgoCD 的 sync 并发数。增加 repo server 的数量可以明显提高发版高峰期的 sync 效率。</p><p>但是，repo server 每次启动时会全量 clone repo（以我们现在的 mono repo 为例，有 40 万个 commits，虽然代码只占 150M，但仓库总大小超过 4Gi，全量克隆一次需要花费 3-4 min）。如果触发 HPA 导致 repo server 频繁扩缩容，反而会影响 sync 性能。因此，我们通常会为 repo server 配置足够的 replicas 以应对日常发版需求。当需要批量变更应用时，再手动扩容 repo server。</p></li><li><p><strong>关闭 auto refresh，将 appResyncPeriod 设置成 0</strong>：</p><p>我们关闭了所有 Applications 的 auto sync，以避免 ArgoCD 大量主动 refresh + sync Application，导致各个组件压力过大，无法处理正常用户的同步请求。</p><p>将 <a href="https://argo-cd.readthedocs.io/en/stable/faq/#how-often-does-argo-cd-check-for-changes-to-my-git-or-helm-repository">appResyncPeriod</a> 设置为 0 意味着 application controller 不会主动请求 refresh Application，从而减轻各个组件的压力，这对性能优化也非常重要。</p></li></ul><h4 id="关闭-auto-sync-后，如何在-commit-之后触发-Application-的-sync-呢-？"><a href="#关闭-auto-sync-后，如何在-commit-之后触发-Application-的-sync-呢-？" class="headerlink" title="关闭 auto sync 后，如何在 commit 之后触发 Application 的 sync 呢 ？"></a>关闭 auto sync 后，如何在 commit 之后触发 Application 的 sync 呢 ？</h4><p>我们使用 <code>GitHub Action Workflow</code> 来监听文件变动，然后找到对应的 Application 并触发相应的 sync.</p><p>这得益于目录结构的规划，使我们能很方便的解析出变动文件对应的 Application。</p><p>目录结构如下:</p><pre><code class="markdown">├── app|infra # 分别对于业务应用和 infra 组件│ ├── $project # 所属的项目│ │ ├── $env # 所属的环境│ │ | |-- $cluster # 所在的集群│ │ | | |-- kustomize|helm # 使用的 argocd plugin│ │ | | | |-- $app # app 名称│ │ | | | | |-- values.yaml # 具体的配置文件│ │ | | | | |-- application.yaml # 用于生成这个 app 的 argo Applications</code></pre><p><strong>Github Action Workflow 的执行步骤</strong>:</p><ol><li>解析 commit 中变动的文件，找出变动的 应用类型 (app or infra), project, env, cluster，根据这些信息找到这个集群的 Global Application。</li><li>Sync Global Application。因为应用的 Application 会存储一些信息，需要先把应用的 Application 的 spec 同步上。</li><li>通过 <code>argocd app sync $app</code> sync 具体应用的 Application。</li></ol><p>这里还存在一个问题，即 Global Application 会管理大量的应用（可能有几千个）。这会导致同步 Global Application 非常慢。此外，当多个用户在同一个集群内同时发布应用时，所有人都需要同步这个 Global Application，必然会造成拥堵，导致大家都在这一步等待。</p><p>因此我们将这一步也进行了优化。</p><p>我们更改了 Global Application 的 sync 方式，不通过 argocd app sync，而是直接执行 kubectl apply。</p><p>在 GitHub Action Workflow 中，我们会先找到发生变动的应用。然后直接渲染出它的所有 Application（一般有多个，因为一个应用可能有多个 overlays）。接着，从 ArgoCD 所在的 Kubernetes 集群中列出这个应用的所有 Applications，并进行差异比较，找出不一样的 Applications，执行 <code>kubectl apply</code>。这样相当于变相刷新了 Global Application。</p><p>流程图如下：</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/argocd_monorepo/argo-cd-workflow.jpg" >        </sapn>      </p><p>经过上述优化后，我们能做到当一个 argocd 管理同一个 repo 中接近 7k 应用时，用户的发版也能在 30-50s 完成。（其中 sync global app 10s 左右，argocd sync app 在 15-35s 左右）<br>而且不会影响各个用户之间的发版。</p><h4 id="但是仍然存在影响-argocd-稳定性的问题"><a href="#但是仍然存在影响-argocd-稳定性的问题" class="headerlink" title="但是仍然存在影响 argocd 稳定性的问题"></a>但是仍然存在影响 argocd 稳定性的问题</h4><p>在完成上述的性能优化后，尽管 argocd 已经能满足业务发版的要求，绝大多数情况下，发版的速度都能满足业务的要求。但不幸的是，仍然存在一些会引起 argocd 抖动的问题。</p><p>在业务 Kubernetes 集群内资源变动时，ArgoCD 会对资源所属的 Applications 进行刷新。刷新过程中，controller 会检查缓存，如果缓存存在则结束处理；如果缓存不存在，则会请求 repo server 重新渲染资源。大多数情况下，刷新 Applications 会命中缓存，但在一些极端情况下，可能无法命中缓存，从而导致请求 fallback 到 repo server。由于 repo server 渲染资源非常耗时且占用大量 CPU，当大量渲染请求涌向 repo server 时，会导致其 CPU 被占满，甚至出现 OOM 情况，进而导致正常的同步请求无法处理，用户发布超时。可能造成这问题的原因包括：</p><ol><li>当业务 DevOps 批量操作集群内数据时，例如批量重启 deployments，ArgoCD 监听到 deployments 被修改或者 pods 被删除&#x2F;创建时，会找到相应的 Applications 并进行刷新。这会导致短时间内大量的 applications 被刷新。</li><li>当 DevOps 升级业务集群时，由于需要替换 nodes，这将导致大量 pods 被重建，进而致短时间内大量的 Applications 被刷新。</li><li>一些经常被 operators 更新的 CRD 也会触发 Applications 刷新。例如，KEDA 会频繁更改 HPA 的 spec，导致 Applications 被频繁刷新。</li></ol><p><strong>优化的方法</strong></p><p>argocd 2.8 之后提供了参数，可以忽略掉集群内一些资源的特定字段的监听：</p><pre><code class="yaml"># argo-cd helm values.yaml# 比如这里配置了所有资源的 `.status` 和 `.metadata.resourceVersion` 的变更，都不会触发 argocd refreshserver:  config:    resource.customizations.ignoreResourceUpdates.all: |      jsonPointers:      - /status      - /metadata/resourceVersion</code></pre><p>但这样仍然不能完全解决问题，因为总有些漏网之鱼，比如新部署了一个 operator 会频繁修改它的 CRD spec 之类的。因此，我们还需要想其他办法来根治这个问题。</p><p>然后我们想到，因为我们禁用了所有 Applications 的 auto sync，所以其实这个监听资源变动 + refresh Application 的机制，对我们来说可有可无。</p><p>因此我们魔改了 argocd controller 的代码：</p><p>如果这个监听到资源的变动，但是这个资源所属的 application 没有开启 auto sync 或者没有正在被操作（正在被手动 sync 中），那么就不 refresh Application。</p><p>至此这个问题，才被彻底解决。</p><p>改动的 PR:</p><p><a href="https://github.com/domechn/argo-cd/pull/1">Improve performance for refreshing apps by domechn · Pull Request #1 · domechn&#x2F;argo-cd (github.com)</a></p><hr><p><strong>除此之外还要保证 repo server 不会因为短时间大量请求被打到 OOM</strong></p><p>首先 repo server OOM 必然导致当前正在处理的 argocd sync 请求失败，其次因为 repo server 重启后要重新拉代码，会很大延长下次 argocd sync 的时间，影响发版稳定性。</p><p>不过 argocd 提供了 <code>reposerver.parallelism.limit</code> 这个参数，可以限制 repo server 同一时间并发处理渲染请求的数量。这个值是一个经验值，可能要根据 mono repo 大小和 repo server pod resource 来调整。以我们自己的经验来看，我们的 repo 在 4G 左右，repo server resource limit 给的是 12Gi4vCPU, sidecar plugin 是 6Gi4vCPU，那么它设置成 15，repo server 几乎就不会被打到 OOM。</p><br/><p>这样一番优化完后，argocd 的性能表现终于变得稳定，几乎很少有抖动，而且理论上之后性能也不会太受到 application 数量增长的影响（也行还会有，但目前没发现）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文中 ArgoCD 为 2.8.x 版本。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在这篇博客中，我将分享我们在使用 ArgoCD 和 Monorepo 的过程中遇到的性能问题以及我们是如何解决这些问题的，最终实现在 ArgoCD 中使用一个 </summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="argocd" scheme="https://domc.me/tags/argocd/"/>
    
    <category term="gitops" scheme="https://domc.me/tags/gitops/"/>
    
    <category term="monorepo" scheme="https://domc.me/tags/monorepo/"/>
    
  </entry>
  
  <entry>
    <title>如何管理我的加密货币资产</title>
    <link href="https://domc.me/2023/06/30/how_to_manage_crypto_assets/"/>
    <id>https://domc.me/2023/06/30/how_to_manage_crypto_assets/</id>
    <published>2023-06-30T17:51:20.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>随着 web3 的发展，以及我对 web3 领域前景的看好，我开始购入越来越多的数字货币资产，我也决定将大部分的资产全部转换成数字货币。这不仅包括我原来有的一些资产（存款，基金，股票），也包括了之后每个月的固定收入。</p><p>通常情况下，我会将法币资产通过中心化交易所换成 USDT，再将 USDT 交易成其他加密货币，例如 BTC，ETH … 最后看情况将 BTC 等货币转入我自己的钱包。</p><p>为了分散风险，我不会将所有资产都存入一个钱包中，避免这个钱包私钥泄漏丢失所有资产。为此我创建了多个钱包，将各个钱包的私钥分开保管。（目前是将每个钱包的助记词存在助记词钢板或者胶囊中，没有做隔离，也就是说万一钢板丢失，还是有泄漏的风险。未来可能考虑将助记词拆开存放，比如 24 位的助记词，拆成 3 ～ 4 份，每份存储 12 ～ 16 个助记词。这样就算有一个丢失，也不会泄漏钱包，而且也能通过另外几份助记词将钱包恢复出来）</p><p>同时为了考虑安全性，我不会选择把资金存入软件钱包中（尽管它们用来很方便）。用于存放资金的钱包必须是硬件钱包。这里我使用了 ledger 和 tangem。</p><p>ledger 不必多说，是非常老牌的硬件钱包品牌。关于 tangem，tangem 是一种卡片式的硬件钱包，它将私钥等敏感信息存储在卡片里，通过它的 app 和 nfc 完成数据的签名。它的好处是，私钥对用户不可见，所以用户不需要担心私钥泄漏的问题。但是也是因为这个问题，如果卡片全部丢失（它默认提供了 2 张备份卡片），那么用户将永远失去这个钱包。并且卡片需要配合 tengem 的 app 使用，如果 tengem 跑路，也会有一定的影响。</p><table><thead><tr><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/ledger.png" >        </sapn>      </th><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/tangem.png" >        </sapn>      </th></tr></thead></table><p>我将这些钱包分成了三种类型。</p><ol><li>用于大量存储加密货币，并且不会做任何除了转账操作以外的交互。</li><li>用于存储一部分加密货币，并且会连接一些知名的 defi 项目（比如 uniswap, pancakeswap)，进行质押操作，赚取一部分的收益。或者通过 opensea 之类的平台，购买一些 nft。</li><li>用于存储很少部分的加密货币，这种类型的钱包的使用相对来说就没有什么限制，可以连接一些新项目，用于打新或者体验新特性。</li></ol><p>鉴于这种拆分钱包管理方式，它带来了安全性上的好处，但同时也造成一些困扰。我的资产分散在各个地方，中心化交易所，已经 n 个数字钱包。这使得做资产统计变成一个很麻烦的事情。</p><p>早期我使用了一些传统的资产统计工具，比如 Percento，它们很强大，能够统计不仅仅是加密货币，甚至包括银行账户，股票等等多种资产，也能做到实时更新各个数字货币以及股票的价格。但他们也存在着一些缺点，比如所有的加密货币持仓数据都需要手动录入。</p><p>手动录入数据在我的场景看来非常麻烦。首先是每个月发工资买入数字货币之后或者是换仓之后，我时常需要来手动更新我的各种加密货币资产，这花费了我很多时间。所以我决定每月只在特定的一天更新各个加密货币的数量，这也导致了数据不一定是实时的。再加上我有很多钱包，有时候甚至会在统计过程中漏掉一些资产，导致数据出现混乱。</p><p>因此我理想中的加密资产统计工具，应该是只需要输入钱包地址，或者连接到中心化交易所，它就能自动读取这些钱包里所有的加密货币资产数量，以及它们的实时价格。出此之外，我还希望能看到对某个数字货币持仓数量的变化，我的所有数字货币持仓的占比，以及它们排名，总价格和单价变化等等数据。</p><p>这样我能清楚的看到我各个阶段的资产详情和持仓变化。此外，我还希望能够对比两天的详细数据，比如我想知道 6 月 1 日的持仓数据和 5 月 1 日比发生了哪些变化，哪些币的价格上涨了，哪些币的持仓数量下降了。这样能帮助我分析出我的换仓决策是否正确。</p><p>所以基于这些需求，我开发出了 <a href="https://github.com/domechn/track3">track3</a>，一个专注于加密资产统计的工具。它目前支持了几个主流数字货币的资产统计，比如 btc, erc20 token ( eth, eth layer2, bnb 等 evm 兼容的货币），doge coin, sol，以及支持了两大主流中心化加密货币交易所 binance 和 okex。</p><!-- ![track3](/images/track3/track3-logo.png) --><div align="center">        <span class="lazyload-img-span">        <img  style="width:72px"             data-src="/images/track3/track3-logo.png" >        </sapn>      </div><p>我可以只输入我的钱包地址，以及各个中心化加密货币交易所的 api key ( 只读权限即可）。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-configuration.png" >        </sapn>      </p><p>在我每次打开应用点击刷新的时候，track3 就会自动读取这些交易所或者钱包里所有主流货币（ coingecko 可以查到的货币）的数量和价格，并对这些资产做分析。</p><p>在资产分析功能上，我可以看到</p><ol><li><p>我当前总资产的法币价格（支持 usd, eur, cny 等 150+ 主流法币）</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-total-value.png" >        </sapn>      </p></li><li><p>我的前 10 大货币持仓占比</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-assets-percentage.png" >        </sapn>      </p></li><li><p>我的总资产的价格变化趋势</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-total-value-change.png" >        </sapn>      </p></li><li><p>我的每种货币的数量和它的总价格变化趋势</p><table><thead><tr><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-coin-amount-change.png" >        </sapn>      </th><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-coin-value-change.png" >        </sapn>      </th></tr></thead></table></li><li><p>我的前 10 大货币排名变化</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-rank.png" >        </sapn>      </p></li><li><p>我的各个货币较其实日期的总价格和单价的百分比变化</p></li></ol><table><thead><tr><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-value-change.png" >        </sapn>      </th><th>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-price-change.png" >        </sapn>      </th></tr></thead></table><p>在资产对比功能上，我可以看到两个日期之间</p><ol><li>总资产的对比</li><li>每种货币的总价格，单价和数量的对比</li></ol><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-comparision.png" >        </sapn>      </p><p>并且，它还支持云同步，所以我可以在多个设备之间来回使用，也不用担心数据不同步的问题。（如果不开启云同步功能，所有数据永远只会存储在本地）</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/track3/track3-cloud-sync.png" >        </sapn>      </p><p>但是为了保证安全性，track3 只会将资产数据同步到云上。因此不需要担心钱包地址或者交易所的 api key 泄漏的问题，它们永远只会被加密之后保存在本地。为了保证隐私性，即使是云上资产数据也无法被除了本人以外的其他人查看。也就是说即使是云上数据库的 admin，也无法查看到用户的数据。能这样实现，这是因为云数据同步功能使用了 <a href="http://polybase.xyz/">polybase.xyz</a> 作为数据库，它是一种去中心化的数据库，感兴趣的朋友可以去他们的官网查看功能文档，了解更多。</p><p>因为 track3 的诞生，此后我统计加密货币资产的方式变得无比轻松。我只需要在我想要查看最新数据的时候，点击下刷新按钮。之后 track3 遍会自动帮我更新所有钱包和交易所内的资产数据。我也只需要在创建了新的钱包之后，将钱包地址配置到 track3 即可。不需要再和以前一样，一个个查询每个钱包内的资产，手动计算它们数量的总和，并一个个录入到资产统计系统中。</p><p>如果你也有类似的困扰，不妨尝试使用 track3 来优化加密货币资产管理。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着 web3 的发展，以及我对 web3 领域前景的看好，我开始购入越来越多的数字货币资产，我也决定将大部分的资产全部转换成数字货币。这不仅包括我原来有的一些资产（存款，基金，股票），也包括了之后每个月的固定收入。&lt;/p&gt;
&lt;p&gt;通常情况下，我会将法币资产通过中心化交易所</summary>
      
    
    
    
    
    <category term="web3" scheme="https://domc.me/tags/web3/"/>
    
    <category term="crypto" scheme="https://domc.me/tags/crypto/"/>
    
    <category term="assets-management" scheme="https://domc.me/tags/assets-management/"/>
    
  </entry>
  
  <entry>
    <title>Cilium 从 0 到 0.1</title>
    <link href="https://domc.me/2021/10/17/cilium_0_to_0_1/"/>
    <id>https://domc.me/2021/10/17/cilium_0_to_0_1/</id>
    <published>2021-10-17T16:40:13.000Z</published>
    <updated>2025-04-11T02:56:57.521Z</updated>
    
    <content type="html"><![CDATA[<p><em>本文基于 <code>Cilium 1.10</code> 编写</em></p><p>这篇文章会和大家分享一些 Cilium 相关的知识，包括以下几个主要部分</p><ul><li>eBPF 的基础知识</li><li>Cilium 对 eBPF 的应用</li><li>Cilium 中一些 Features 的使用</li><li>在 Cilium 中如何做 TroubleShooting</li></ul><p>当然在开始将 eBPF 和 Cilium 之前，会还是会简单介绍一下以下一些基础知识</p><ul><li>在传统网络协议栈中，Linux 接收数据包的过程</li><li>CNI 中的网络是如何通信</li></ul><p>那么接下来开始这篇文章的主要内容</p><h2 id="Linux-处理数据包的过程"><a href="#Linux-处理数据包的过程" class="headerlink" title="Linux 处理数据包的过程"></a>Linux 处理数据包的过程</h2><h3 id="接收"><a href="#接收" class="headerlink" title="接收"></a>接收</h3><p>这里会简单介绍接收数据包的过程，即数据包是如何从网络被送到内核协议栈的</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/network_data_path.jpg" >        </sapn>      </p><ol><li>首先当数据包从网络到达网卡（NIC）</li><li>数据包被复制（通过 DMA）到内核内存的环形缓冲区（Ring Buffer）</li><li>NIC 产生硬件中断（IRQ）让 CPU 知道数据包在内存中</li><li>CPU 根据注册好的中断函数，找到并调用 NIC Driver 中对应的函数</li><li>NIC Driver 会禁用硬件中断，之后有数据包继续到 NIC 后，NIC 会直接把数据包写到内存中，不再会产生硬件中断</li><li>NIC Driver 启动软中断（Soft IRQ），让内核中的 <code>ksoftirqd</code> 进程调用 Driver 中的 poll 函数读取 NIC DMA 到内存的数据</li><li>通过 Driver 中的函数将读取到的数据，转换成 <code>skb</code> 格式（内核规定的格式）</li><li>GRO 将多个 <code>skb</code> 合并成大小相同的 skb</li><li><code>skb</code> 数据被被协议栈相关的函数处理成协议层的格式</li><li>等内存中所有的数据包被处理完成后，NIC Driver 启动 NIC 的硬件终端</li></ol><h3 id="发送"><a href="#发送" class="headerlink" title="发送"></a>发送</h3><ol><li>数据包经过网络协议栈的封装成 <code>skb</code> 后会被放到网卡的发送队列中</li><li>内核通知网卡发送数据包</li><li>网卡发送完成后发送中断给 CPU</li><li>CPU 收到中断后进行 skb 的清理工作</li></ol><h2 id="CNI-中的网络通信"><a href="#CNI-中的网络通信" class="headerlink" title="CNI 中的网络通信"></a>CNI 中的网络通信</h2><p>这里会简单介绍下在 Kubernetes 中，Pod 和 Pod 之间是如何通信的。那么在 K8S 中存在两种 Pod 之间通信的情况，<code>同节点 Pod 通信</code> 和 <code>跨节点 Pod 通信</code></p><p>在讨论 Pod 和 Pod 通信之前，先了解下 Pod 的数据包是如何发送到 Host 节点上的。</p><p>Pod 在 Kubernetes 中是指一组 Containers 的统称，Container 技术本质上是使用了 Linux 的 Namespace 技术。在 Pod 中的所有 Containers 通过共享同一个 Namespace 来实现它们之间的网络共享以及和外部网络的隔离</p><p>但被网络隔离起来 Containers 总有访问其他服务的需求，因此它们发出的数据包必定是要能送达到外部的，为了满足这种需求当然有很多方案都能做到，比如将物理网卡或者虚拟网卡 attach 到 Namespace 中</p><p>但这里主要介绍的还是大部分 CNI 所采用的方案，那就是 veth-pair</p><h3 id="veth-pair"><a href="#veth-pair" class="headerlink" title="veth-pair"></a>veth-pair</h3><p>veth-pair 就是一对的虚拟设备接口，它都是成对出现的，两端彼此相连，一端发送的数据会被另一端收到。正因为有这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备</p><p>如果想要让 Namespace 中的服务能够把数据报文发送到外部，可以创建出一对 veth-pair，一端放在 Namespace 中，另一端放在 Host 中，这样当 Namespace 中的服务发送报文时，报文就会被发送到 Root Namespace （ Host ）中，这样 Host 可以根据 route 表中的路由规则将报文转发到目标服务</p><p>当然上面这段只是理想情况，实际上还有很多东西没有考虑进去，比如网络通信的四要素，<code>src_ip</code>, <code>src_mac</code>, <code>dst_ip</code> 和 <code>dst_mac</code>，没有这四种信息数据报文自然是无法送达到目的地的。在上述场景中 <code>src_ip</code>（Namespace 中被 attach 的 veth-pair 的网卡的 ip）, <code>src_mac</code>（同上） 和 <code>dst_ip</code> 都是已知的，所以接下来的问题就是如何获取 <code>dst_mac</code></p><blockquote><p>在网络通信中 <code>dst_mac</code> 是指 nextHop 的地址，如果走 L3 转发那么 dst_mac 则为网关地址，如果走 L2 交换，<code>dst_mac</code> 则为目的地址</p></blockquote><h3 id="同节点-Pod-通信"><a href="#同节点-Pod-通信" class="headerlink" title="同节点 Pod 通信"></a>同节点 Pod 通信</h3><p>如上面所说，同节点 Pod 之间的通信的问题就是如何解决 <code>dst_mac</code> 的问题</p><p>不同的 CNI 有不同的解决方案。</p><p>比如在 <code>Flannel</code> 中，Flannel 会在 Host 上创建一个 <code>NetworkBridge</code> (<code>cni0</code>)，然后将 Pod 的 IP 设置成 <code>24</code> 位，<code>veth-pair</code> 的一端 attach 到 Pod 中，另一端 attach 到 cni0 上。因为同节点上的 Pod 都是 24 位 IP，所以 Pod 之间通信走二层交换，src Pod 可以通过 arp 请求获取到 dst Pod 的 mac 地址，这样也就解决了 <code>dst_mac</code> 的问题</p><blockquote><p>NetworkBridge 可以看作是虚拟的交换机，attach 到 network bridge 上的设备之间可以相互通信</p></blockquote><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/flannel_cni0.jpg" >        </sapn>      </p><p>但这篇文章主要会介绍另一种模式（之后所有内容也是基于该模式作为基础），该模式也在 <code>Calico</code> 或者是 <code>Cilium</code> 中使用</p><p>在 <code>Calico</code> 或者 <code>Cilium</code> 中，Pod 的 IP 会被设置成 32 位。因此在这种情况下，Pod 访问任何其他 IP 都是走 L3 路由</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/calico_veth.jpg" >        </sapn>      </p><p>上面是我从 kubernetes 集群中获取到的数据，cni 为 Calico</p><p>从 Pod 中的路由表可以看到，默认网关的地址为 <code>169.254.1.1</code>，因此如果要从 Pod1 访问其他服务需要先获取到 <code>169.254.1.1</code> 的 mac 地址，但很明显这个 mac 地址是无法通过 arp 请求获取到的</p><p>那么如何解决这个问题呢？事实上 Calico 和 Cilium 采用了不同的方案</p><p><strong>Calico</strong></p><p>Calico 使用了 <code>proxy_arp</code> 来解决，简单来说开启 proxy_arp 的网络设备可以被当作是一个 arp 网关，当它接收到 arp 请求时，它会把自己的 mac 地址回复给请求者。因为 veth-pair 的缘故，Pod 中 eth0 发出的请求都会被送到它所绑定的 <code>veth*</code> 中。</p><p>因此给该 <code>veth*</code> 开启 proxy_arp 之后，veth* 就能够把它的 mac 回复给 Pod，这样数据报文就能被送出来，当数据包文被送到 Host 中后，再根据 Host 内的本地路由表，将数据报文送到对应的 Pod 挂在 Host 上的 veth-pair 设备上了</p><p><strong>Cilium</strong></p><p>Cilium 的做法则是过在 <code>veth*</code> 上 attach <code>tc ingress bpf 程序</code>，为 Pod 的所有 arp 请求都返回 veth* 的 mac</p><h3 id="跨节点-Pod-通信"><a href="#跨节点-Pod-通信" class="headerlink" title="跨节点 Pod 通信"></a>跨节点 Pod 通信</h3><p>本篇文章不会着重去介绍跨节点通信时用到的，比如 <code>Overlay</code> 或者是 <code>Underlay</code> 之类这些网络技术</p><p>而且 Cilium 着重解决的也不是这方面的问题，且 Cilium 在解决跨节点传输的问题上用的也都是非常主流的技术，比如 <code>VxLan</code> 或者是 <code>BGP</code>，这里就不展开说了</p><p>如果对这部分感兴趣的话，推荐大家去看 Calico 这部分的文档</p><h2 id="eBPF-基础知识"><a href="#eBPF-基础知识" class="headerlink" title="eBPF 基础知识"></a>eBPF 基础知识</h2><p>在结束了上面的铺垫之后，接下来开始介绍本次的主角 eBPF</p><p>目前谈到的 BPF 技术分两种，<code>cBPF</code> 和 <code>eBPF</code></p><p><code>cBPF</code> 诞生于 1997 年，内核 2.1.75 版本，是类 Unix 系统上数据链路层的一种原始接口，提供原始链路层封包的收发。<code>tcpdump</code> 的底层正是采用 cBPF 作为底层包过滤技术</p><p><code>eBPF</code> 诞生于 2014 年，内核 3.18 版本，eBPF 新的设计针对现代硬件进行了优化，所以 eBPF 生成的指令集比旧的 BPF 解释器生成的机器码执行得更快</p><p>cBPF 现在已经基本废弃，目前内核只会运行 eBPF，内核会将加载的 cBPF 字节码透明地转换成 eBPF 再执行（本文以下皆用 BPF 指代 eBPF）</p><p>BPF 拥有以下优点：</p><ul><li>在内核中运行沙箱程序，而无需修改内核源码或者加载内核模块，将 Linux 内核变成可编程之后，就能基于现有的（而非增加新的）抽象层来打造更加智能、 功能更加丰富的基础设施软件，而不会增加系统的复杂度，也不会牺牲执行效率和安全性</li><li>可以热重启，并且内核会帮助管理状态，在不会引起流量中断（traffic interrupt）的前提下，原子地替换运行中的程序</li><li>内核原生，不需要倒入第三方内核模块</li><li>安全性，内核会检查 BPF 程序确保它不会造成内核崩溃</li></ul><p>同时 BPF 拥有以下几种特性：</p><ol><li>BPF map：高效的 kv 存储</li><li>辅助函数：可以方便地使用内核功能或和内核交互</li><li>尾调用：高效地调用其他 BPF 程序</li><li>安全加固原语</li><li>支持 object pinning，实现持久存储</li><li>支持 offload 到网卡</li></ol><p>接下来会对这些特性做介绍</p><h3 id="BPF-Map"><a href="#BPF-Map" class="headerlink" title="BPF Map"></a>BPF Map</h3><p>为了让 BPF 能够持久化状态，内核提供了驻留在内核空间的高效 kv 存储器（BPF map），BPF map 可以被 BPF 程序访问，可以在多个 BPF 程序之间共享（共享的程序之间不一定要求是相同的类型），也可以通过 fd 的形式被用户空间的程序访问。因此用户程序可以使用 fd 相关的 api 方便的操作 map</p><blockquote><p>共享的程序之间不一定要求是相同的类型指： tracing programs 也可以和 networking programs 程序共享 map</p></blockquote><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/bpf_map.png" >        </sapn>      </p><p>但 fd 受到进程的生命周期的影响，使得 map 的共享等操作实现起来变得复杂,为了解决这个问题，内核开发了 object pinning 功能，该功能能将 map 的 fd 能保留住，不会随着进程退出而被删除</p><h3 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h3><p>使得 BPF 能够通过一组内核定义的函数调用来从内核中查询数据，或者将数据推送到内核</p><blockquote><p>不同类型的 BPF 程序能够使用的辅助函数可能是不同的</p></blockquote><h3 id="尾调用-Tail-Calls"><a href="#尾调用-Tail-Calls" class="headerlink" title="尾调用 (Tail Calls)"></a>尾调用 (Tail Calls)</h3><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/bpf_tailcall.png" >        </sapn>      </p><p>BPF 支持一个 BPF 程序可以调 用另一个 BPF 程序，并且调用完成后不用返回到原来的程序（只有相同类型的 BPF 程序才可以尾调用）</p><p>和普通函数调用相比，这种调用方式开销最小，因为它是用长跳转（longjump）实现的，复用了原来的栈帧</p><p>使用场景</p><ul><li>可以通过尾调用结构化地解析网络头</li><li>在运行时原子地添加或替换功能，即动态地改变 BPF 程序的执行行为</li></ul><h3 id="offload"><a href="#offload" class="headerlink" title="offload"></a>offload</h3><p>BPF 网络程序，尤其是 tc 和 XDP BPF 程序在内核中都有一个 offload 到硬件的接口，这 样就可以直接在网卡上执行 BPF 程序</p><p>本文接下来会讨论在 Cilium 得到广泛使用的两个 BPF 子系统， XDP 和 tc 子系统</p><h3 id="XDP"><a href="#XDP" class="headerlink" title="XDP"></a>XDP</h3><p>XDP 为Linux内核提供了高性能、可编程的网络数据路径</p><p>XDP hook 位于网络驱动的快速路径上，XDP 程序直接从接收缓冲区中将包拿下来，此时 Driver 还没有将数据包转换成 <code>skb</code>，因此该数据包的元信息还没有被解析出。理论上这是软件层最早可以处理包的位置</p><p>同时因为 XDP hook 运行在网络驱动的快速路径的原因，运行 XDP BPF 程序必需得到网络驱动的支持</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/network_xdp_data_path.jpg" >        </sapn>      </p><blockquote><p>xdp program 会在第 7 步到第 8 步之间执行</p></blockquote><p>同时 XDP 运行在内核态，并不会绕过内核，这带来了以下好处</p><ul><li>它可以复用所有上游开发的内核网络驱动，用户空间工具，以及其他一些内核接触设施（例如 BPF 辅助函数在调用自身时可以使用系统路由表，socket 等）</li><li>XDP 在访问硬件是与内核其他部分相同的安全模型</li><li>无需跨越内核和用户空间</li><li>可以复用 TCP&#x2F;IP 协议栈</li><li>不需要显式地将专门 CPU 分配给 XDP，它可以支持 <code>不停轮询</code> 或者是 <code>中断驱动</code> 的模式</li></ul><p>XDP BPF 程序能够修改数据包的内容。同时 XDP 为每个数据包提供了 256 个字节的 headroom，XDP BPF 程序可以对这部分进行修改，比如在数据包前面添加自定义元数据，该部分数据对内核协议栈不可见，但是对 tc BPF 程序可见</p><pre><code class="clang">struct xdp_buff &#123;    void *data;    void *data_end;    void *data_meta;    void *data_hard_start;    struct xdp_rxq_info *rxq;&#125;;</code></pre><p>这个结构是 XDP 程序获取到的数据包的格式</p><ul><li><code>data</code>: 指向数据包起始位置</li><li><code>data_end</code>: 指向数据包结尾</li><li><code>data_hard_start</code>: 指向 hardroom 开始位置</li><li><code>data_meta</code>: 指向 <code>meta</code> 信息开始， 刚开始 <code>data_meta</code> 和 <code>data</code> 相同，随着 meta 信息增加，<code>data_meta</code> 开始向 <code>data_hard_start</code> 靠近</li><li><code>rxq</code>: 字段指向某些额外的、和每个接收队列相关的元数据</li></ul><p>从上面可以得出 <code>xdp_buff</code> 的结构</p><p><code>data_hard_start |___| data_meta |___| data |___| data_end</code></p><p>以及这些 data 字段之间的关系</p><p><code>data_hard_start &lt;= data_meta &lt;= data &lt; data_end</code></p><p><strong>XDP BPF 程序返回码</strong></p><p>XPD BPF 程序运行结束后会返回一个状态码，告诉驱动如何处理这个数据包</p><ul><li><code>XDP_DROP</code>: 在 Driver 层将该数据包丢弃</li><li><code>XDP_PASS</code>: 将这个包送到内核网络协议栈，这和没有 XDP 时默认的包处理行为是一样的</li><li><code>XDP_TX</code>: 在收到该数据包的网卡上，将该数据包再发出去（数据包一般会被修改）</li><li><code>XDP_REDIRECT</code>: 和 <code>XDP_TX</code> 类似，不过是在另一张网卡上将数据包发出去</li><li><code>XDP_ABORTED</code>: 表示程序发生异常，行为和 XDP_DROP 一致，但是会经过 trace_xdp_exception tracepoint，可以通过 tracing 工具来监控这种非正常行为</li></ul><p><strong>XDP 使用案例</strong></p><ul><li><code>DDoS 防御、防火墙</code>: 得益于 XDP 能够在最早的位置拿到数据包，然后用 <code>XDP_DROP</code> 命令驱动将包丢弃，XDP 能够实现非常高效的防火墙策略，对于 DDoS 攻击的场景来说非常理想</li><li><code>转发和负载均衡</code>: 通过 <code>XDP_TX</code> 和 <code>XDP_REDIRECT</code> 两个动作实现。<code>XDP_TX</code> 能够实现发卡模式的负载均衡器</li><li><code>栈前过滤/处理</code>: 对于不符合要求的流量可以尽早使用 <code>XDP_DROP</code> 丢弃，比如节点只接受 TCP 流量，那么对于 UDP 请求可以直接丢弃掉。同时 XDP 能够在 NIC Driver 分配 skb 之前修改数据包的内容，对于某些 Overlay 场景（需要封装和解封数据包）来说很有用，并且 XDP 能够在数据包的前面 push 元数据，且该部分对内核协议栈不可见</li><li><code>流抽样和监控</code>：XDP 可以将流量进行分析，对于异常流量可以放到 BPF map 中，提供给其他进程用于分析</li></ul><blockquote><p>一些智能网卡（例如支持 Netronome’s nfp 驱动的网卡）实现了 xdpoffload 模式，允许将整个 BPF&#x2F;XDP 程序 offload 到硬件，因此程序在网卡收到包时就直接在网卡进行处理，不过在该模式中某些 BPF map 类型 和 BPF 辅助函数是不能用的</p></blockquote><h3 id="tc"><a href="#tc" class="headerlink" title="tc"></a>tc</h3><p>除了 XDP 等类型的程序之外，BPF 还可以用于内核数据路径的 tc (traffic control，流量控制)层</p><p>tc 和 XDP 主要有以下三点不同之处:</p><p><strong>输入上下文</strong></p><p>相较于 XDP，tc 在流量路径中位于更加靠后的位置（skb 分配之后）。因此对 tc BPF 程序而言，它的输入的上下文是 <code>sk_buff</code>, 而非 <code>xdp_buff</code>，所以 tc ingress BPF 程序可以利用 <code>sk_buff</code> 中内核处理好的包的元数据。当然内核处理这些元数据是需要开销的，包括协议栈执行的缓冲区分配，元数据的提取和其他处理等过程，而 xdp_buff 不需要访问这些元数据，因为 XDP hook 在这之前被调用，所以这是 XDP 和 tc hook 性能差距的重要原因之一</p><p><strong>是否依赖驱动支持</strong></p><p>因为 tc BPF 程序运行在网络栈通用层中的 hook 点，所以它们不需要驱动做任何支持</p><p><strong>触发点</strong></p><p>tc BPF 程序在数据路径上的 ingress 和 egress 都可以触发</p><p>XDP BPF 程序只能在 ingress 点触发</p><p><strong>tc ingress</strong></p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/network_tc_data_path.jpg" >        </sapn>      </p><p>tc ingress hook 位于 GRO 之后，处理协议之前最早的处理点</p><p><strong>tc egress</strong></p><p>tc egress hook 运行的位置是，内核将数据包交给 NIC Driver 之前最晚的位置，这个地方在传统 iptables 防火墙 <code>POSTROUTING</code> 链之后，但是在 GSO 引擎处理之前</p><p><strong>tc BPF 程序执行模式</strong></p><p>在 tc 中，有以下 4 种组件</p><ul><li><code>qdisc</code>: Linux 排队规则，根据某种算法完成限速、整形等功能</li><li><code>class</code>: 用户定义的流量类别</li><li><code>classifier</code>: 分类器，分类规则。传统的 tc 方案中，classifier 和 action modules 之间是分开的，每个分类器能 attach 多个 action，当匹配到这个分类器时这些 action 就会执行。除此之外，它不仅能够读取 skb 元数据和包数据，还能任意<strong>修改</strong>这两者，最后结束 tc 处理过程，返回一个返回码</li><li><code>action modules</code>: 要对包执行什么动作</li></ul><p>当需要给某个网络设备挂载 tc BPF 程序时，需要执行以下操作</p><ul><li>为网络设备创建 <code>qdisc</code></li><li>创建 <code>class</code>，并 attach 到 <code>qdisc</code></li><li>创建 <code>filter (classifier)</code>，并 attach 到 <code>qdisc</code><ul><li>filter  对网络设备上的流量进行分类，并将包分发（dispatch）到前面定义的不同 class</li><li>filter 会对每个包进行过滤，返回下列值之一<ul><li><code>0</code>: 表示 mismatch，如果后面有其他 filters，则继续向下执行</li><li><code>-1</code>: 执行这个 filter 上的默认 <code>class</code></li><li><code>其他</code>: 表示一个 classid，接下来系统会将数据包送到这个 <code>class</code></li></ul></li></ul></li><li>给 <code>filter</code> 添加 <code>action</code><ul><li>例如，将选中的包丢弃（drop），或者将流量镜像到另一个网络设备等等</li></ul></li></ul><p><strong>cls_bpf classifier</strong></p><p>cls_bpf 是一种分类器，相比于其它类型的 tc 分类器，它有一个优势：能够使用 direct-action 模式</p><p>Cilium 中就使用了 cls_bpf 分类器，它在部署 cls_bpf 服务时，对于给定的 hook 点只会 attach 一个程序，且用的正是 direct-action模式</p><p><strong>direction-action</strong></p><p>如上文所说，传统 tc BPF 程序的执行模式是 classifier 分类和 action modules 执行是分开的，一个 classifier attach 多个 actions，classifier 负责匹配流量，然后将匹配到的流量交给 action 执行</p><p>但是对于很多场景，eBPF classifier 已经有足够的能力完成完成任务处理，无需再 attach 额外的 qdisc 或 class 了</p><p>所以，为了</p><ul><li>避免因套用 tc 原有流程而引入一个功能单薄的 action</li><li>简化那些 classifier 独自就能完成所有工作的场景</li><li>提升性能</li></ul><p>社区为 tc 引入了一个新的 flag：direct-action，简写 da。 这个 flag 用在 filter 的 attach time，告诉系统： classifier 的返回值应当被解读为 action 类型的返回值。这意味着 classifier 加载的 eBPF 现在可以返回 action code 了。比如在要求丢包的场景中，就不需要再引入一个 action 执行 drop 操作，可以直接在 classifier 中完成</p><p>这样带了的好处</p><ul><li>性能提升，因为 tc 子系统无需再调用到额外的 action 模块 ，而后者是在内核之外的</li><li>程序结构更加简单易用</li></ul><p><strong>tc BPF 程序返回码</strong></p><ul><li><code>TC_ACT_UNSPEC</code>: 结束当前程序的处理，不指定接下来的操作（内核会根据情况执行下一步操作）。对于以下三种情况，默认操作分别为。<ul><li>当 <code>cls_bpf</code> 被 attach 了多个 tc BPF 程序时，继续下一个 tc BPF 程序</li><li>当 <code>cls_bpf</code> 被 attach 了 offloaded tc BPF 程序（和 offloaded XDP 程序类似）时，cls_bpf 会返回 <code>TC_ACT_UNSPEC</code>，内核会执行下一个没有被 offloaded BPF 程序（一张 NIC 只能 offloaded 一个程序）</li><li>当只有单个 tc BPF 程序时，返回该 code 通知内核继续执行 skb 处理，不会带来其他副作用</li></ul></li><li><code>TC_ACT_OK</code>: 结束当前程序的处理，并告诉内核下一个执行的 tc BPF 程序</li><li><code>TC_ACT_SHOT</code>: 通知内核丢弃数据包，返回 <code>NET_XIT_DROP</code> 给调用方表示包被丢弃</li><li><code>TC_ACT_STOLEN</code>: 通知内核丢弃数据包，返回 <code>NET_XMIT_SUCCESS</code> 给调用方，假装这个包被正确发送</li><li><code>TC_ACT_REDIRECT</code>: 使用这个返回码并加上 <code>bpf_redirect()</code> 辅助函数，允许重定向一个 <code>skb</code> 到同一个或另一个网络设备的 ingress 或 egress 路径。对目标网络设备没有额外的要求，只要本身是 一个网络设备就行了，在目标设备上不需要运行 <code>cls_bpf</code> 实例或其他限制</li></ul><p><strong>tc 使用案例</strong></p><ul><li><code>为容器落实策略</code>: 在容器网络中，veth-pair 一端连接着 Namespace，另一段连接着 Host。容器内所有的网络都会经过 Host 端的 veth 设备，因此在 veth 设备上的 tc ingress 和 egress hook 点可以 attach tc BPF 程序。此时容器内发出的流量都会经过 veth 的 tc ingress hook，进入容器的流量都会经过 veth 的 tc egress hook（对于 veth 这样的虚拟设备，XDP 在该场景下并不合适，因为内核在这里只操作 skb，而通用 XDP 有几个限制，导致无法操作克隆的 skb。而且 XDP 无法处理 egress 流量）</li><li><code>转发和负载均和</code>: 使用场景和 XDP 很类似，只是目标更多的是在东西向容器流量而不是南北向。tc 还可以在 egress 方向使用，例如对容器的 egress 流量做 NAT 和 负载均衡，整个过程对容器是透明的。由于在内核网络栈的实现中，egress 流量已经是 sk_buff 形式的了，因此很适合 tc BPF 对其进行重写（rewrite）和重定向（redirect）。 使用 bpf_redirect() 辅助函数，BPF 就可以接管转发逻辑，将包推送到另一个网络设 备的 ingress 或 egress 路径上</li><li><code>流抽样和监控</code>: tc BPF 程序可以同时 attach 到 ingress 和 egress，另外，这两个 hook 都在（通用）网络栈的更低层，这使得可以监控每台节点 的所有双向网络流量。Cilium 中会使用 tc BPF 能够给数据包添加自定义 annotations 的特性，对被 drop 的包打上 annotations，标记它所属的容器以及被 drop 的原因，提供了丰富的信息</li></ul><h2 id="Cilium-与-eBPF"><a href="#Cilium-与-eBPF" class="headerlink" title="Cilium 与 eBPF"></a>Cilium 与 eBPF</h2><p>上文已经描述了 Pod 中的流量是如何顺利发送到 Host 的 veth 上的，接下来会介绍在 Cilium 中流量的完整路径</p><p><em>当前环境基于 Legacy Host Routing 模式</em></p><p>在开始之前，先观察一下当集群中安装了 Cilium 之后，会多出来哪些东西，这里可以直接进入 cilium agent 的 Pod，执行网络命令，因为 agent 使用的是 <code>hostNetwork: true</code>，所以它和主机是共享网络的，因此在 Pod 中查看到的网络设备也就是主机上的网络设备</p><pre><code class="shell">$ k exec -it -n kube-system cilium-qv2cb -- ip a18: cilium_net@cilium_host: &lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP group default qlen 1000    link/ether fa:f3:f0:8d:5e:ae brd ff:ff:ff:ff:ff:ff    inet6 fe80::f8f3:f0ff:fe8d:5eae/64 scope link       valid_lft forever preferred_lft forever19: cilium_host@cilium_net: &lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP group default qlen 1000    link/ether 3e:87:e2:f3:58:47 brd ff:ff:ff:ff:ff:ff    inet 10.0.1.118/32 scope link cilium_host       valid_lft forever preferred_lft forever    inet6 fe80::3c87:e2ff:fef3:5847/64 scope link       valid_lft forever preferred_lft forever</code></pre><pre><code class="shell">$ k exec -it -n kube-system cilium-qv2cb -- iptables -LChain CILIUM_FORWARD (1 references)target     prot opt source               destinationACCEPT     all  --  anywhere             anywhere             /* cilium: any-&gt;cluster on cilium_host forward accept */ACCEPT     all  --  anywhere             anywhere             /* cilium: cluster-&gt;any on cilium_host forward accept (nodeport) */ACCEPT     all  --  anywhere             anywhere             /* cilium: cluster-&gt;any on lxc+ forward accept */ACCEPT     all  --  anywhere             anywhere             /* cilium: cluster-&gt;any on cilium_net forward accept (nodeport) */Chain CILIUM_INPUT (1 references)target     prot opt source               destinationACCEPT     all  --  anywhere             anywhere             mark match 0x200/0xf00 /* cilium: ACCEPT for proxy traffic */Chain CILIUM_OUTPUT (1 references)target     prot opt source               destinationACCEPT     all  --  anywhere             anywhere             mark match 0xa00/0xfffffeff /* cilium: ACCEPT for proxy return traffic */MARK       all  --  anywhere             anywhere             mark match ! 0xe00/0xf00 mark match ! 0xd00/0xf00 mark match ! 0xa00/0xe00 /* cilium: host-&gt;any mark as from host */ MARK xset 0xc00/0xf00</code></pre><p>可以看到分别多出来了 2 个网络设备 <code>cilium_host</code> 和 <code>cilium_net</code>（如果使用了 overlay 模式，还会有 <code>cilium_vxlan</code>），以及 3 条 iptables 规则 <code>CILIUM_FORWARD</code>, <code>CILIUM_INPUT</code> 和 <code>CILIUM_OUTPUT</code></p><p><strong>cilium_host 和 cilium_net</strong></p><p><code>cilium_host</code> 和 <code>cilium_net</code> 是一对 veth pair</p><p>通过 <code>route -n</code> 查看路由表发现，<code>cilium_host</code> 负责处理本机 pod 流量之间的路由</p><pre><code>10.0.1.0        10.0.1.118        255.255.255.0   UG    0      0        0 cilium_host10.0.1.118        0.0.0.0         255.255.255.255 UH    0      0        0 cilium_host</code></pre><p>那么当流量从同节点的 Pod1 发送到 Pod2 时，需要用到这个路由表吗？答案是不需要，之前讨论 tc 的时候有说到，tc 可以使用辅助函数获取路由表的内容，并且支持直接 redirect 流量到另一张网卡，那么只需要在 Pod1 的 lxc (veth pair) 上 attach tc ingress BPF 程序就可以直接将流量发送到 Pod2 的 lxc</p><p>那么 <code>cilium_host</code> 这个虚拟网络设备的用处是什么呢？那就是用来接收跨节点的 Pod 或者是外网到 Pod 的流量。当有非本地发送过来到 Pod 的流量时，Host 上的 eth0 网卡上的 tc ingress 程序会对流量做某些处理（如果是 Pod 访问外网的回程报文则是 SNAT 还原，如果是 Pod 之间的访问则不需要操作），处理完成后将数据报文交给内核路由系统，最终送到 <code>cilium_host</code> 中，再由 cilium_host 上的 tc BPF 程序将流量 redirect 到 Pod 的 lxc</p><p>除此之外，Cilium 还在 Host 的出口网卡上 attach 了 tc ingress 和 egress hook</p><h3 id="tc-BPF-Program"><a href="#tc-BPF-Program" class="headerlink" title="tc BPF Program"></a>tc BPF Program</h3><p>以上了解了在 Cilium 中通信会依赖三个网络设备，分别是 <code>eth0</code>, <code>cilium_host</code> 和 <code>lxc</code></p><p>那么在来看下这些设备上都被 attach 了哪些 tc hook</p><p>通过 <code>tc filter show dev &lt;dev_name&gt; (ingress|egress)</code> 命令可以查看到 attach 的 program 名称</p><p><strong>lxc</strong></p><ul><li>ingress: <code>to-container</code></li><li>egress: <code>from-container</code></li></ul><p><code>to-container</code></p><ul><li>提取数据包中的 identity 信息（该信息由 Cilium 设置，包含了数据包所属的 namespace，service，pod 和 container 信息）</li><li>检查接收容器 ingress policy</li><li>如果检查通过，将数据包发送到 lxc 的对端，也就是 pod 的网卡</li></ul><p><code>from_container</code></p><ul><li>如果访问的是 service ip，先对 service ip 做负载均衡</li><li>执行 DNAT，将 dst_ip 替换成 pod ip</li><li>将数据包交给内核路由系统</li></ul><p><strong>cilium_host</strong></p><ul><li>ingress: <code>to-host</code></li><li>egress: <code>from-host</code></li></ul><p><code>to-host</code></p><ul><li>解析这个包所属的 identity，并存储到包的结构体中</li><li>验证 dst 的 ingress policy</li></ul><p><code>from-host</code></p><ul><li>尾调用 dst endpoint 的 ingress BPF 程序 (to-container)</li></ul><p><strong>eth0</strong></p><ul><li>ingress: <code>from-netdev</code></li><li>egress: <code>to-netdev</code></li></ul><p><code>from-netdev</code></p><ul><li>解析这个包所属的 identity (还原SNAT)，并存储到包的结构体中</li><li>将包交给内核转发</li></ul><p><code>to-netdev</code></p><ul><li>在某些场景下对数据包做 SNAT</li></ul><p>因此基于以上的前提，大概可以画出集群内的流量路径</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/cilium_legacy_data_path.png" >        </sapn>      </p><blockquote><p>上图引用自 <a href="https://www.infoq.cn/article/p9vg2g9t49kpvhrckfwu">https://www.infoq.cn/article/p9vg2g9t49kpvhrckfwu</a></p></blockquote><ol><li>跨节点 Pod to Pod<br>  Pod 发出的流量经过 lxc 上 tc ingress hook 的处理后发给内核协议栈进行路由查找，然后通过 eth0 的 egress 发出到达 Node2 的 eth0<br>  Node2 接收流量后，先经过 Node2 物理口 eth0 的 tc ingress hook 的 eBPF 处理，然后送给 kernel stack 进行路由查找，发给 cilium_host 接口，流量经过 cilium_host 的 tc egress hook 点后，最后 redirect 到目的 Pod 的宿主机 lxc 接口上，最终送给目的 pod。</li><li>同节点 Pod to Pod<br>  同节点 Pod 之间的流量可以通过 lxc 接口上的 tc ingress hook 直接 redirect 到目标 Pod，这种方法可以绕过内核网络协议栈，将数据包送往目标 Pod 的 lxc 接口，进而送给目标 Pod</li><li>NodePort Local Endpoint<br>  流量从 Node2 的 eth0 进入后，经过 tc ingress hook 的 DNAT 处理后，将流量交给内核协议栈路由，内核将数据包转发给 cilium_host 接口，cilium_host 的 tc egress hook 处理后将流量 redirect 到 Pod 的 lxc 接口，进而送给 Pod<br>  回程数据包经过 ；xc 的 tc ingress hook 反 DNAT 后，将流量重定向到 eth0，过程不经过 kernel，最后经过 eth0 的 tc egress hook 返回给请求者</li><li>NodePort Remote Endpoint<br>  eth0 的 tc ingress hook 进行 DNAT 将 nodeport ip 转换成 pod ip，然后 tc egress hook 经过 SNAT 将数据包的 src ip 换成 node ip 后将流量发出</li><li>Pod 访问外网<br>  Pod 发出流量后，经过 lxc 的 tc ingress hook 处理后，送给内核协议栈进行路由查找，确定需要从 eth0 发出，因此流量回发给物理口 eth0，而经过物理口 eth0 的 tc egress hook 做 SNAT，将源地址转换成 Node 节点的物理接口的 IP 和 Port 发出。<br>  从外网回来的反向流量，经过 Node 节点物理口 eth0 tc ingress hook 进行 SNAT 还原，然后将流量发给内核协议栈查路由，流量流到 cilium_host 接口后，经过 tc egress eBPF 程序，直接识别出是发给 Pod 的流量，将流量直接 redirect 到目的 Pod 的宿主机 lxc 接口上，最终反向流量回到 Pod</li><li>主机访问本 Pod<br>  主机访问 Pod 流量使用 cilium_host 口发出，所以在 tc egress hook 点 eBPF 程序直接 redirect 到目的 Pod 的宿主机 lxc 接口上，最终发给 Pod。<br>  反向流量，从 Pod 发出到宿主机的接口 lxc，经过 tc ingress hook eBPF 识别送给 kernel stack，回到宿主机上</li></ol><p>那么上面提到的 Cilium 在 iptables 中创建的三条链是用来做什么的呢？</p><p>因为在 <code>Legacy</code> 模式中数据包在某些情况下仍然需要进入内核协议栈，因此仍然需要用到 iptables。通过 <code>FORWARD</code> + <code>POSTROUTING</code> 链将数据包从 lxc 接口送到 <code>cilium_host</code>，而 <code>INPUT</code> 链则是为了处理 L7 Network Policy Proxy 的情况，这个部分下面会讲</p><h3 id="BPF-Host-Routing"><a href="#BPF-Host-Routing" class="headerlink" title="BPF Host Routing"></a>BPF Host Routing</h3><p>在 5.10 内核以后，Cilium 新增了 eBPF Host-Routing 功能，该功能更加速了 eBPF 数据面性能，新增了 <code>bpf_redirect_peer</code> 和 <code>bpf_redirect_neigh</code> 两个 redirect 方式</p><ul><li><code>bpf_redirect_peer</code><br>可以理解成 <code>bpf_redirect</code> 的升级版，其将数据包直接送到 veth pair Pod 里面接口 <code>eth0</code> 上，而不经过宿主机的 <code>lxc</code> 接口，这样实现的好处是数据包少进入一次 cpu backlog queue 队列，该特性引入后，路由模式下，Pod -&gt; Pod 性能接近 Node -&gt; Node 性能</li><li><code>bpf_redirect_neigh</code><br>用来填充 pod egress 流量的 src 和 dst mac 地址<br>流量无需经过 kernel 的 route 协议栈<br>处理过程<ul><li>首先会查找路由，<code>ip_route_output_flow()</code></li><li>将 skb 和匹配的路由条目（dst entry）关联起来，<code>skb_dst_set()</code></li><li>然后调用到 <code>neighbor subsystem</code>，<code>ip_finish_output2()</code><ul><li>填充 neighbor 信息，即 <code>src/dst MAC</code> 地址</li><li>保留 <code>skb-&gt;sk</code> 信息，因此物理网卡上的 qdisc 都能访问到这个字段</li></ul></li></ul></li></ul><p>在引入了该特性之后，流量路径发生了较大的变化</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/cilium_bpf_data_path.png" >        </sapn>      </p><p>可以看到在该场景下，除了本机访问本地 Pod，其他流量均不会经过内核协议栈，<code>cilium_host</code> 和 <code>cilium_net</code> 上</p><ol><li>跨节点 Pod to Pod<br>  Pod 发出的流量通过 lxc 的 tc ingress hook 中的 <code>redirect_neigh</code> 直接通过 eth0 的 egress 发出到达 Node2 的 eth0<br>  Node2 的 eth0 接收流量后，先经过 Node2 物理口 eth0 的 tc ingress hook 的 eBPF 处理，然后送给 Pod2 的 lxc 上的 tc ingress hook 中的 <code>redirect_peer</code> 直接把数据送到 Pod2 的 eth0</li><li>同节点 Pod to Pod<br>  不同于 Legacy 模式，直接将数据包送到目的 Pod 的 eth0 网卡</li></ol><p>以下这张图会更加直观的表示在 BPF Host Routing 模式下数据路径的简单</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/ebpf_hostrouting.png" >        </sapn>      </p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><h3 id="Network-Policy"><a href="#Network-Policy" class="headerlink" title="Network Policy"></a>Network Policy</h3><p>Cilium 支持基于 <code>Layer 3/4/7</code> 的网络策略，同时支持 <code>allow</code> 和 <code>deny</code> 两种模式</p><blockquote><p>相同的规则，deny 的优先级更高</p></blockquote><p>Cilium 除了兼容 Kuberentes 的 <code>NetworkPolicy</code> API，也提供了 CRD 用于定义网络策略 <code>CiliumNetworkPolicy</code>（具体字段请看<a href="https://docs.cilium.io/en/stable/policy/intro/#rule-basics">官方文档</a>，这里不做介绍）</p><p>Cilium 提供了三种网络策略模式</p><ul><li><code>default</code>: 默认模式，如果一个 <code>endpoint</code> 设置了一个 <code>ingress</code>，那么不符合这个规则的 ingress 请求都会被拒绝。<code>egress</code> 同理。不过 <code>deny</code> 规则不同，如果一个 <code>endpoint</code> 只设置了 deny，那么只有命中 deny 规则的请求会被拒绝，其他还是会被放行，如果一个 <code>endpoint</code> 没有设置任何 rule，那么它的网络不受任何限制</li><li><code>always</code>: 如果一个 <code>endpoint</code> 没有设置任何 rule，那么它无法访问其他服务，也无法被其他服务访问</li><li><code>never</code>: 不启动 Cilium 的网络策略，所有服务之间都能互相或者和外部访问</li></ul><p><strong>Layer 3</strong></p><ul><li><code>Labels Based</code>: 根据 Pod labels 选择对应的 ip 然后过滤</li><li><code>Services Based</code>: 根据 Service labels 选择对于的 ip 然后过滤</li><li><code>Entities Based</code>: Cilium 内置了以下字段来指定特定的流量<ul><li><code>host</code>: 本节点的流量</li><li><code>remote-node</code>: 集群内其他节点的 Endpoint</li><li><code>cluster</code>: 集群内部 Endpoint</li><li><code>init</code>: 在引导阶段还没有被解析的 Endpoint</li><li><code>health</code>: Cilium 用来检查集群状态的 Endpoint</li><li><code>unmanaged</code>: 不是由 Cilium 管理的 Endpoint</li><li><code>world</code>: 所有外部流量，允许 world 流量相当于允许 <code>0.0.0.0/0</code></li><li><code>all</code>: 以上所有</li></ul></li><li><code>IP/CIDR Based</code>: 基于 IP 过滤</li><li><code>DNS Based</code>: 基于 dns 解析后的 IP 过滤，Cilium 会在 agent 中运行 dns proxy，然后缓存 dns 解析后的 ip list，如果 dns name 满足 allow 或者 deny 规则，那么所有发往这些 ip list 的请求都会被过滤</li></ul><p><strong>Layer 4</strong></p><ul><li><code>Port</code>: 按端口号过滤</li><li><code>Protocol</code>: 按协议过滤，支持 <code>TCP</code>, <code>UDP</code> 和 <code>ANY</code></li></ul><p><strong>Layer 7</strong></p><ul><li><code>HTTP</code>: 支持根据请求 <code>Path</code>, <code>Method</code>, <code>Host</code>, <code>Headers</code> 过滤</li><li><code>DNS</code>: 不同于 Layer 3 中的 <code>DNS Based</code> 过滤，这是直接过滤 DNS 请求（因为是在 Layer 7 的缘故）</li><li><code>Kafka</code>: 支持根据 <code>Role</code> (<code>produce</code>, <code>consume</code>), <code>Topic</code>, <code>ClientID</code>, <code>APIKey</code> 和 <code>APIVersion</code> 过滤</li></ul><p>注意：使用了 Layer 7 Network Policy 之后，所有请求都会被转发到 Cilium agent 的 Proxy 中，该 Proxy 由 Envoy 提供，在 Legacy Host Routing 模式下，数据路径会变成下面这样</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/cilium_0_0_1/cilium_bpf_endpoint.svg" >        </sapn>      </p><p>上图描述了 Endpoint to Endpoint 使用了 Layer 7 NetworkPolicy 时的数据路径，上半部分为默认情况下的数据路径，下半部分为使用了 socket 增强之后数据的路径</p><h3 id="Bandwidth-Manager"><a href="#Bandwidth-Manager" class="headerlink" title="Bandwidth Manager"></a>Bandwidth Manager</h3><p>Cilium 还提供了带宽限制的功能</p><p>可以通过在 Pod annotation 中添加 <code>kubernetes.io/ingress-bandwidth: &quot;10M&quot;</code> 或者 <code>kubernetes.io/egress-bandwidth: &quot;10M&quot;</code> 来限制单个 Pod 的带宽。这样 Pod 的出口和入口带宽会被限制在 <code>10Mbit/s</code></p><p>不过目前该功能还无法和 Layer 7 NetworkPolicy 同时工作，两者都使用的话会导致带宽限制失效</p><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><p>这部分会介绍一些工具来方便排查网络问题，主要是 cilium cli 的使用</p><p>可以 exec 到 cilium pod 中使用 cilium 命令行进行调试</p><p><strong>查看集群网络状态</strong></p><p><code>cilium status</code></p><pre><code class="shell">KVStore:                Ok   DisabledKubernetes:             Ok   1.21+ (v1.21.2-eks-0389ca3) [linux/amd64]Kubernetes APIs:        [&quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;, &quot;cilium/v2::CiliumEndpoint&quot;, &quot;cilium/v2::CiliumNetworkPolicy&quot;, &quot;cilium/v2::CiliumNode&quot;, &quot;core/v1::Namespace&quot;, &quot;core/v1::Node&quot;, &quot;core/v1::Pods&quot;, &quot;core/v1::Service&quot;, &quot;discovery/v1::EndpointSlice&quot;, &quot;networking.k8s.io/v1::NetworkPolicy&quot;]KubeProxyReplacement:   DisabledCilium:                 Ok   1.10.3 (v1.10.3-4145278)NodeMonitor:            Listening for events on 8 CPUs with 64x4096 of shared memoryCilium health daemon:   OkIPAM:                   IPv4: 1/254 allocated from 10.0.1.0/24,BandwidthManager:       DisabledHost Routing:           LegacyMasquerading:           DisabledController Status:      30/30 healthyProxy Status:           OK, ip 10.0.1.118, 0 redirects active on ports 10000-20000Hubble:                 Ok   Current/Max Flows: 4095/4095 (100.00%), Flows/s: 12.47   Metrics: OkEncryption:             DisabledCluster health:         5/5 reachable   (2021-10-18T15:01:07Z)</code></pre><p><strong>抓包</strong></p><p><code>cilium monitor</code></p><p>在 BPF 场景下因为数据路径和传统网络栈发生较大改变，如果不熟悉这套模式在使用比如 <code>tcpdump</code> 等工具抓包调试时可以产生一些问题。（比如在 BPF Host Routing 下，lxc 接口是无法抓到回程报文的）</p><p>好在 cilium 提供了一套工具用于分析数据包，方便开发者进行问题排查</p><p><strong>NetworkPolicy Tracing</strong></p><p>如果集群中使用了较多的网络策略，有可能导致某些情况下请求命中了意料之外的 NetworkPolicy 导致失败</p><p>Cilium 也提供了 policy tracing 的功能用来追踪请求命中 <code>NetworkPolicy</code> 的情况</p><p><code>cilium policy trace</code></p><pre><code class="shell"># 验证从 default ns 下 xwing 发出的流量，到 default ns 下带有 deathstar label 的 endpoint 的 80 端口的流量会命中哪些 cnp$ cilium policy trace --src-k8s-pod default:xwing -d any:class=deathstar,k8s:org=expire,k8s:io.kubernetes.pod.namespace=default --dport 80level=info msg=&quot;Waiting for k8s api-server to be ready...&quot; subsys=k8slevel=info msg=&quot;Connected to k8s api-server&quot; ipAddr=&quot;https://10.96.0.1:443&quot; subsys=k8s----------------------------------------------------------------Tracing From: [k8s:class=xwing, k8s:io.cilium.k8s.policy.serviceaccount=default, k8s:io.kubernetes.pod.namespace=default, k8s:org=alliance] =&gt; To: [any:class=deathstar, k8s:org=empire, k8s:io.kubernetes.pod.namespace=default] Ports: [80/ANY]Resolving ingress policy for [any:class=deathstar k8s:org=empire k8s:io.kubernetes.pod.namespace=default]* Rule &#123;&quot;matchLabels&quot;:&#123;&quot;any:class&quot;:&quot;deathstar&quot;,&quot;any:org&quot;:&quot;empire&quot;,&quot;k8s:io.kubernetes.pod.namespace&quot;:&quot;default&quot;&#125;&#125;: selected    Allows from labels &#123;&quot;matchLabels&quot;:&#123;&quot;any:org&quot;:&quot;empire&quot;,&quot;k8s:io.kubernetes.pod.namespace&quot;:&quot;default&quot;&#125;&#125;      Labels [k8s:class=xwing k8s:io.cilium.k8s.policy.serviceaccount=default k8s:io.kubernetes.pod.namespace=default k8s:org=alliance] not found1/1 rules selectedFound no allow ruleIngress verdict: deniedFinal verdict: DENIED</code></pre><p><strong>Dns Based NetworkPolicy Debug</strong></p><p>随着 DNS 查询结果的变化，FQDN Policy 的拦截结果也在变，导致这部分难以 debug</p><p>可以使用 <code>cilium fqdn cache list</code> 查看当前 dns proxy 中缓存了哪些 dns-ip</p><p>如果流量是允许的，那么这些 IPs 应该存在于 local identities，<code>cilium identity list | grep &lt;IP&gt;</code> 应该返回结果</p><p><strong>Hubble</strong></p><p>Cilium 还提供了 Hubble 用来加强网络监控和报警，Hubble 提供了以下功能</p><ul><li>可视化<ul><li>服务的调用关系</li><li>支持 HTTP，kafka 协议</li></ul></li><li>监控和报警<ul><li>网络监控<ul><li>过去一段时间内是否有网络通信失败，通信失败的原因</li><li>应用监控<ul><li><code>4xx</code> 和 <code>5xx</code> HTTP Response code 出现的频率，出现在哪些服务</li><li>http 调用的 <code>latency</code></li></ul></li></ul></li><li>安全性监控<ul><li>是否有因为 NetworkPolicy Deny 失败的请求</li><li>哪些服务，有接收集群外发来的请求</li><li>哪些服务要求解析某个特定的 dns</li></ul></li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://docs.cilium.io/en/stable/">Cilium Documentation</a></li><li><a href="https://www.infoq.cn/article/p9vg2g9t49kpvhrckfwu">网易轻舟对 CIlium 容器网络的探索和实践</a></li><li><a href="https://linuxplumbersconf.org/event/7/contributions/674/">Kubernetes service load-balancing at scale with BPF &amp; XDP</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;本文基于 &lt;code&gt;Cilium 1.10&lt;/code&gt; 编写&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;这篇文章会和大家分享一些 Cilium 相关的知识，包括以下几个主要部分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;eBPF 的基础知识&lt;/li&gt;
&lt;li&gt;Cilium 对 eBPF 的应用&lt;</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="cilium" scheme="https://domc.me/tags/cilium/"/>
    
    <category term="cni" scheme="https://domc.me/tags/cni/"/>
    
    <category term="network" scheme="https://domc.me/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Gateway API</title>
    <link href="https://domc.me/2021/05/10/k8s_gateway_api/"/>
    <id>https://domc.me/2021/05/10/k8s_gateway_api/</id>
    <published>2021-05-10T00:14:15.000Z</published>
    <updated>2025-04-11T02:56:57.521Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是-Kubernetes-Gateway-API"><a href="#什么是-Kubernetes-Gateway-API" class="headerlink" title="什么是 Kubernetes Gateway API"></a>什么是 Kubernetes Gateway API</h2><p>Kubernetes Gateway API（以下简称 <code>KGA</code>）是由 sig-network 小组提出的，用于帮助用户在 K8S 中对服务网络进行建模的资源的集合。</p><p>它通过提供标准化的接口，允许各个供应商提供自己的实现方式，来帮助用户规划集群内的服务网络。</p><h2 id="演进历史"><a href="#演进历史" class="headerlink" title="演进历史"></a>演进历史</h2><p>最初 K8S 提供了 <code>Ingress</code> API 来让用户可以定义集群的入口网络。随着流量管理（熔断，限流，灰度）等需求的增加，最初的 <code>Ingress</code> 的定义已经无法满足，因此社区开始分裂出了不同的实现方式。</p><ul><li>利用 <code>CRD</code> 扩展 <code>Ingress</code>，例如：<a href="https://dave.cheney.net/paste/ingress-is-dead-long-live-ingressroute.pdf">https://dave.cheney.net/paste/ingress-is-dead-long-live-ingressroute.pdf</a></li><li>利用 <code>Annotation</code> 扩展 <code>Ingress</code>，例如：<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">nginx-controller</a></li></ul><p>这些方式虽然能解决现有的问题，但也导致了</p><ul><li>没有统一的标准，各个 ingress-controller 按自己的方式实现</li><li>没有可移植性</li></ul><p>因此 KGA 应运而生，KGA 需要能让用户很方便的使用以下功能</p><h3 id="流量管理"><a href="#流量管理" class="headerlink" title="流量管理"></a>流量管理</h3><ul><li>HTTP 流量能够按 <code>Header</code>, <code>URI</code> 路由</li><li>流量按权重路由</li><li>流量镜像</li><li>TCP 和 UDP 等路由</li></ul><h3 id="面向角色的设计"><a href="#面向角色的设计" class="headerlink" title="面向角色的设计"></a>面向角色的设计</h3><ul><li>将 Routing 和 Service 的角色分离</li></ul><h3 id="扩展性"><a href="#扩展性" class="headerlink" title="扩展性"></a>扩展性</h3><ul><li>支持层级配置，用户可以在上层配置中添加或覆盖下层配置中的属性</li></ul><h3 id="灵活一致性"><a href="#灵活一致性" class="headerlink" title="灵活一致性"></a>灵活一致性</h3><ul><li>KGA 提供了三种级别的 API<ul><li>core<ul><li>必须实现</li></ul></li><li>extended<ul><li>可以不实现，如果实现必须保证移植性</li></ul></li><li>custom<ul><li>各个 controller 可以自定义，且不需要提供可移植保证</li></ul></li></ul></li></ul><h2 id="KGA"><a href="#KGA" class="headerlink" title="KGA"></a>KGA</h2><p>为满足上述功能，KGA 提供了以下几种 API</p><ul><li><p><strong>GatewayClass</strong></p><p>指定 KGA 的实现方式，（比如 <code>istio</code> or <code>nginx</code>），类似于 <code>StorageClassess</code></p></li><li><p><strong>Gateway</strong></p><p>指定 Route 应该使用哪个 GatewayClass</p></li><li><p><strong>Route</strong></p><p>定义路由的具体规则，有 <code>HTTPRoute</code>, <code>TCPRoute</code>, <code>UDPRoute</code></p><p>指定符合规则的流量应该路由到哪个 <code>Service</code></p></li></ul><p>关于各个 API 的具体字段，可以查看该<a href="https://gateway-api.sigs.k8s.io/api-types/gatewayclass/">文档</a></p><p>        <span class="lazyload-img-span">        <img              data-src="https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/gateway-api-resources.png" >        </sapn>      </p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>有以下常用的情况</p><p>foo 小组需要将多个服务部署在 <code>foo</code> namespace 中，并要求按照 http 请求能按照 uri 路由到不同的 Service</p><pre><code class="yaml">kind: HTTPRouteapiVersion: networking.x-k8s.io/v1alpha1metadata:  name: foo-route  namespace: foo  labels:    gateway: external-https-prodspec:  hostnames:  - &quot;foo.example.com&quot;  rules:  - matches:    - path:        type: Prefix        value: /login    forwardTo:    - serviceName: foo-auth      port: 8080  - matches:    - path:        type: Prefix        value: /home    forwardTo:    - serviceName: foo-home      port: 8080  - matches:    - path:        type: Prefix        value: /    forwardTo:    - serviceName: foo-404      port: 8080</code></pre><p>bar 小组将服务部署在 <code>bar</code> namespace 中，并需要 canary 发布他们的服务，将流量按照 9:1 的比例分别发送到, bar-v1 和 bar-v2，且当请求的 Header 中带有 <code>env: canary</code> 时，将流量发布到 bar-v2</p><p>        <span class="lazyload-img-span">        <img              data-src="https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/httproute.png" >        </sapn>      </p><pre><code class="yaml">kind: HTTPRouteapiVersion: networking.x-k8s.io/v1alpha1metadata:  name: bar-route  namespace: bar  labels:    gateway: external-https-prodspec:  hostnames:    - &quot;bar.example.com&quot;  rules:    - forwardTo:        - serviceName: bar-v1          port: 8080          weight: 90        - serviceName: bar-v2          port: 8080          weight: 10    - matches:        - headers:            values:              env: canary      forwardTo:        - serviceName: bar-v2          port: 8080</code></pre><p>当两个团队创建好了 <code>HTTPRoute</code> 后，该如何将入口流量匹配到这些规则上呢？这里就需要用到 <code>Gateway</code> 了</p><p>infra 团队可以创建如下 <code>Gateway</code> 来绑定 <code>Routes</code></p><pre><code class="yaml">kind: GatewayapiVersion: networking.x-k8s.io/v1alpha1metadata:  name: prod-webspec:  gatewayClassName: acme-lb  listeners:    - protocol: HTTPS      port: 443      routes:        kind: HTTPRoute        selector:          matchLabels:            gateway: external-https-prod        namespaces:          from: All      tls:        certificateRef:          name: admin-controlled-cert</code></pre><p>可以看到，<code>Gateway</code> 中使用 <code>selector.matchLabels</code> 来选中所有 namespace 下带有 <code>gateway: external-https-prod</code> Label 的 <code>HTTPRoute</code></p><p>同时 <code>Gateway</code> 中指定了 <code>gatewayClassName</code> 为 <code>acme-lb</code>，所以这些 route 规则就会被同步到 acme-lb 上</p><p>因此只要将 <code>foo.example.com</code> 和 <code>bar.example.com</code> 的 dns 解析到 acme-lb 的 externalIP 上，就可以完成流量的路由</p><p>以下是这些 API 的拓补图<br>        <span class="lazyload-img-span">        <img              data-src="https://gateway-api.sigs.k8s.io/images/schema-uml.svg" >        </sapn>      </p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/">https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</a></li><li><a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io/</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是-Kubernetes-Gateway-API&quot;&gt;&lt;a href=&quot;#什么是-Kubernetes-Gateway-API&quot; class=&quot;headerlink&quot; title=&quot;什么是 Kubernetes Gateway API&quot;&gt;&lt;/a&gt;什么是 Kube</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="network" scheme="https://domc.me/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>记一次 WebSocket 连接泄露排查</title>
    <link href="https://domc.me/2019/12/29/websocket_leak/"/>
    <id>https://domc.me/2019/12/29/websocket_leak/</id>
    <published>2019-12-29T16:34:34.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>之前重构了某个 <code>WebSocket</code> 服务，在运行了一段时间之后有同事反应该服务卡顿，时常获取不到数据。</p><p>于是我尝试开始复现该情况。</p><p>打开浏览器跳转到相应的页面后，发现数据立刻就加载了出来，但是多次刷新页面之后，就开始出现无法加载数据的情况。打开控制台，发现报了如下的错误。</p><pre><code class="bash">WebSocket connection to &#39;ws://xxxxx/&#39; failed: Error in connection establishment: net::ERR_CONNECTION_FAILED</code></pre><p>一般来说出现该问题可能是因为网络问题或者连接超时引起的，在排除了自身网络问题的前提下，我调大了 WebSocet 的连接超时时间。但还是出现了上述的情况。</p><p>接着我猜想是不是我们的长连接网关出了问题。于是我查看了网关的日志，发现网关压根就没接收到出现 <code>ERR_CONNECTION_FAILED</code> 错误的连接。</p><p>于是我确定是之前重构的项目问题。</p><p>我开始查看 <code>Pod</code> 的日志，发现在连接建立成功并断开的时候没有将日志打印出来。</p><blockquote><p>该项目用了 <code>echo</code> 框架，并使用了 <code>LogMiddleware</code> ，它会在每条请求结束后将请求信息打印出来。</p></blockquote><p>那应该就是在客户端断开 <code>WebSocket</code> 连接的时候，服务端并没有断开，导致了 <code>WebSocket</code> 连接泄露。</p><p>进入到 <code>Pod</code> 中查看连接数</p><pre><code class="bash">netstat -na|grep ESTABLISHED|wc -l&gt; 19</code></pre><p>成功建立连接之后</p><pre><code class="bash">netstat -na|grep ESTABLISHED|wc -l&gt; 23</code></pre><p>客户端退出之后</p><pre><code class="bash">netstat -na|grep ESTABLISHED|wc -l&gt; 21</code></pre><p>显然是连接泄露了。浏览器会对同一域名的连接数做限制，所以有可能是因为连接泄露导致连接数过大，使之后的连接无法建立才出现的 <code>ERR_CONNECTION_FAILED</code> 的情况。</p><p>于是开始定位代码中的问题，以下是大概的代码逻辑。</p><p>这是一个和 <code>K8S</code> 管理平台相关的项目。服务会监听 <code>Pod</code> 的状态变化，并通过 <code>WebSocket</code> 将变化结果通知给前端。</p><pre><code class="go">func handler(c echo.Context) error &#123;    // ...    // 使用 request.Context() 作为全局 context ctx := c.Request().Context() websocket.Handler(func(ws *websocket.Conn) &#123;  defer ws.Close()  if err := watchPodStatus(ctx, param.Namespace, func(e watch.Event) (bool, error) &#123;    // 将变化发给前端    websocket.Message.Send(ws, ...)        &#125;); err != nil &#123;   websocket.Message.Send(ws, err.Error())  &#125; &#125;).ServeHTTP(c.Response(), c.Request()) return nil&#125;// WatchPodStatus// name: deployment namefunc watchPodStatus(ctx context.Context, namespace string, conditionFunc func(e watch.Event) (bool, error)) error &#123;    // ...    // 这步会列出变化的 pods    // 通过 conditionFunc 方法处理事件    // 注：该方法是阻塞的，除非 ctx 被 cancel 或者 conditionFunc 返回 true 否则不会结束 _, err = watchtools.UntilWithSync(ctx, listAndWatchFunction, &amp;corev1.Pod&#123;&#125;, preconditionFunc, conditionFunc) return err&#125;</code></pre><p>这是我使用了 <code>echo.Request().Context()</code> 作为 <code>watchtools.UntilWithSync</code> 的 context，原本设想的是在请求结束后 context 就会 <code>cancel</code> ，从而终止 <code>watch</code> 事件，并退出整个请求。</p><p>结果 <code>request.Context()</code> ，并没有在前端调用 <code>socket.close()</code> 的时候退出。导致 <code>watch</code> 也没有退出，所以 <code>defer ws.Close()</code> 也就无法触发，导致服务端无法结束该次请求。</p><p>在知道是该原因后问题也好解决了。以 <code>request.Context()</code> 作为父 <code>context</code> 创建一个新的 <code>context</code> 。使用一个 <code>goroutine</code> 监听 <code>WebSocketConnection</code> 的状态。（通过不停的 <code>conn.Receive()</code> ），因为该场景没有客户端向服务端发送数据的场景，所以该调用会被一直阻塞，直到客户端调用 <code>socket.close()</code> 之后， <code>conn.Receive()</code> 会接收到一个错误信息，这时再 <code>cancel()</code> 掉上述的 <code>context</code> 退出 <code>watch</code> 即可，以下是修改后的代码逻辑。</p><pre><code class="go">func handler(c echo.Context) error &#123;    // ...    // 使用 request.Context() 作为全局 context    ctx := c.Request().Context()    cancelctx, cancel := context.WithCancel(ctx)    websocket.Handler(func(ws *websocket.Conn) &#123;        go func()&#123;            for &#123;                // receive 是阻塞方法，如果没有收到消息就会一直等待                // 收到错误说明连接断开了或者出现异常了，这是取消掉 ctx，watch 行为也会推出                if err := websocket.Message.Receive(ws, &amp;res); err != nil&#123;                    cancel()                &#125;            &#125;        &#125;()    defer ws.Close()    if err := watchPodStatus(cancelctx, param.Namespace, func(e watch.Event) (bool, error)&#123;                // 将变化发给前端                websocket.Message.Send(ws, ...)            &#125;); err != nil &#123;    websocket.Message.Send(ws, err.Error())    &#125;    &#125;).ServeHTTP(c.Response(), c.Request())    return nil&#125;</code></pre><p>完！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前重构了某个 &lt;code&gt;WebSocket&lt;/code&gt; 服务，在运行了一段时间之后有同事反应该服务卡顿，时常获取不到数据。&lt;/p&gt;
&lt;p&gt;于是我尝试开始复现该情况。&lt;/p&gt;
&lt;p&gt;打开浏览器跳转到相应的页面后，发现数据立刻就加载了出来，但是多次刷新页面之后，就开始出现</summary>
      
    
    
    
    
    <category term="issues" scheme="https://domc.me/tags/issues/"/>
    
    <category term="websocket" scheme="https://domc.me/tags/websocket/"/>
    
  </entry>
  
  <entry>
    <title>ExitCode: 128 之无任何错误信息</title>
    <link href="https://domc.me/2019/11/21/exit_code_128/"/>
    <id>https://domc.me/2019/11/21/exit_code_128/</id>
    <published>2019-11-21T02:04:24.000Z</published>
    <updated>2025-04-11T02:56:57.521Z</updated>
    
    <content type="html"><![CDATA[<p>今天在使用 <code>Drone CI</code> 构建项目时出现一个异常奇怪的错误。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/exitcode-128/drone-error.png" >        </sapn>      </p><p>在 <code>clone</code> 某分支代码时构建直接失败退出了。</p><p>首先我排查了 <code>rpc error</code>，发现这个错误出现的原因是正在获取 <code>Pod</code> 的 <code>Log</code> 的过程中，Pod 被删除了。</p><p>当时我就觉得很奇怪，就算因为 <code>Step</code> 执行失败要删除 Pod，那也会先停止 <code>getLog</code>，而不是在 getLog 的过程中直接删除 Pod。</p><p>于是使用了 <code>kubectl describe</code> 了这个 Pod，发现该 Container 的 <code>Exit Code</code> 是 128。说明是该 Container 执行时出错，自己退出的。</p><p>但是从上图的日志中来看 Container 应该是执行成功了，并没有出现错误的日志。那为什么该 Container 的退出码是 128 而非 0 呢。</p><p>于是我开始怀疑是不是 <code>git checkout -b master</code> 时出现了错误，于是我在 Docker 容器中模拟了该 Step 的步骤，<code>git checkout -b master | echo $?</code>，发现即使是显示 <code>Already on master</code> 它的退出码也是 0。于是排除了是 git 出错的可能性。</p><p>那么问题出在哪呢？</p><p>经过一段时间排查，我发现只有当 <code>Image</code> 为 <code>registry.cn-hangzhou.aliyuncs.com/xxx/xxxx:xxx</code> 是才会出现这种不正常情况。</p><p>原来是因为我们集群里还运行着另一个服务，该服务会在 <strong>Pod 被创建时</strong>判断 Image 的类型，将这种使用了公网地址的 Image，替换成使用内网地址 <code>registry-vpc.cn-hangzhou.aliyuncs.com/xxx/xxxx:xxx</code>。再加上我刚好对 <code>drone-runner-kube</code> 做过一些<a href="https://github.com/domgoer/drone-runner-kube/commit/d2b8fd19b69206ece37e00754b3d1b92fb58ad11">优化</a>（提前设置初始镜像，减少 pod update 次数）。导致了这两个服务冲突。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/exitcode-128/steps.jpg" >        </sapn>      </p><p>在 runner 创建 pod 时，先将 image 设为了公网地址（Step 1），接着 image 被另一个服务设置成了内网地址，并提交到 k8s 中运行了。然后 runner 通过对比发现 pod 的 image 不是自己刚开始设置的 image，于是就 update 了这个 pod（Step 4）。</p><p>因为 Update Image 的缘故导致了该 Step 没有按期望运行。所以出现了这个默默奇妙的错误。</p><p>解决方法：将 Image 的初始地址设置成内网地址，这样另一个服务就不会去修改该 Pod 的 Image，也就不会出现该错误。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天在使用 &lt;code&gt;Drone CI&lt;/code&gt; 构建项目时出现一个异常奇怪的错误。&lt;/p&gt;
&lt;p&gt;
        &lt;span class=&quot;lazyload-img-span&quot;&gt;
        &lt;img   
           data-src=&quot;/imag</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="issues" scheme="https://domc.me/tags/issues/"/>
    
    <category term="ci/cd" scheme="https://domc.me/tags/ci-cd/"/>
    
    <category term="drone" scheme="https://domc.me/tags/drone/"/>
    
    <category term="devops" scheme="https://domc.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>如何将Drone CI调度到Virtual Kubelet</title>
    <link href="https://domc.me/2019/11/18/drone_virtual_kublet/"/>
    <id>https://domc.me/2019/11/18/drone_virtual_kublet/</id>
    <published>2019-11-18T23:53:30.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是-virtual-kubelet"><a href="#什么是-virtual-kubelet" class="headerlink" title="什么是 virtual kubelet"></a>什么是 virtual kubelet</h2><p>以下是来自 <a href="https://github.com/virtual-kubelet/virtual-kubelet#virtual-kubelet">Virtual Kubelet</a> 项目的文档的中文翻译。</p><blockquote><p>Virtual Kubelet 是开源的 Kubernetes kubelet 的实现，它可以伪装成 Kubelet 将 Kubernetes 连接到其他 API。这样就允许 Node 的背后由其他服务支撑，例如：ACI, AWS Fargate, IoT Edge。Virtual Kubelet 的主要作用是扩展无服务器（Serverless）平台，让它能够与 Kubernetes 通信。</p></blockquote><p>以 <code>阿里云ECI</code> 为例（以下均以 ECI 代指 Virtual Kubelet），ECI 是阿里云的弹性容器实例。可以将每台 ECI 实例看作是一个 <code>Container</code>，所以它的创建和销毁是很廉价的。同时它拥有启动快（秒级）、成本低（按运行的秒数收费）、弹性能力强等特点。通过 Virtual Kubelet 提供的 <a href="https://github.com/virtual-kubelet/virtual-kubelet#current-features">Kubernetes API</a>，ECI 就能和 K8S 交互，我们就可以在 ECI 上执行创建 Pod 或者删除 Pod 等操作。</p><h2 id="使用-virtual-kubelet-执行-drone-任务的好处"><a href="#使用-virtual-kubelet-执行-drone-任务的好处" class="headerlink" title="使用 virtual kubelet 执行 drone 任务的好处"></a>使用 virtual kubelet 执行 drone 任务的好处</h2><p>在使用 virtual kubelet 之前，为了不影响业务的稳定性。我们的 K8S 集群中开了几台固定的 ECS 实例专门给 CI 使用（给这些 Node 打上了污点，所以业务的服务不会调度到上面）。</p><p>这种做法虽然在不影响业务的情况下也保证了 CI 的稳定运行，但是它会造成一定程度的浪费。因为 CI 本身不像大多数业务服务，需要一天 24 小时的运行。CI 的场景是白天需要很多资源，但是到了晚上几乎不消耗任何资源。所以可以说这些机器有接近 1&#x2F;3 的时间是在浪费 💰 的。</p><p>虽说 K8S 本身也提供了动态扩容机制，可以设置很少的固定资源再通过 CA 动态扩容集群来减少资源消耗。但是 CA 的启动速度（分钟级）满足不了 CI 这种时效要求高的场景。</p><h2 id="如何让-Drone-兼容-ECI"><a href="#如何让-Drone-兼容-ECI" class="headerlink" title="如何让 Drone 兼容 ECI"></a>如何让 Drone 兼容 ECI</h2><h3 id="HostPath"><a href="#HostPath" class="headerlink" title="HostPath"></a>HostPath</h3><p>关于 Drone In K8S 的运行模式，可以翻看我之前写的文章 <a href="https://domc.me/2019/10/22/ck34o2dab0007u79ki5yxke0l/">Drone 在 K8S 中执行一次构建都经历了什么</a>。</p><p>简单来说，<code>Drone Server</code> 接收到构建任务后，会在其运行的 <code>Namespace</code>（假设为 CICD）下创建一个 <code>Job</code>，该 Job 会创建一个随机名称的 Namespace，再在创建出来的 Namespace 按配置文件中的顺序执行每个 Step，每个 <code>Step</code> 就是一个 <code>Pod</code>。这些 Pods 之间通过 <code>HostPath</code> 类型的 <code>Volume</code> 来交换文件。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/drone_virtual_kubelet/origin.jpg" >        </sapn>      </p><p>那么问题来了，ECI 是不支持 HostPath Volume 的。它只支持 EmptyDir、NFS 和 ConfigFile（也就是 ConfigMap 和 Secret）。</p><p>所以要如何解决之前提的 Pods 之间使用 HostPath 交换文件的问题呢？首先想到了是通过 <code>Mutating Webhook</code> 将 HostPath 替换成 NFS，这样每个 Pod 之间使用 NFS 共享文件，这样带来了 NFS 文件的清理问题，不像之前 <code>Pipeline</code> 执行完之后可以直接使用 <code>os.Remove(path)</code> 来清理文件，使用 NFS 后需要实现额外 <code>Cornjob</code> 来清理 NFS 上的琐碎的文件。这样便增添了服务之间的关系复杂度。</p><p>好在在浏览 Drone 社区相关信息后发现了 Drone 发布了 1.6 版本。在 1.6 版本之后，Drone 为 K8S 实现了单独的 <a href="https://github.com/drone-runners/drone-runner-kube">Runner</a>。</p><p>与之前执行 Job 的方式不同，新的执行方式是 Drone Server 接收到构建信息后会将构建信息存入基于内存的 <code>Queue</code> 中，runner 会向 Server 拉取构建信息，然后将构建信息解析成<strong>一个</strong> <code>Pod</code>，每个 <code>Step</code> 是一个 <code>Container</code>。为了保证 Step 执行的顺序行（因为 Pod 创建的时候 Container 执行是无序的），Kube-Runner 将每个还未轮到执行的 Step 的 <code>Image</code> 设置成了 <code>placeholder</code>，placeholder 是一个占位 image，该 image 不停地 sleep 不作任何操作。等到前置 Step 执行完成了，Kube-Runner 会将下个要执行的 Step 的 Image 由 placeholder 改成其对应的 image。通过上述操作来完成执行的顺序性。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/drone_virtual_kubelet/latest.jpg" >        </sapn>      </p><p>因为所有的 Step 都在一个 Pod 内，所以它们的数据就可以使用 EmptyDir 来共享。这便解决了之前的 HostPath 兼容性的问题。</p><h3 id="Privileged-Context"><a href="#Privileged-Context" class="headerlink" title="Privileged Context"></a>Privileged Context</h3><p>在执行 CI 时，重要一步就是构建镜像。以 docker 为例。使用 docker 构建镜像就需要用到 <code>Docker Deamon</code>，Docker Deamon 可以使用宿主机上的或者可以在 Container 内启动一个 Docker Deamon，这里就形成了两种不同的模式，也就是 <code>Docker Outside Docker</code> 和 <code>Docker In Docker</code>。</p><p>因为 Docker Outside Docker 需要挂载宿主机的文件，所以自然在这种情况下是无法使用的。而 Docker In Docker 因为需要在容器内启动 Docker Deamon，所以需要 Privileged 权限，遗憾的是目前 ECI 中并不支持 Container 使用 Privileged Context。所有这两种方法在当前情况下都无法有效地构建镜像。</p><p>那么如何解决这种问题？</p><ol><li>通过一个服务将某台主机的 docker.sock 通过 TCP 的方式暴露出来，再通过 TCP 的方式访问 Docker Deamon。</li><li>使用 <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> 来构建镜像。</li></ol><p>这里，我们选用了 kaniko 作为构建工具。</p><h3 id="Knaiko"><a href="#Knaiko" class="headerlink" title="Knaiko"></a>Knaiko</h3><p>        <span class="lazyload-img-span">        <img              data-src="https://raw.githubusercontent.com/GoogleContainerTools/kaniko/master/logo/Kaniko-Logo.png" >        </sapn>      </p><blockquote><p>Knaiko 是从容器或 Kubernetes 集群内部的 Dockerfile 构建容器映像的工具。不依赖 Docker 守护程序，而是完全在用户空间中执行 Dockerfile 中的每个命令。这样就可以在无法轻松或安全地运行 Docker 守护程序的环境（例如标准Kubernetes集群）中构建容器映像。</p></blockquote><p>Kaniko 执行器首先根据 Dockerfile 中的 <code>FROM</code> 一行命令解析基础镜像，按照 Dockerfile 中的顺序来执行每一行命令，在每执行完一条命令之后，会在用户目录空间中产生一个文件系统的快照，并与存储于内存中的上一个状态进行对比，若有改变，则将其认为是对基础镜像进行的修改，并以新层级的形式对文件系统进行增加扩充，并将修改写入镜像的元数据中。在执行完 Dockerfile 中的每一条指令之后， Kaniko 执行器将最终的镜像文件推送到指定的镜像仓库。</p><p>Kaniko 可以在不具有 <code>ROOT</code> 权限的环境下，完全在用户空间中执行解压文件系统，执行构建命令以及生成文件系统的快照等一系列操作，以上构建的过程完全没有引入 docker 守护进程以及CLI的操作。</p><p>到这，构建镜像问题也解决了。接下来就可以将整个构建调度到 ECI 了。</p><h2 id="实际过程中遇到的问题"><a href="#实际过程中遇到的问题" class="headerlink" title="实际过程中遇到的问题"></a>实际过程中遇到的问题</h2><p>虽说上面已经为 Drone 兼容 ECI 的目标做了很多事情，但也只是理论上的。实际在操作中仍然遇到了很多问题。</p><p>因为 Drone 在执行一次构建时需要不断的 Update Pod，而目前看来似乎 ECI 的 Update 机制做的不是很完善，在 Update 过程中出现了很多问题。</p><ol><li><p>在 Update 时，所有 Pod 的 <code>Environment</code> 都丢失。</p><pre><code class="bash">&gt; k exec -it -n beta nginx envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binNGINX_VERSION=1.17.5NJS_VERSION=0.3.6PKG_RELEASE=1~busterKUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_ADDR=172.21.0.1KUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP=tcp://172.21.0.1:443KUBERNETES_SERVICE_HOST=172.21.0.1TESTHELLO=testKUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_SERVICE_PORT=443KUBERNETES_PORT=tcp://172.21.0.1:443TERM=xtermHOME=/root&gt; k edit pod -n beta nginxpod/nginx edited&gt; k exec -it -n beta nginx envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binTERM=xtermHOME=/root</code></pre></li><li><p>Pod 中每个 Container 的 Environment 数量不能超过 92 个。因为 Drone 将每个 Step 执行的关键信息都保存在 Env 中，导致每个 Container 需要包含大量 Env，因此在调度 Pod 到 ECI 上时出现了 <code>ExceedParamError</code>。</p></li><li><p>Pod Update 时 EmptyDir 中的文件会丢失。Drone 通过 EmptyDir 来共享每步获得或者修改的文件，EmptyDir 丢失导致 CI 失败。</p></li><li><p>Pod Update 某个 <code>Image</code> 时，K8S 内显示 Pod 更新成功，但 ECI 的接口返回了失败。</p><pre><code class="bash">Normal   SuccessfulMountVolume  2m5s                 kubelet, eci                    MountVolume.SetUp succeeded for volume &quot;drone-dggd12eq2mlm5zeevff9&quot;Normal   SuccessfulMountVolume  2m5s                 kubelet, eci                    MountVolume.SetUp succeeded for volume &quot;drone-sw7d8bri9rpn0tjr90bp&quot;Normal   SuccessfulMountVolume  2m5s                 kubelet, eci                    MountVolume.SetUp succeeded for volume &quot;default-token-l6jsc&quot;Normal   Started                2m4s (x4 over 2m4s)  kubelet, eci                    Started containerNormal   Pulled                 2m4s                 kubelet, eci                    Container image &quot;registry-vpc.cn-hangzhou.aliyuncs.com/drone-git:latest&quot; already present on machineNormal   Created                2m4s (x4 over 2m4s)  kubelet, eci                    Created containerNormal   Pulled                 2m4s (x3 over 2m4s)  kubelet, eci                    Container image &quot;drone/placeholder:1&quot; already present on machineWarning  ProviderInvokeFailed   104s                 virtual-kubelet/pod-controller  SDK.ServerErrorErrorCode: UnknownErrorRecommend:RequestId: 94D6A5E0-9F90-47EF-99E9-64DFAE37XXXX</code></pre></li><li><p>ECI 中 Pod Update 的策略和 K8S 中的不一致，K8S 中 Update Pod 时，只会 Kill 掉更改过的 Container，并进行替换。而在 ECI 上，会 Kill 掉所有正在运行的 Container，在进行替换。</p><pre><code class="bash">Type    Reason                 Age                 From          Message----    ------                 ----                ----          -------Normal  Pulling                111s                kubelet, eci  pulling image &quot;nginx&quot;Normal  Pulled                 102s                kubelet, eci  Successfully pulled image &quot;nginx&quot;Normal  Pulling                101s                kubelet, eci  pulling image &quot;redis&quot;Normal  Pulled                 97s                 kubelet, eci  Successfully pulled image &quot;redis&quot;Normal  SuccessfulMountVolume  69s (x2 over 112s)  kubelet, eci  MountVolume.SetUp succeeded for volume &quot;test-volume&quot;Normal  Killing                69s                 kubelet, eci  Killing container with id containerd://image-2:Need to kill PodNormal  Killing                69s                 kubelet, eci  Killing container with id containerd://image-3:Need to kill PodNormal  Pulled                 69s (x2 over 111s)  kubelet, eci  Container image &quot;busybox&quot; already present on machineNormal  SuccessfulMountVolume  69s (x2 over 112s)  kubelet, eci  MountVolume.SetUp succeeded for volume &quot;default-token-l6jsc&quot;Normal  Pulling                68s                 kubelet, eci  pulling image &quot;mongo&quot;Normal  Started                46s (x5 over 111s)  kubelet, eci  Started containerNormal  Created                46s (x5 over 111s)  kubelet, eci  Created containerNormal  Pulled                 46s                 kubelet, eci  Successfully pulled image &quot;mongo&quot;</code></pre><p> 感觉它的更新方式就是把 Pod 删了重新创建，这里只更新了 image-2 ，但是它把 image-3 也 Kill 掉了，还重新 Pull 了 image-1 的镜像。</p></li></ol><p>尽管上述问题在我们向阿里云反馈之后，一部分得到修复。但是 Drone 没执行一步都需要 Update Pod 的操作，对 ECI 来说都需要很大的代价。</p><p>所以我想是否能改变 Kube Runner 的执行方式来优化整个 CI 执行的流程。</p><blockquote><p>ps: 还不是要改代码。。。</p></blockquote><p>在 <a href="https://github.com/domgoer/drone-runner-eci">drone-runner-eci</a> 中，我通过在 CI 启动时就确定好所有的镜像，然后在执行每个 <code>Step</code> 时，<code>Attach</code> 到容器中执行 <code>Commands</code> 的方式来避免 <code>Pod Update</code> 的巨大开销。</p><p>但是这样做就要求每个 <code>Image</code> 都必须有 <code>shell</code>，好在我们的配置是中心化管理的，所以改起来还算方便。</p><p>实际执行时，仍然遇到一个尴尬的问题，就是 Pod 的 Spec 过大（每个 Step 大概都有 100 多个 Env，大概有 10 多个 Step），在 virtual kubelet 向阿里云 ECI 提交创建请求时被阿里云的网关拒绝，返回了 414 。。。。</p><p>于是只能接着优化。因为 Pod 中大多数 Env 都是相同的，所以我将他们合并在一起放到了 Pod 的 Annotations 中，再通过 DownwardAPI 将 Pod Annotations 映射到文件，再在执行每步 Step 之前将它们 export 成环境变量。</p><blockquote><p>我太难了</p></blockquote><p>于是 Pod 的 Spec 瞬间少了许多，也就解决了上面的 414 问题。</p><p>目前 <a href="https://github.com/domgoer/drone-runner-eci">drone-runner-eci</a> 已在我们生产环境稳定运行，想要体验弹性 CI 的朋友也可以尝试尝试。</p><h2 id="持续优化"><a href="#持续优化" class="headerlink" title="持续优化"></a>持续优化</h2><p>解决了上面提到的问题，Drone 也算能在 ECI 上运行了。但仍然有许多优化的空间。</p><h3 id="镜像拉取"><a href="#镜像拉取" class="headerlink" title="镜像拉取"></a>镜像拉取</h3><p>不同与在宿主机上创建 Pod，每次创建时可以使用宿主机上的镜像缓存。每次在 ECI 上创建 Pod 都需要拉取镜像，这便加慢了构建的速度，为了解决这一现象，阿里云也提供了方案 <a href="https://help.aliyun.com/document_detail/141241.html?spm=a2c4g.11174283.6.614.36ed4b5bSDZ6jm">imagecache</a>。</p><blockquote><p>imagecache 运行用户事先将需要用到的镜像作为云盘快照缓存，在创建ECI容器组实例时基于快照创建，避免或减少镜像层下载，从而提升ECI容器组实例创建速度。</p></blockquote><h3 id="资源优化"><a href="#资源优化" class="headerlink" title="资源优化"></a>资源优化</h3><p>ECI 给 Drone 提供了一个巨大的便利，就是在限制运行资源时不需要为每个 Container 设置 resources。</p><p>比如有两个 Step（Container），每个都需要 1vCPU 和 1Gi Mem，那么调度这个 Pod 就需要 node 上有超过 2vCPU 和 2Gi Mem 的资源，但又因为大多数时候 CI 构建任务是串行的，这两个 Step 不需要同时执行，也就是说在这种场景下其实只需要 1vCPU 和 1Gi Mem 就足够了，上面这种情况就造成了资源的浪费，但在宿主机上并没有办法很好的解决这个问题。不过在 ECI 中每个 Pod 会独占一台 ECI 实例，所以所有 Pod 内的所有 Container 都可以享受 ECI 的全额配置。</p><p>这种资源分配方式和资源大小可以通过 Pod <code>Annotations</code> 来指定：</p><pre><code class="yaml">annotations:    k8s.aliyun.com/eci-cpu: 1    k8s.aliyun.com/eci-memory: 1Gi</code></pre><p>这样 Pod 就能在一个 1vCPU 和 1Gi Mem 的 ECI 实例中稳定运行完两个 Step。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，<code>Virtual Kubelet</code> 可能是目前最好的弹性 CI 解决方案之一。(ps: 在 ECI 的产品文档中也包含了 Jenkins 和 Gitlab CI 的<a href="https://help.aliyun.com/document_detail/98298.html?spm=a2c4g.11174283.6.601.4c6a4b5bNIYtpx">最佳实践</a>)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是-virtual-kubelet&quot;&gt;&lt;a href=&quot;#什么是-virtual-kubelet&quot; class=&quot;headerlink&quot; title=&quot;什么是 virtual kubelet&quot;&gt;&lt;/a&gt;什么是 virtual kubelet&lt;/h2&gt;&lt;p&gt;以下</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="ci/cd" scheme="https://domc.me/tags/ci-cd/"/>
    
    <category term="drone" scheme="https://domc.me/tags/drone/"/>
    
    <category term="devops" scheme="https://domc.me/tags/devops/"/>
    
  </entry>
  
  <entry>
    <title>Drone 在 K8S 中执行一次构建都经历了什么</title>
    <link href="https://domc.me/2019/10/22/drone_in_k8s/"/>
    <id>https://domc.me/2019/10/22/drone_in_k8s/</id>
    <published>2019-10-22T14:37:10.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>在谈这个问题之前我们先来看看 drone 的结构。</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>drone 由 3 个主要的部分组成，分别是 <code>drone-server</code>、<code>drone-controller</code> 和 <code>drone-agent</code>。</p><h3 id="drone-server"><a href="#drone-server" class="headerlink" title="drone-server"></a>drone-server</h3><p>顾名思义，<code>drone-server</code> 是 drone 的服务端，它会启动一个 <code>http</code> 服务来处理各种请求，如 github 每次 push 或者其他操作触发的 webhook 亦或者是 <code>drone-web-ui</code> 的每个请求。</p><h3 id="drone-controller"><a href="#drone-controller" class="headerlink" title="drone-controller"></a>drone-controller</h3><p>controller 的作用是初始化 pipeline 信息，它会定义好 pipeline 的每个 step 在执行之前、执行之后以及获取和写入执行日志的函数，并保证每个 pipeline 能按顺序执行 step。</p><h3 id="drone-agent"><a href="#drone-agent" class="headerlink" title="drone-agent"></a>drone-agent</h3><p>可以将 <code>drone-agent</code> 在 <code>drone</code> 中的作用简单的理解为是 <code>kubelet</code> 在 <code>k8s</code> 中的作用，因为本文主要讨论的是 drone 在 k8s 中的执行过程，在 k8s 中，drone 的执行并不依赖 <code>drone-agent</code>，所以本文并不会对该组件做详细介绍。</p><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><ul><li>当在 github 上完成一次提交后，github 会将本次提交的信息发送到 <code>/hook</code>，<code>drone-server</code> 收到了请求之后，会将这些信息解析成 <code>core.Hook</code> 和 <code>core.Repository</code>。</li><li>接着会根据仓库的 <code>namespace</code> 和 <code>name</code> 在 drone 的数据库中查找该仓库，如果找不到或者项目不是 active 状态，则直接结束构建并返回错误信息。否则会将接下来的任务交给 <code>trigger</code> 来完成。</li></ul><h3 id="trigger"><a href="#trigger" class="headerlink" title="trigger"></a>trigger</h3><ol><li><code>trigger</code> 接收到 <code>core.Hook</code> 和 <code>core.Repository</code> 后会检查 commit message 中是否存在 <code>[ci skip]</code> 等跳过执行 ci 的字段，如果存在则直接结束。</li><li>接着 <code>trigger</code> 会验证 <code>repo</code> 和 <code>owner</code> 是否合法，commit message 是否为空，如果为空 <code>trigger</code> 会调用相关 api 获取上次提交的 commit message。</li><li>接下来 <code>trigger</code> 会向 <code>ConfigService</code> 请求构建的配置，一般情况下也就是 <code>.drone.yml</code> 中的内容。<code>ConfigService</code> 可以通过 <code>DRONE_YAML_ENDPOINT</code> 这一环境变量变量扩展，如果不扩展会使用默认的 <code>FileService</code> 也就是调用 github 的相关接口来获取 file data。</li><li>获取到 config 后 trigger 会调用 <code>ConvertService</code> 来转换 config，将 config 转换成 <code>yaml</code> (因为配置文件并非都是 <code>yaml</code>，可能是 <code>jsonnet</code> 或者 <code>script</code> 等其他格式)。<code>ConvertService</code> 目前支持 <code>jsonnet</code> 和 <code>starlark</code>。其中 starlark 需要使用外部扩展，也就是 <code>DRONE_CONVERT_PLUGIN_ENDPOINT</code> 来配置。</li><li>Convert 之后，trigger 会再解析一次 yaml，这里一来是可以将旧版本的 drone 的 yaml 格式转换为新的格式，二来是 drone 兼容 <code>gitlab-ci</code>，这一步可以将 gitlab-ci 的配置格式转换为 drone 的配置格式。</li><li>接下来 trigger 将 yaml 解析成 <code>yaml.Manifest</code> 结构体。之后会调用 <code>ValidateService</code> 来验证 <code>config</code>、<code>core.Build</code>、<code>repo owner</code> 和 <code>repo</code>，<code>ValidateService</code> 由 <code>DRONE_VALIDATE_PLUGIN_ENDPOINT</code> 环境变量配置，如果没有则不会这一步验证。</li><li>接下来 <code>trigger</code> 会验证 <code>yaml.Manifest</code> 中每个 <code>pipeline</code> 是否合法。会检查是否有 <code>重名 pipeline</code>，是否 step 中有 <code>自我依赖</code> 是否有 <code>依赖不存在</code> 以及 <code>权限</code> 等信息。</li><li>每个 pipeline 自身都通过验证后，<code>trigger</code> 会使用 <code>directed acyclic graph</code> 也就是有向无环图来检查各个 pipeline 之间是否有 <code>循环依赖</code>，并检查有哪些 piepline 不满足执行条件。</li><li>并同时检查每个 pipeline 是否满足执行条件。包括<code>branch</code>、<code>event</code>、<code>action</code>、<code>ref</code>等是否满足。</li><li>当上述验证条件都通过后，<code>trigger</code> 会更新数据库中的信息。然后将每个 pipeline 构建成 <code>core.Stage</code>，stage 没有依赖，那么它的 <code>status</code> 就会被设置为 <code>Pending</code>，这意味着它可以被执行了，如果有依赖，那么 <code>status</code> 就会被设置成 <code>Waiting</code>。</li><li><code>trigger</code> 会在数据库中创建好 <code>build</code> 信息，会向 github 发送构建状态，此时在 github 中我们就能看到那个小黄点了。</li><li>接着 <code>trigger</code> 会遍历每个 <code>stage</code>，并将 status 是 <code>Pending</code> 的 stage 进行调度。</li><li>然后将构建信息发送到环境变量 <code>DRONE_WEBHOOK_ENDPOINT</code> 配置的地址。至此 <code>trigger</code> 的工作就结束了。</li></ol><p>        <span class="lazyload-img-span">        <img              data-src="/images/drone_k8s/server.png" >        </sapn>      </p><h3 id="controller"><a href="#controller" class="headerlink" title="controller"></a>controller</h3><p>因为 <code>k8s</code> 带来的便利性，调度 <code>stage</code> 仅仅需要创建一个 <code>job</code>，在创建 job 之前， <code>scheduler</code> 会将 <code>drone-server</code> 中的大部分环境变量注入到该 job 也就是 <code>drone-job-stage.ID-randomString</code>(因为 k8s 对于每个资源的名称都有规范(不能以 <code>. _ -</code> 开头或结尾)，而在 <code>drone</code> 的其他 runtime 中并没有这一要求，所以为了符合 k8s 的命名规范，<code>drone</code> 使用了随机字符来作为资源名称。在创建 job 时，处于某些原因(后面会提到)，drone 还会该 job 挂载一个 <code>HostPath</code> 的 <code>volume</code>，路径为 <code>/tmp/drone</code>。该 <code>job</code> 的 <code>image name</code> 就是 <code>drone-controller</code>。</p><ol><li><code>drone-controller</code> 会初始化外部的 <code>SecretService</code>，该 service 有 <code>DRONE_SECRET_ENDPOINT</code> 配置。</li><li>接下来 <code>drone-controller</code> 会初始化三个 <code>registryService</code>，分别是 两个外部定义（由 <code>DRONE_SECRET_ENDPOINT</code> 和 <code>DRONE_REGISTRY_ENDPOINT</code> 配置) 和 本地文件（路径由 <code>DRONE_DOCKER_CONFIG</code> 定义）。</li><li><code>drone-controller</code> 还会初始化 <code>rpc client</code> 用于和 <code>drone-server</code> 通信。</li><li>最后 controller 会初始化好 <code>k8s engine</code>，至此 controller 的初始化工作就完成了，接下来的工作会交给 <code>runner</code> 这个内部组件来执行。</li></ol><h3 id="runner"><a href="#runner" class="headerlink" title="runner"></a>runner</h3><ol><li><code>runner</code> 会首先根据 <code>stage</code> 的 id 向 <code>drone-server</code> 获取 stage 的详细信息。</li><li>然后根据获取到的 repo.ID 获取 clone repo 所需要的 <code>token</code>。<code>drone-server</code> 接收到该请求后会先验证 repo 和 user，通过后会向 github 获取 token，用于拉取项目。</li><li>然后 runner 会检查构建的状态，如果不是 <code>killed</code> 或者 <code>skipped</code> 就会执行构建。</li><li>验证完成后 <code>runner</code> 会再次解析一次 <code>yaml</code> 的格式，这一步和 trigger 中执行的步骤一样。</li><li>之后 runner 会将 yaml 中所有 <code>$&#123;&#125;</code> 内的数据替换成对应的环境变量。</li><li>然后 runner 会根据 stage name 来从 yaml 中获取自己需要执行的 pipeline 的详细信息（因为 yaml 中往往包含多个 pipeline），之后再对自身的 pipeline 信息进行一次 lint，这次 lint 和 <code>trigger</code> 中第 <code>7</code> 步的操作是一样的，旨在保证元数据的合法性。</li><li>接着 runner 开始设置一系列的 <code>transform function</code>，包括 <code>registry</code>、<code>secret</code>、<code>volume</code> 等函数，这些函数会在之后的 <code>Compile</code> 中为 <code>Spec</code> 注入对应的资源，比如 <code>secret function</code> 会获取相应的 secret 并添加到 <code>spec</code> 中。</li><li>当上述操作都完成后，runner 便调用 <code>compiler</code> 模块开始编译 <code>pipeline</code>。</li></ol><h3 id="compiler"><a href="#compiler" class="headerlink" title="compiler"></a>compiler</h3><ol><li>在 <code>Compile</code> 开始时，compiler 会先确认 pipeline 的所有 steps 是否是 <code>serial</code> 的（如果 step 中存在依赖，则不是 serial）。然后会为 pipeline 挂载工作目录，也就是向 <code>spec</code> 中注入 <code>volume</code>，该 volume 为 <code>EmptyDir</code>。</li><li>接着把 yaml 中所有定义的 volumes 注入到 spec 之中。</li><li>compiler 会检查 piepline 是否需要 clone repo，如果 pipeline 没有定义</li></ol><pre><code class="yaml">clone:    disable: true</code></pre><p>的话，compiler 会在 spec 中注入 <code>clone-step</code>，compiler 会初始化好该步骤信息，如 <code>step name</code>、<code>image</code>、<code>mount workspace</code>。<br>4. 处理完 clone step 后，compiler 会处理 pipeline 中所有的 <code>Services</code>，每个 Service 也会被转换成 step 注入到 spec 中，不过与普通 step 不同的是 service 会被设置为单独运行，换个说法它们不依赖任何 step，同样 compiler 会为每个 service 相关的 step 做好和 clone step 类似的初始化工作。<br>5. 接下来就是处理不同的 steps，处理普通 step 分两种情况，一种是 step 中使用了 build 配置，使用了这种配置 drone 会在执行该 step 的过程中自动为 repo 打包，因为打包过程中需要使用到 docker，所有在处理该 step 时，需要<code>额外</code>将<code>docker.sock</code> 文件挂在到 <code>container</code>，另一种情况则是按正常的流程处理。</p><blockquote><p>正常的处理流程（包括clone,services 和 steps)：1. 将 yaml 中的数据能复制的都复制到 spec 2. 为 spec 注入 yaml 中配置的 volumes 3. 为 spec 注入 yaml 中配置的 envs 4. 将 yaml 中的 setting 中的配置作为环境变量 “PLUGIN_:&quot;+key: value 注入 spec （有些 env 和 setting 的值可能为 from_secret，这里就为会 spec 注入 secret）5. 将 yaml 中定义的 command 转换为 file，路径为 “&#x2F;usr&#x2F;drone&#x2F;bin&#x2F;init” 并注入 spec（之后运行时只需要运行该脚本即可）</p></blockquote><ol start="6"><li>最后 compiler 会执行之前定义好的所有 transform function，为 spec 注入 <code>docker auth</code>、来自 controller 和 <code>DRONE_RUNNER_*</code> 定义的环境变量，网络规则和 <code>SecretService</code> 中获取的 secret 等资源。至此 spec 的所有信息已经生成完毕。</li></ol><h3 id="hook"><a href="#hook" class="headerlink" title="hook"></a>hook</h3><ol><li>接下来 runner 会将 stage 中所有 step 的状态设置为 <code>Pending</code> 并保存到执行列表中，并初始化 <code>runtime.Hook</code>，在 hook 中会定义好执行每个 step 之前之后需要执行哪些步骤。在执行每个 step 之前，会创建一个 <code>streamer</code> 用于接收 log，并在数据库中更新 step 的状态，然后将 repo 的信息通过长连接推送给绑定的客户端。每次 step 执行完之后，会更新数据库中的信息，推送事件，并将之前创建的 <code>streamer</code> 删除。hook 还定义了写入日志函数，这样会将获取到的日志写入日志库。</li><li>定义好 hook 后，runner 会将 stage 的状态设置为 <code>Running</code>。在开始 build 之前，runner 会更新 stage 的状态，并将每个 step 保存到数据库，然后更新整个 build 的信息。</li></ol><h3 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h3><ol><li><code>runner</code> 初始化好 <code>runtime</code> 的信息后就会调用 <code>runtime.Run</code> 来执行构建，这一步才是真正开始构建。runtime 先调用先前定义好的 <code>Before</code> 函数创建好 streamer，接着 k8s 会创建出一个随机字符串为 name 的 namespace，接下来所有的 step 都会在该 namespace 完成。创建好 namespace 之后会创建构建所需的 <code>secret</code>，接着会将每个 step 中 <code>command</code> 内的信息创建为一个 <code>configmap</code>。</li><li>创建完成后 runtime 会开始执行每个 step，runtime 会判断各个 step 之间是否有依赖关系，如果没有则会按顺序一个一个执行。如果有依赖关系，则先运行没有依赖的 step，每次运行完该 step 后，将该 step 从其他依赖它的 step 的 depend 列表中移除，再进行如上操作，直到所有 step 都被运行（这一过程是并发的）。</li><li>每个 step 的执行其实就是在 k8s 中创建一个 pod，该 pod 的 image 就是设置的 yaml 中每个 step 定义的 image，所有的 pod 都会在👆的 namespace 下运行，为了保证每个 pod 都能共享文件，所有的 pod 都需要被调度到同一台机器，并且挂载同一个目录下的 <code>HostPath Volume</code>，而这台机器也就是 <code>drone-job-stage.ID-randomString</code> 被调度到的那台机器。当每个 pod 运行后，runtime 会注册一个回调函数来监听 pod 的变化，如果 pod 的状态变为 <code>running</code> 或者 <code>succeed</code> 或者 <code>failed</code> 之后，runtime 就会去获取该 pod 的日志，并把日志写入 <code>streamer</code> 中。最后 pod 运行结束后，runtime 会收集 pod 的退出状态，以判断是正常退出还是非正常退出。</li><li>等到所有的 pod 都执行完毕(或者有 pod 执行失败)，runtime 首先会更新数据库中相关数据的状态，然后会做清理工作，并检查当前 build 内所有的 pipeline，如果有 pipeline 依赖其他 pipeline 并且其他 piepline 已经执行完成，那么就会<code> 调度</code> 该 pipeline。</li><li>直到 build 中所有 pipeline 都完成了调度，本次 build 即为结束。</li></ol><p>        <span class="lazyload-img-span">        <img              data-src="/images/drone_k8s/controller.png" >        </sapn>      </p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在谈这个问题之前我们先来看看 drone 的结构。&lt;/p&gt;
&lt;h2 id=&quot;结构&quot;&gt;&lt;a href=&quot;#结构&quot; class=&quot;headerlink&quot; title=&quot;结构&quot;&gt;&lt;/a&gt;结构&lt;/h2&gt;&lt;p&gt;drone 由 3 个主要的部分组成，分别是 &lt;code&gt;drone-se</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="drone" scheme="https://domc.me/tags/drone/"/>
    
    <category term="devops" scheme="https://domc.me/tags/devops/"/>
    
    <category term="CI/CD" scheme="https://domc.me/tags/CI-CD/"/>
    
  </entry>
  
  <entry>
    <title>kubectl run 背后做了什么</title>
    <link href="https://domc.me/2019/08/29/whats_run_in_kubectl/"/>
    <id>https://domc.me/2019/08/29/whats_run_in_kubectl/</id>
    <published>2019-08-29T16:18:17.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章随便写写，只写流程，不写原理</p><h2 id="执行-kubectl-run"><a href="#执行-kubectl-run" class="headerlink" title="执行 kubectl run"></a>执行 kubectl run</h2><h3 id="Api-Server"><a href="#Api-Server" class="headerlink" title="Api Server"></a>Api Server</h3><ol><li><p>本地验证，确保不合法的请求（比如：创建不支持的资源或者格式不对等等）会快速失败，不会发给 api-server，减轻服务端压力</p></li><li><p>准备向 api-server 发送 HTTP 请求，将修改后的数据进行序列化。但是 URI PATH 是什么呢？这里就依赖资源内的 apiVersion 这个值，再加上资源类型，kubectl 就能在 api 列表中找到应该发往的地址。api 列表可以通过 api-server 的 &#x2F;apis 这个 URL 获取，获取之后会在本地缓存一份，提高效率。</p></li><li><p>api-server 肯定不会接受不合法的请求，所以 kubectl 还要在请求之前设置好认证信息。认证信息一般可以从 ~&#x2F;.kube&#x2F;config 中获取，它支持 4 种。</p><ul><li>tls: 需要使用 x509 证书</li><li>token: 在 HEADER 中添加 Authorization</li><li>basic username password: 基本的账号密码认证</li><li>openid: 类似于 token，openid 由用户事先手动设置</li></ul></li><li><p>这时 api-server 已经成功接收到了请求。它会判断我们是不是有权限操作这个资源。那么怎么验证呢，在 api-server 启动的时候，可以通过参数 –authorization_mode 进行设置，这个值有 4 种。</p><ul><li>webhook: 与集群外的 HTTPS 服务交互</li><li>ABAC: 静态文件定义的策略</li><li>RBAC: 动态配置的策略</li><li>Node: 每个 kubelet 只能访问自己节点上的资源</li></ul><p> 如果配置了多种授权方式，只要其中一种能通过那么请求就能继续。</p></li><li><p>授权通过了，但是现在仍然还不能够向 etcd 中写数据，还需要经过<code>准入控制链</code>这道关卡。准入控制链由 <code>Admission Controller</code> 控制。官方标准的准入控制链有近 10 个之多，而且支持自定义扩展。不同于授权，准入控制链一旦有一个验证失败，那么请求就会被拒绝。以下介绍三个准入控制器。</p><ul><li>SecurityContextDeny: 禁止创建设置了 Security Context 的 Pod</li><li>ResourceQuota: 限制某个 Namespace 下总系统资源的占用量和资源的数量</li><li>LimitRanger: 限制某个 Namespace 下单个资源的占用量</li></ul></li><li><p>通过上面所有验证后，api-server 将 kubectl 提交的数据反序列化，然后保存到 etcd 中。</p></li></ol><h3 id="InitializerConfiguration"><a href="#InitializerConfiguration" class="headerlink" title="InitializerConfiguration"></a>InitializerConfiguration</h3><ol><li>虽然数据已经持久化到 etcd 中了，但 apiserver 还无法完全看到或调度它，在此之前还要执行一系列 <code>Initializers</code>。<code>Initializers</code> 会在资源对外可用之前执行某些逻辑。比如 <code>将 Sidecar</code> 注入到暴露 80 端口的 Pod 中，或者加上特定的 <code>annotation</code> 等。<code>InitializerConfiguration</code> 资源对象允许你声明某些资源类型应该运行哪些Initializers。</li></ol><h3 id="Contorller"><a href="#Contorller" class="headerlink" title="Contorller"></a>Contorller</h3><ol><li><p>数据已经保存到 etcd 中了，并且初始化逻辑也完成了，下面就需要 k8s 中的 <code>Controller</code> 来完成资源的创建了。各个 Controller 会监听各自负责的资源，比如 <code>Deployment Controller</code> 就会监听 <code>Deployment</code> 资源的变化。当 api-server 将资源保存到 etcd 后，Controller 发现了资源的变化，然后就根据变化类型会调用相应回调函数。每个 Controller 都会尽力将资源当前的状态逐步转化为 etcd 中保存的状态。</p></li><li><p>当所有的 Controller 正常运行后，etcd 中就会保存一个 Deployment、一个 ReplicaSet 和 三个 Pod 资源记录，并且可以通过 kube-apiserver 查看。然而，这些 Pod 资源现在还处于 Pending 状态，因为它们还没有被调度到集群中合适的 Node 上运行。这个问题最终要靠调度器（<code>Scheduler</code>）来解决。</p></li></ol><h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><ol><li><p>Scheduler 会将待调度的 Pod 按照特定的算法和调度策略绑定到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中（它会过滤其 PodSpec 中 NodeName 字段为空的 Pod）。</p></li><li><p>Scheduler 一旦找到了合适的节点，就会创建一个 <code>Binding</code> 对象，该对象的 <code>Name</code> 和 <code>Uid</code> 与 Pod 相匹配，并且其 <code>ObjectReference</code> 字段包含所选节点的名称，然后通过 POST 请求发送给 apiserver。</p></li><li><p>当 kube-apiserver 接收到此 Binding 对象时,会更新 Pod 资源中的以下字段:</p><ul><li><p>将 <code>NodeName</code> 的值设置为 <code>ObjectReference</code> 中的 <code>NodeName</code>。</p></li><li><p>添加相关的注释。</p></li><li><p>将 <code>PodScheduled</code> 的 <code>status</code> 值设置为 <code>True</code>。</p></li></ul></li></ol><h3 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h3><p>在 Kubernetes 集群中，每个 Node 节点上都会启动一个 Kubelet 服务进程，该进程用于处理 Scheduler 下发到本节点的任务，管理 Pod 的生命周期，包括挂载卷、容器日志记录、垃圾回收以及其他与 Pod 相关的事件。</p><ol><li><p>Kubelet 每隔 20s，会通过 <code>NodeName</code> 向 api-server 发送查询请求来获取自身 Node 上所要运行的 Pod 清单。获取到数据后会和自身内部缓存比较来获取有差异的 Pod 列表。并开始同步这些 Pod。</p><ul><li><p>记录 Pod 启动相关的 <code>Metrics</code></p></li><li><p>生成一个 <code>PodStatus</code> 对象，它表示 Pod 当前阶段的状态。PodStatus 的值取决于：一、<code>PodSyncHandlers</code> 会检查 Pod 是否应该运行在 Node，如果不应该 <code>PodStatus</code> 会有 <code>Phase</code> 变成 <code>PodFailed</code>。二、接下来 PodStatus 会由 <code>init 容器</code> 和<code>应用容器</code>的状态共同来决定。</p></li></ul></li><li><p>生成 PodStatus 之后（Pod 中的 status 字段），Kubelet 就会将它发送到 Pod 的状态管理器，该管理器的任务是通过 apiserver 异步更新 etcd 中的记录。</p></li><li><p>接下来运行一系列准入处理器来确保该 Pod 是否具有相应的权限，被准入控制器拒绝的 Pod 将一直保持 Pending 状态。</p></li><li><p>如果 Kubelet 启动时指定了 <code>cgroups-per-qos</code> 参数，Kubelet 就会为该 Pod 创建 cgroup 并进行相应的资源限制。这是为了更方便地对 Pod 进行服务质量（QoS）管理。</p></li><li><p>然后为 Pod 创建相应的目录，包括 Pod 的目录（<code>/var/run/kubelet/pods/&lt;podID&gt;</code>），该 Pod 的卷目录（<code>&lt;podDir&gt;/volumes</code>）和该 Pod 的插件目录（<code>&lt;podDir&gt;/plugins</code>）。</p></li><li><p>卷管理器会挂载 <code>Spec.Volumes</code> 中定义的相关数据卷，然后等待是否挂载成功。根据挂载卷类型的不同，某些 Pod 可能需要等待更长的时间（比如 NFS 卷）。</p></li><li><p>从 apiserver 中检索 <code>Spec.ImagePullSecrets</code> 中定义的所有 Secret，然后将其注入到容器中。</p></li></ol><h3 id="CRI"><a href="#CRI" class="headerlink" title="CRI"></a>CRI</h3><ol><li>上步之后大量的初始化工作都已经完成，容器已经准备好开始启动了。Kubelet 通过容器运行时接口与容器运行时（默认是 <code>Docker</code>）交互。第一次启动 Pod 时，Kubelet 会创建 <code>sandbox</code>，sandbox 作为 Pod 中所有的容器的基础容器，为 Pod 中的每个业务容器提供了大量的 Pod 级别资源，这些资源都是 Linux 命名空间（包括网络命名空间，IPC 命名空间和 PID 命名空间）。</li></ol><h3 id="CNI"><a href="#CNI" class="headerlink" title="CNI"></a>CNI</h3><ol><li>接着 Kubelet 会为 Pod 创建网络环境，来保证跨主机之间 <code>Pod 和 Pod</code>，<code>Pod 和 Service</code> 的通信。当 Kubelet 为 Pod 创建网络时，它会将创建网络的任务交给 <code>CNI</code> 插件。CNI 表示容器网络接口（Container Network Interface），和容器运行时的运行方式类似，它也是一种抽象，允许不同的网络提供商为容器提供不同的网络实现。不同的 CNI 插件运行原理会有不同，可以参考对应的文章。</li></ol><h3 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h3><p>所有网络都配置完成后，接下来就开始真正启动业务容器了！</p><ol><li><p>一旦 <code>sanbox</code> 完成初始化并处于 active 状态，Kubelet 就可以开始为其创建容器了。首先启动 <code>PodSpec</code> 中定义的 init 容器，然后再启动业务容器。</p></li><li><p>首先拉取容器的镜像。如果是私有仓库的镜像，就会利用 <code>PodSpec</code> 中指定的 <code>Secret</code> 来拉取该镜像。</p></li><li><p>然后通过 <code>CRI</code> 接口创建容器。<code>Kubelet</code> 向 <code>PodSpec</code> 中填充了一个 <code>ContainerConfig</code> 数据结构（在其中定义了命令，镜像，标签，挂载卷，设备，环境变量等待），然后通过 protobufs 发送给 CRI 接口。对于 Docker 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 Dockerd 守护进程。在这个过程中，它会将一些元数据标签（例如容器类型，日志路径，dandbox ID 等待）添加到容器中。</p></li><li><p>接下来会使用 <code>CPU</code> 管理器来约束容器，这是 Kubelet 1.8 中新添加的 alpha 特性，它使用 <code>UpdateContainerResources CRI</code> 方法将容器分配给本节点上的 CPU 资源池。</p></li></ol><p>最后容器开始真正启动。</p><p>如果 <code>Pod</code> 中配置了容器生命周期钩子（<code>Hook</code>），容器启动之后就会运行这些 Hook。Hook 的类型包括两种：Exec（执行一段命令） 和 HTTP（发送HTTP请求）。如果 PostStart Hook 启动的时间过长、挂起或者失败，容器将永远不会变成 <code>running</code> 状态。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上文所述的创建 Pod 整个过程的流程图如下所示：</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/kubectl_run/short.svg" >        </sapn>      </p><p>参考：</p><ul><li><a href="https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw">https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这篇文章随便写写，只写流程，不写原理&lt;/p&gt;
&lt;h2 id=&quot;执行-kubectl-run&quot;&gt;&lt;a href=&quot;#执行-kubectl-run&quot; class=&quot;headerlink&quot; title=&quot;执行 kubectl run&quot;&gt;&lt;/a&gt;执行 kubectl run&lt;/h2</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubeDNS和coreDNS</title>
    <link href="https://domc.me/2019/08/07/kube_dns_and_core_dns/"/>
    <id>https://domc.me/2019/08/07/kube_dns_and_core_dns/</id>
    <published>2019-08-07T21:32:39.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章主要围绕两种 DNS Server 的实现方式展开，会比较两种 Server 的优缺点。</p><p>在说两个 Service 之前，我们先来了解一下在k8s中域名是如何被解析的。</p><p>我们都知道，在 k8s 中，一个 Pod 如果要访问同 Namespace 下的 Service（比如 user-svc），那么只需要curl user-svc。如果 Pod 和 Service 不在同一域名下，那么就需要在 Service Name 之后添加上 Service 所在的 Namespace（比如 beta），curl user-svc.beta。那么 k8s 是如何知道这些域名是内部域名并为他们做解析的呢？</p><h2 id="DNS-In-Kubernetes"><a href="#DNS-In-Kubernetes" class="headerlink" title="DNS In Kubernetes"></a>DNS In Kubernetes</h2><h3 id="x2F-etc-x2F-resolv-conf"><a href="#x2F-etc-x2F-resolv-conf" class="headerlink" title="&#x2F;etc&#x2F;resolv.conf"></a>&#x2F;etc&#x2F;resolv.conf</h3><p>resolv.conf 是 DNS 域名解析的配置文件。每行都会以一个关键字开头，然后跟配置参数。这里主要使用到的关键词有3个。</p><ul><li>nameserver   #定义 DNS 服务器的 IP 地址</li><li>search       #定义域名的搜索列表，当查询的域名中包含的 <code>.</code> 的数量少于 <code>options.ndots</code> 的值时，会依次匹配列表中的每个值</li><li>options      #定义域名查找时的配置信息</li></ul><p>那么我们进入一个 Pod 查看它的 resolv.conf</p><pre><code class="conf">nameserver 100.64.0.10search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5</code></pre><blockquote><p>这里的 nameserver、search 和 options 都是可以通过 dnsConfig 字段进行配置的，<a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">官方文档</a>中已有详细的讲述</p></blockquote><p>上述配置文件 resolv.conf 是 <code>dnsPolicy: ClusterFirst</code> 情况下，k8s 为 Pod 自动生成的，这里的 nameserver 所对应的地址正是 DNS Service 的Cluster IP（该值在启动 kubelet 的时候，通过 clusterDNS 指定）。所以，从集群内请求的所有的域名解析都需要经过 DNS Service 进行解析，不管是 k8s 内部域名还是外部域名。</p><p>可以看到这里的 search 域默认包含了 namespace.svc.cluster.local、svc.cluster.local 和 cluster.local 三种。当我们在 Pod 中访问 <code>a</code> Service时（ <code>curl a</code> ），会选择nameserver 100.64.0.10 进行解析，然后依次带入 search 域进行 DNS 查找，直到找到为止。</p><pre><code class="bash"># curl aa.default.svc.cluster.local</code></pre><p>显然因为 Pod 和 <code>a</code> Service 在同一 Namespace 下，所以第一次 lookup 就能找到。</p><p>如果 Pod 要访问不同 Namespace（例如： <code>beta</code> ）下的 Service <code>b</code> （ <code>curl b.beta</code> ），会经过两次 DNS 查找，分别是</p><pre><code class="bash"># curl b.betab.beta.default.svc.cluster.local（未找到）b.beta.svc.cluster.local（找到）</code></pre><p>正是因为 search 的顺序性，所以访问同一 Namespace 下的 Service， <code>curl a</code> 是要比 <code>curl a.default</code> 的效率更高的，因为后者多经过了一次 DNS 解析。</p><pre><code class="bash"># curl aa.default.svc.cluster.local# curl a.defaultb.default.default.svc.cluster.local（未找到）b.default.svc.cluster.local（找到）</code></pre><h3 id="那么当Pod中访问外部域名时仍然需要走search域吗？"><a href="#那么当Pod中访问外部域名时仍然需要走search域吗？" class="headerlink" title="那么当Pod中访问外部域名时仍然需要走search域吗？"></a>那么当Pod中访问外部域名时仍然需要走search域吗？</h3><p>这个答案，不能说肯定也不能说否定，看情况，可以说，大部分情况要走 search 域。</p><p>以 <code>domgoer.com</code> 为例，通过抓包的方式，在某一个Pod中访问 domgoer.com ，可以看到 DNS 查找的过程，都产生了什么样的数据包。首先先进入 DNS 容器的网络。</p><blockquote><p>ps: 由于 DNS 容器往往不具备 bash，所以不能通过 docker exec 的方式进入容器抓包，需要采用其他方法</p></blockquote><pre><code class="bash">// 1.找到容器ID，打印它的NS IDdocker inspect --format &quot;&#123;&#123;.State.Pid&#125;&#125;&quot; container_id// 2.进入此容器的Namespacensenter -n -t pid// 3.DNS抓包tcpdump -i eth0 -N udp dst port 53</code></pre><p>在其他容器中进行domgoer.com域名查找</p><pre><code class="bash">nslookup domgoer.com dns_container_ip</code></pre><blockquote><p>指定 dns_container_ip，是为了避免有多个DNS容器的情况，DNS请求会分到各个容器。这样可以让 DNS 请求只发往这个地址，这样抓包的数据才会完整。</p></blockquote><p>可以看到如下的结果：</p><pre><code class="bash">17:01:28.732260 IP 172.20.92.100.36326 &gt; nodexxxx.domain: 4394+ A? domgoer.com.default.svc.cluster.local. (50)17:01:28.733158 IP 172.20.92.100.49846 &gt; nodexxxx.domain: 60286+ A? domgoer.com.svc.cluster.local. (45)17:01:28.733888 IP 172.20.92.100.51933 &gt; nodexxxx.domain: 63077+ A? domgoer.com.cluster.local. (41)17:01:28.734588 IP 172.20.92.100.33401 &gt; nodexxxx.domain: 27896+ A? domgoer.com. (27)17:01:28.734758 IP nodexxxx.34138 &gt; 192.168.x.x.domain: 27896+ A? domgoer.com. (27)</code></pre><p>可以看到在真正解析 domgoer.com 之前，经历了 domgoer.com.default.svc.cluster.local. -&gt; domgoer.com.svc.cluster.local. -&gt; domgoer.com.cluster.local. -&gt; domgoer.com.</p><p>这样也就意味着有3次DNS请求是浪费的，没有意义的。</p><h3 id="如何避免这样的情况"><a href="#如何避免这样的情况" class="headerlink" title="如何避免这样的情况"></a>如何避免这样的情况</h3><p>在研究如何避免之前可以下思考一下造成这种情况的原因。在 &#x2F;etc&#x2F;resolv.conf 文件中，我们可以看到 <code>options</code> 中有个配置项 <strong>ndots:5</strong> 。</p><p>ndots:5，表示：如果需要 lookup 的 Domain 中包含少于5个 <code>.</code> ，那么将使用非绝对域名，如果需要查询的 DNS 中包含大于或等于5个 <code>.</code> ，那么就会使用绝对域名。如果是绝对域名则不会走 search 域，如果是非绝对域名，就会按照 search 域中进行逐一匹配查询，如果 search 走完了都没有找到，那么就会使用 <code>原域名.（domgoer.com.）</code> 的方式作为绝对域名进行查找。</p><p>综上可以找到两种优化的方法</p><ol><li><p>直接使用绝对域名</p><p> 这是最简单直接的优化方式，可以直接在要访问的域名后面加上 <code>.</code> 如：domgoer.com. ，这样就会避免走 search 域进行匹配。</p></li><li><p>配置ndots</p><p> 还记得之前说过 &#x2F;etc&#x2F;resolv.conf 中的参数都可以通过k8s中的 <code>dnsConfig</code> 字段进行配置。这就允许你根据你自己的需求配置域名解析的规则。</p><p> 例如 当域名中包含两个 <code>.</code> 或以上时，就能使用绝对域名直接进行域名解析。</p><pre><code class="yaml">apiVersion: v1kind: Podmetadata:namespace: defaultname: dns-examplespec:containers:- name: test  image: nginxdnsConfig:  options:  - name: ndots    value: 2</code></pre></li></ol><h3 id="Kubernetes-DNS-策略"><a href="#Kubernetes-DNS-策略" class="headerlink" title="Kubernetes DNS 策略"></a>Kubernetes DNS 策略</h3><p>在k8s中，有4中DNS策略，分别是 <code>ClusterFirstWithHostNet</code> 、<code>ClusterFirst</code> 、<code>Default</code> 、和 <code>None</code>，这些策略可以通过 <code>dnsPolicy</code> 这个字段来定义</p><p>如果在初始化 Pod、Deployment 或者 RC 等资源时没有定义，则会默认使用 <code>ClusterFirst</code> 策略</p><ol><li><p>ClusterFirstWithHostNet</p><p> 当一个 Pod 以 HOST 模式（和宿主机共享网络）启动时，这个 POD 中的所有容器都会使用宿主机的&#x2F;etc&#x2F;resolv.conf 配置进行 DNS 查询，但是如果你还想继续使用 Kubernetes 的 DNS 服务，<br> 就需要将 dnsPolicy 设置为 ClusterFirstWithHostNet。</p></li><li><p>ClusterFirst</p><p> 使用这是方式表示 Pod 内的 DNS 优先会使用 k8s 集群内的DNS服务，也就是会使用 kubedns 或者  coredns 进行域名解析。如果解析不成功，才会使用宿主机的 DNS 配置进行解析。</p></li><li><p>Default</p><p> 这种方式，会让 kubelet 来绝定 Pod 内的 DNS 使用哪种 DNS 策略。kubelet 的默认方式，其实就是使用宿主机的 &#x2F;etc&#x2F;resolv.conf 来进行解析。你可以通过设置 kubelet 的启动参数，<br> –resolv-conf&#x3D;&#x2F;etc&#x2F;resolv.conf 来决定 DNS 解析文件的地址</p></li><li><p>None</p><p> 这种方式顾名思义，不会使用集群和宿主机的 DNS 策略。而是和 dnsConfig 配合一起使用，来自定义 DNS 配置，否则在提交修改时报错。</p></li></ol><h2 id="kubeDNS"><a href="#kubeDNS" class="headerlink" title="kubeDNS"></a><a href="https://github.com/kubernetes/dns">kubeDNS</a></h2><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>kubeDNS由3个部分组成。</p><ol><li>kubedns: 依赖 <code>client-go</code> 中的 <code>informer</code> 机制监视 k8s 中的 <code>Service</code> 和 <code>Endpoint</code> 的变化，并将这些结构维护进内存来服务内部 DNS 解析请求。</li><li>dnsmasq: 区分 Domain 是集群内部还是外部，给外部域名提供上游解析，内部域名发往 10053 端口，并将解析结果缓存，提高解析效率。</li><li>sidecar: 对 kubedns 和 dnsmasq 进行健康检查和收集监控指标。</li></ol><p>以下是结构图</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/kubeDNS%E5%92%8CcoreDNS/kube_dns_structure.png" >        </sapn>      </p><h3 id="kubedns"><a href="#kubedns" class="headerlink" title="kubedns"></a>kubedns</h3><p>在 kubedns 包含两个部分, kubedns 和 skydns。</p><p>其中 kubedns 是负责监听 k8s 集群中的 <code>Service</code> 和 <code>Endpoint</code> 的变化，并将这些变化通过 <code>treecache</code> 的数据结构缓存下来，作为 Backend 给 skydns 提供 Record。<br>而真正负责dns解析的其实是 <code>skydns</code>（skydns 目前有两个版本 skydns1 和 skydns2，下面所说的是 skydns2，也是当前 kubedns 所使用的版本）。</p><p>我们可以先看下 treecache，以下是 treecache 的数据结构</p><pre><code class="go">// /dns/pkg/dns/treecache/treecache.go#54type treeCache struct &#123;    ChildNodes map[string]*treeCache    Entries    map[string]interface&#123;&#125;&#125;</code></pre><p>我们再看一组实际的数据</p><pre><code class="json">&#123;    &quot;ChildNodes&quot;: &#123;        &quot;local&quot;: &#123;            &quot;ChildNodes&quot;: &#123;                &quot;cluster&quot;: &#123;                    &quot;ChildNodes&quot;: &#123;                        &quot;svc&quot;: &#123;                            &quot;ChildNodes&quot;: &#123;                                &quot;namespace&quot;: &#123;                                    &quot;ChildNodes&quot;: &#123;                                        &quot;service_name&quot;: &#123;                                            &quot;ChildNodes&quot;: &#123;                                                &quot;_tcp&quot;: &#123;                                                    &quot;ChildNodes&quot;: &#123;                                                        &quot;_http&quot;: &#123;                                                            &quot;ChildNodes&quot;: &#123;&#125;,                                                            &quot;Entries&quot;: &#123;                                                                &quot;6566333238383366&quot;: &#123;                                                                    &quot;host&quot;: &quot;service.namespace.svc.cluster.local.&quot;,                                                                    &quot;port&quot;: 80,                                                                    &quot;priority&quot;: 10,                                                                    &quot;weight&quot;: 10,                                                                    &quot;ttl&quot;: 30                                                                &#125;                                                            &#125;,                                                        &#125;                                                    &#125;,                                                    &quot;Entries&quot;: &#123;&#125;                                                &#125;                                            &#125;,                                            &quot;Entries&quot;: &#123;                                                &quot;3864303934303632&quot;: &#123;                                                    &quot;host&quot;: &quot;100.70.28.188&quot;,                                                    &quot;priority&quot;: 10,                                                    &quot;weight&quot;: 10,                                                    &quot;ttl&quot;: 30                                                &#125;                                            &#125;                                        &#125;                                    &#125;,                                    &quot;Entries&quot;: &#123;&#125;                                &#125;                            &#125;,                            &quot;Entries&quot;: &#123;&#125;                        &#125;                    &#125;,                    &quot;Entries&quot;: &#123;&#125;                &#125;            &#125;,            &quot;Entries&quot;: &#123;&#125;        &#125;    &#125;,    &quot;Entries&quot;: &#123;&#125;&#125;</code></pre><p>treeCache 的结构类似于目录树。从根节点到叶子节点的每个路径与一个域名是相对应的，顺序是颠倒的。它的叶子节点只包含 Entries，非叶子节点只包含 ChildNodes。叶子节点中保存的就是 SkyDNS 定义的 msg.Service 结构，可以理解为 DNS 记录。</p><p>在 Records 接口方法实现中，只需根据域名查找到对应的叶子节点，并返回叶子节点中保存的所有msg.Service 数据。K8S 就是通过这样的一个数据结构来保存 DNS 记录的，并替换了 Etcd（ skydns2 默认使用 etcd 作为存储），来提供基于内存的高效存储。</p><p>我们可以直接阅读代码来了解 kubedns 的启动流程。</p><p>首先看它的结构体</p><pre><code class="go">// dns/cmd/kube-dns/app/server.go#43type KubeDNSServer struct &#123;    // DNS domain name. = cluster.local.    domain         string    healthzPort    int    // skydns启动的地址和端口    dnsBindAddress string    dnsPort        int    nameServers    string    kd             *dns.KubeDNS&#125;</code></pre><p>下来可以看到一个叫 <code>NewKubeDNSServerDefault</code> 的函数，它初始化了 KubeDNSServer。并执行 <code>server.Run()</code> 启动了服务。那么我们来看下 <code>NewKubeDNSServerDefault</code> 这个方法做了什么。</p><pre><code class="go">// dns/cmd/kube-dns/app/server.go#53func NewKubeDNSServerDefault(config *options.KubeDNSConfig) *KubeDNSServer &#123;    kubeClient, err := newKubeClient(config)    if err != nil &#123;      glog.Fatalf(&quot;Failed to create a kubernetes client: %v&quot;, err)    &#125;    // 同步配置文件，如果观察到配置信息改变，就会重启skydns    var configSync dnsconfig.Sync    switch &#123;    // 同时配置了 configMap 和 configDir 会报错    case config.ConfigMap != &quot;&quot; &amp;&amp; config.ConfigDir != &quot;&quot;:      glog.Fatal(&quot;Cannot use both ConfigMap and ConfigDir&quot;)    case config.ConfigMap != &quot;&quot;:      configSync = dnsconfig.NewConfigMapSync(kubeClient, config.ConfigMapNs, config.ConfigMap)    case config.ConfigDir != &quot;&quot;:      configSync = dnsconfig.NewFileSync(config.ConfigDir, config.ConfigPeriod)    default:      conf := dnsconfig.Config&#123;Federations: config.Federations&#125;      if len(config.NameServers) &gt; 0 &#123;        conf.UpstreamNameservers = strings.Split(config.NameServers, &quot;,&quot;)      &#125;      configSync = dnsconfig.NewNopSync(&amp;conf)    &#125;    return &amp;KubeDNSServer&#123;      domain:         config.ClusterDomain,      healthzPort:    config.HealthzPort,      dnsBindAddress: config.DNSBindAddress,      dnsPort:        config.DNSPort,      nameServers:    config.NameServers,      kd:             dns.NewKubeDNS(kubeClient, config.ClusterDomain, config.InitialSyncTimeout, configSync),    &#125;&#125;// 启动skydns serverfunc (d *KubeDNSServer) startSkyDNSServer() &#123;    skydnsConfig := &amp;server.Config&#123;      Domain:  d.domain,      DnsAddr: fmt.Sprintf(&quot;%s:%d&quot;, d.dnsBindAddress, d.dnsPort),    &#125;    if err := server.SetDefaults(skydnsConfig); err != nil &#123;      glog.Fatalf(&quot;Failed to set defaults for Skydns server: %s&quot;, err)    &#125;    // 使用d.kd作为存储的后端，因为kubedns实现了skydns.Backend的接口    s := server.New(d.kd, skydnsConfig)    if err := metrics.Metrics(); err != nil &#123;      glog.Fatalf(&quot;Skydns metrics error: %s&quot;, err)    &#125; else if metrics.Port != &quot;&quot; &#123;      glog.V(0).Infof(&quot;Skydns metrics enabled (%v:%v)&quot;, metrics.Path, metrics.Port)    &#125; else &#123;      glog.V(0).Infof(&quot;Skydns metrics not enabled&quot;)    &#125;    d.kd.SkyDNSConfig = skydnsConfig    go s.Run()&#125;</code></pre><p>可以看到这里 <code>dnsconfig</code> 会返回一个 <code>configSync</code> 的 interface 用来实时同步配置，也就是 <code>kube-dns</code> 这个 configmap，或者是本地的 dir（但一般来说这个 dir 也是由 configmap 挂载进去的）。在方法的最后 <code>dns.NewKubeDNS</code> 返回一个 KubeDNS 的结构体。那么我们看下这个函数初始化了哪些东西。</p><pre><code class="go">// dns/pkg/dnsdns.go#124func NewKubeDNS(client clientset.Interface, clusterDomain string, timeout time.Duration, configSync config.Sync) *KubeDNS &#123;    kd := &amp;KubeDNS&#123;      kubeClient:          client,      domain:              clusterDomain,      cache:               treecache.NewTreeCache(),      cacheLock:           sync.RWMutex&#123;&#125;,      nodesStore:          kcache.NewStore(kcache.MetaNamespaceKeyFunc),      reverseRecordMap:    make(map[string]*skymsg.Service),      clusterIPServiceMap: make(map[string]*v1.Service),      domainPath:          util.ReverseArray(strings.Split(strings.TrimRight(clusterDomain, &quot;.&quot;), &quot;.&quot;)),      initialSyncTimeout:  timeout,      configLock: sync.RWMutex&#123;&#125;,      configSync: configSync,    &#125;    kd.setEndpointsStore()    kd.setServicesStore()    return kd&#125;</code></pre><p>可以看到<code>kd.setEndpointsStore()</code> 和 <code>kd.setServicesStore()</code> 这两个方法会在 <code>informer</code>中注册 <code>Service</code> 和 <code>Endpoint</code> 的回调，用来观测这些资源的变动并作出相应的调整。</p><p>下面我们看下当集群中新增一个 Service,kubedns 会以怎样的方式处理。</p><pre><code class="go">// dns/pkg/dns/dns.go#499func (kd *KubeDNS) newPortalService(service *v1.Service) &#123;    // 构建了一个空的叶子节点, recordLabel是clusterIP经过 FNV-1a hash运算后得到的32位数字    // recordValue 的结构    // &amp;msg.Service&#123;    //  Host:     service.Spec.ClusterIP,    //  Port:     0,    //  Priority: defaultPriority,    //  Weight:   defaultWeight,    //  Ttl:      defaultTTL,    // &#125;    subCache := treecache.NewTreeCache()    recordValue, recordLabel := util.GetSkyMsg(service.Spec.ClusterIP, 0)    subCache.SetEntry(recordLabel, recordValue, kd.fqdn(service, recordLabel))    // 查看service的ports列表，将每个port信息转换成skydns.Service并加入上面构建的叶子节点    for i := range service.Spec.Ports &#123;      port := &amp;service.Spec.Ports[i]      if port.Name != &quot;&quot; &amp;&amp; port.Protocol != &quot;&quot; &#123;        srvValue := kd.generateSRVRecordValue(service, int(port.Port))        l := []string&#123;&quot;_&quot; + strings.ToLower(string(port.Protocol)), &quot;_&quot; + port.Name&#125;        subCache.SetEntry(recordLabel, srvValue, kd.fqdn(service, append(l, recordLabel)...), l...)      &#125;    &#125;    subCachePath := append(kd.domainPath, serviceSubdomain, service.Namespace)    host := getServiceFQDN(kd.domain, service)    reverseRecord, _ := util.GetSkyMsg(host, 0)    kd.cacheLock.Lock()    defer kd.cacheLock.Unlock()    // 将构建好的叶子节点加入treecache    kd.cache.SetSubCache(service.Name, subCache, subCachePath...)    kd.reverseRecordMap[service.Spec.ClusterIP] = reverseRecord    kd.clusterIPServiceMap[service.Spec.ClusterIP] = service&#125;</code></pre><p>再看一下当 Endpoint 添加到集群时，kubedns 会如何处理</p><pre><code class="go">// dns/pkg/dns/dns.go#460func (kd *KubeDNS) addDNSUsingEndpoints(e *v1.Endpoints) error &#123;    // 获取ep所属的svc    svc, err := kd.getServiceFromEndpoints(e)    if err != nil &#123;      return err    &#125;    // 判断这个svc，如果这个svc不是 headless，就不会处理此次添加，因为 svc 有 clusterIP 的情况，在处理    // svc 的增删改时已经都被处理了。所以当 ep 属于 headless svc 时，需要将这个 ep 加入到 cache    if svc == nil || v1.IsServiceIPSet(svc) || svc.Spec.Type == v1.ServiceTypeExternalName &#123;      // No headless service found corresponding to endpoints object.      return nil    &#125;    return kd.generateRecordsForHeadlessService(e, svc)&#125;// 把 endpoint 添加到它所属的 headless service 的缓存下func (kd *KubeDNS) generateRecordsForHeadlessService(e *v1.Endpoints, svc *v1.Service) error &#123;    subCache := treecache.NewTreeCache()    generatedRecords := map[string]*skymsg.Service&#123;&#125;    // 遍历这个 ep 下所有的 ip+port，并将它们添加到 treecache 中    for idx := range e.Subsets &#123;      for subIdx := range e.Subsets[idx].Addresses &#123;        address := &amp;e.Subsets[idx].Addresses[subIdx]        endpointIP := address.IP        recordValue, endpointName := util.GetSkyMsg(endpointIP, 0)        if hostLabel, exists := getHostname(address); exists &#123;          endpointName = hostLabel        &#125;        subCache.SetEntry(endpointName, recordValue, kd.fqdn(svc, endpointName))        for portIdx := range e.Subsets[idx].Ports &#123;          endpointPort := &amp;e.Subsets[idx].Ports[portIdx]          if endpointPort.Name != &quot;&quot; &amp;&amp; endpointPort.Protocol != &quot;&quot; &#123;            srvValue := kd.generateSRVRecordValue(svc, int(endpointPort.Port), endpointName)            l := []string&#123;&quot;_&quot; + strings.ToLower(string(endpointPort.Protocol)), &quot;_&quot; + endpointPort.Name&#125;            subCache.SetEntry(endpointName, srvValue, kd.fqdn(svc, append(l, endpointName)...), l...)          &#125;        &#125;        // Generate PTR records only for Named Headless service.        if _, has := getHostname(address); has &#123;          reverseRecord, _ := util.GetSkyMsg(kd.fqdn(svc, endpointName), 0)          generatedRecords[endpointIP] = reverseRecord        &#125;      &#125;    &#125;    subCachePath := append(kd.domainPath, serviceSubdomain, svc.Namespace)    kd.cacheLock.Lock()    defer kd.cacheLock.Unlock()    for endpointIP, reverseRecord := range generatedRecords &#123;      kd.reverseRecordMap[endpointIP] = reverseRecord    &#125;    kd.cache.SetSubCache(svc.Name, subCache, subCachePath...)    return nil&#125;</code></pre><p>整体流程其实和 Service 差不多，只不过在添加 cache 之前会先去查找Endpoint所属的 Service，然后不同的是 Endpoint 的叶子节点中的host存储的是 EndpointIP，而 Service 的叶子节点的 host 中存储的是 fqdn。</p><h3 id="kubedns总结"><a href="#kubedns总结" class="headerlink" title="kubedns总结"></a>kubedns总结</h3><ol><li><p>kubedns 有两个模块，kubedns和skydns，kubedns负责监听<code>Service</code>和<code>Endpoint</code>并将它们转换为 skydns 能够理解的格式，以目录树的形式存在内存中。</p></li><li><p>因为 skydns 是以 etcd 的标准作为后端存储的，所以为了兼容 etcd ，kubedns 在错误信息方面都以 etcd 的格式进行定义的。因此 kubedns 的作用其实可以理解为为 skydns 提供存储。</p></li></ol><h3 id="dnsmasq"><a href="#dnsmasq" class="headerlink" title="dnsmasq"></a>dnsmasq</h3><p>dnsmasq 由两个部分组成</p><p>1.dnsmasq-nanny，容器里的1号进程，不负责处理 DNS LookUp 请求，只负责管理 dnsmasq。<br>2.dnsmasq，负责处理 DNS LookUp 请求，并缓存结果。</p><p>dnsmasq-nanny 负责监控 config 文件（&#x2F;etc&#x2F;k8s&#x2F;dns&#x2F;dnsmasq-nanny，也就是kube-dns-config这个 configmap 所挂载的位置）的变化（每 10s 查看一次），如果 config 变化了就会<strong>Kill</strong>掉 dnsmasq，并重新启动它。</p><pre><code class="go">// dns/pkg/dnsmasq/nanny.go#198// RunNanny 启动 nanny 服务并处理配置变化func RunNanny(sync config.Sync, opts RunNannyOpts, kubednsServer string) &#123;    //  ...    configChan := sync.Periodic()    for &#123;      select &#123;      // ...      // 观察到config变化      case currentConfig = &lt;-configChan:        if opts.RestartOnChange &#123;          // 直接杀掉dnsmasq进程          nanny.Kill()          nanny = &amp;Nanny&#123;Exec: opts.DnsmasqExec&#125;          // 重新加载配置          nanny.Configure(opts.DnsmasqArgs, currentConfig, kubednsServer)          // 重新启动dnsmasq进程          nanny.Start()        &#125; else &#123;          glog.V(2).Infof(&quot;Not restarting dnsmasq (--restartDnsmasq=false)&quot;)        &#125;        break      &#125;    &#125;&#125;</code></pre><p>让我们看下 sync.Periodic() 这个函数做了些什么</p><pre><code class="go">// dns/pkg/dns/config/sync.go#81func (sync *kubeSync) Periodic() &lt;-chan *Config &#123;    go func() &#123;      // Periodic函数中设置了一个Tick，每10s会去load一下configDir下      // 所有的文件，并对每个文件进行sha256的摘要计算      // 并将这个结果返回。      resultChan := sync.syncSource.Periodic()      for &#123;        syncResult := &lt;-resultChan        // processUpdate函数会比较新的文件的版本和旧的        // 文件的版本，如果不一致会返回changed。        // 值得注意的是有三个文件是需要单独处理的        // federations        // stubDomains        // upstreamNameservers        // 当这三个文件变化是会触发单独的函数（打印日志）        config, changed, err := sync.processUpdate(syncResult, false)        if err != nil &#123;          continue        &#125;        if !changed &#123;          continue        &#125;        sync.channel &lt;- config      &#125;    &#125;()    return sync.channel&#125;</code></pre><p>dnsmasq 中是如何加载配置的呢？</p><pre><code class="go">// dns/pkg/dnsmasq/nanny.go#58// Configure the nanny. This must be called before Start().// 这个函数会配置 dnsmasq，Nanny 每次 Kill 掉 dnsmasq 后，调用 Start() 之前都会调用这个函数// 重新加载配置。func (n *Nanny) Configure(args []string, config *config.Config, kubednsServer string) &#123;    // ...    for domain, serverList := range config.StubDomains &#123;      resolver := &amp;net.Resolver&#123;        PreferGo: true,        Dial: func(ctx context.Context, network, address string) (net.Conn, error) &#123;          d := net.Dialer&#123;&#125;          return d.DialContext(ctx, &quot;udp&quot;, kubednsServer)        &#125;,      &#125;      // 因为 stubDomain 中可以是以 host:port 的形式存在，所以这里还要做一次 上游的 dns 解析      for _, server := range serverList &#123;        if isIP := (net.ParseIP(server) != nil); !isIP &#123;          switch &#123;          // 如果 server 是以 cluster.local（不知道为什么这里是 hardCode 的）结尾的，就会发往 kubednsServer 进行 DNS 解析          // 因为上面已经配置了 d.DialContext(ctx, &quot;udp&quot;, kubednsServer)          case strings.HasSuffix(server, &quot;cluster.local&quot;):            // ...            resolver.LookupIPAddr(context.Background(), server)          default:          // 如果没有以 cluster.local 结尾，就会走外部解析 DNS            // ...            net.LookupIP(server)          &#125;        &#125;      &#125;    &#125;    // ...&#125;</code></pre><h3 id="sidecar"><a href="#sidecar" class="headerlink" title="sidecar"></a>sidecar</h3><p>sidecar 启动后会在内部开启一个协程，并在循环中每默认 5s 向 kubedns 发送一次 dns 解析。并记录解析结果。</p><p>sidecar 提供了两个http的接口 <code>/healthcheck/kubedns</code> 和 <code>/healthcheck/dnsmasq</code> 给 k8s 用作 <code>livenessProbe</code> 的健康检查。每次请求，sidecar 会将上述记录的 DNS 解析结果返回。</p><h3 id="kubedns的优缺点"><a href="#kubedns的优缺点" class="headerlink" title="kubedns的优缺点"></a>kubedns的优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>各个模块都做了很好的解耦，方便开发者上手。</li><li>依赖 dnsmasq ，性能有保障</li></ol><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol><li><p>因为 dnsmasq-nanny 重启 dnsmasq 的方式，先杀后起，方式比较粗暴，有可能导致这段时间内大量的 DNS 请求失败。</p></li><li><p>dnsmasq-nanny 检测文件的方式，可能会导致以下问题：</p><ol><li><p>dnsmasq-nanny 每次遍历目录下的所有文件，然后用 ioutil.ReadFile 读取文件内容。如果目录下文件数量过多，可能出现在遍历的同时文件也在被修改，遍历的速度跟不上修改的速度。<br> 这样可能导致遍历完了，某个配置文件才更新完。那么此时，你读取的一部分文件数据并不是和当前目录下文件数据完全一致，本次会重启 dnsmasq。进而，下次检测，还认为有文件变化，到时候，又重启一次 dnsmasq。这种方式不优雅，但问题不大。</p></li><li><p>文件的检测，直接使用 ioutil.ReadFile 读取文件内容，也存在问题。如果文件变化，和文件读取同时发生，很可能你读取完，文件的更新都没完成，那么你读取的并非一个完整的文件，而是坏的文件，这种文件，dnsmasq-nanny 无法做解析，不过官方代码中有数据校验，解析失败也问题不大，大不了下个周期的时候，再取到完整数据，再解析一次。</p></li></ol></li></ol><h2 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a><a href="https://github.com/coredns/coredns">CoreDNS</a></h2><p>CoreDNS 是一个高速并且十分<strong>灵活</strong>的DNS服务。CoreDNS 允许你通过编写插件的形式去自行处理DNS数据。</p><h3 id="结构图"><a href="#结构图" class="headerlink" title="结构图"></a>结构图</h3><p>        <span class="lazyload-img-span">        <img              data-src="/images/kubeDNS%E5%92%8CcoreDNS/core_dns_structure.png" >        </sapn>      </p><p>CoreDNS 使用<a href="https://github.com/caddyserver/caddy">Caddy</a>作为底层的 Web Server，Caddy 是一个轻量、易用的Web Server，它支持 HTTP、HTTPS、HTTP&#x2F;2、GRPC 等多种连接方式。所有 coreDNS 可以通过四种方式对外直接提供 DNS 服务，分别是 UDP、gRPC、HTTPS 和 TLS</p><p>CoreDNS 的大多数功能都是由插件来实现的，插件和服务本身都使用了 Caddy 提供的一些功能，所以项目本身也不是特别的复杂。</p><h3 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h3><p>CoreDNS 定义了一套插件的接口，只要实现 Handler 接口就能将插件注册到<strong>插件链</strong>中。</p><pre><code class="go">type (    // 只需要为插件实现 ServeDNS 以及 Name 这两个接口并且写一些用于配置的代码就可以将插件集成到 CoreDNS 中    Handler interface &#123;        ServeDNS(context.Context, dns.ResponseWriter, *dns.Msg) (int, error)        Name() string    &#125;)</code></pre><p>现在在 CoreDNS 中已经支持40种左右的插件。</p><h4 id="kubernetes"><a href="#kubernetes" class="headerlink" title="kubernetes"></a>kubernetes</h4><p>该插件可以让 coreDNS 读取到k8s集群内的 endpoint 以及 service 等信息，从而替代 kubeDNS 作为 k8s 集群内的 DNS 解析服务。不仅如此，该插件还支持多种配置如：</p><pre><code class="conf">kubernetes [ZONES...] &#123;    ; 使用该配置可以连接到远程的k8s集群    kubeconfig KUBECONFIG CONTEXT    endpoint URL      tls CERT KEY CACERT    ; 可以设置需要暴露Service的namespace列表    namespaces NAMESPACE...    ; 可以暴露带有特定label的namespace    labels EXPRESSION    ; 是否可以解析10-0-0-1.ns.pod.cluster.local这种domain（为了兼容kube-dns）    pods POD-MODE    endpoint_pod_names    ttl TTL    noendpoints    transfer to ADDRESS...    fallthrough [ZONES...]    ignore empty_service&#125;</code></pre><h4 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h4><p>提供上游解析功能</p><pre><code class="conf">forward FROM TO... &#123;    except IGNORED_NAMES...    ; 强制使用tcp进行域名解析    force_tcp    ; 当请求是tcp时，先尝试一次udp解析，失败了再使用tcp    prefer_udp    expire DURATION    ; upstream的healthcheck失败的最多次数，默认2，超过的话upstream就会被下掉    max_fails INTEGER    tls CERT KEY CA    tls_servername NAME    ; 选择nameserver的策略，random、round_robin、sequential    policy random|round_robin|sequential    health_check DURATION&#125;</code></pre><p>更多的插件可以到 CoreDNS 的<a href="https://coredns.io/plugins/">插件市场</a>查看</p><h3 id="Corefile"><a href="#Corefile" class="headerlink" title="Corefile"></a>Corefile</h3><p>CoreDNS 提供了一种简单易懂的 DSL 语言，它允许你通过 Corefile 来自定义 DNS 服务。</p><pre><code class="conf">coredns.io:5300 &#123;    file db.coredns.io&#125;example.io:53 &#123;    log    errors    file db.example.io&#125;example.net:53 &#123;    file db.example.net&#125;.:53 &#123;    kubernetes    proxy . 8.8.8.8    log    errors    cache&#125;</code></pre><p>通过以上的配置，CoreDNS 会开启两个端口 5300 和 53 ，提供 DNS 解析服务。对于 coredns.io 相关的域名会通过 5300 端口进行解析，其他域名会被解析到 53 端口，不同的域名会应用不同的插件。</p><p>        <span class="lazyload-img-span">        <img              data-src="/images/kubeDNS%E5%92%8CcoreDNS/core_dns_plugin_example.png" >        </sapn>      </p><h3 id="插件原理"><a href="#插件原理" class="headerlink" title="插件原理"></a>插件原理</h3><p>在 CoreDNS 中 <code>Plugin</code> 其实就是一个出入参数都是 <code>Handler</code> 的函数</p><pre><code class="go">// 所谓的插件链其实是一个Middle layer，通过传递链中的下一个Handler，将一个Handler链接到下一个Handler。type  Plugin func(Handler) Handler</code></pre><p>同时 CoreDNS 提供了 <code>NextOrFailure</code> 方法，供每个插件在执行完自身的逻辑之后执行下一个插件</p><pre><code class="go">// NextOrFailure calls next.ServeDNS when next is not nil, otherwise it will return, a ServerFailure and a nil error.func NextOrFailure(name string, next Handler, ctx context.Context, w dns.ResponseWriter, r *dns.Msg) (int, error) &#123; // nolint: golint    if next != nil &#123;      if span := ot.SpanFromContext(ctx); span != nil &#123;        child := span.Tracer().StartSpan(next.Name(), ot.ChildOf(span.Context()))        defer child.Finish()        ctx = ot.ContextWithSpan(ctx, child)      &#125;      return next.ServeDNS(ctx, w, r)    &#125;    return dns.RcodeServerFailure, Error(name, errors.New(&quot;no next plugin found&quot;))&#125;</code></pre><p>如果 next 为 nil，说明插件链已经调用结束，直接返回 <code>no next plugin found</code> 的 error 即可。</p><p>每个 <code>Plugin</code> 也可以调用 <code>(dns.ResponseWriter).WriteMsg(*dns.Msg)</code> 方法来结束整个调用链。</p><h3 id="kubernetes-插件做了什么"><a href="#kubernetes-插件做了什么" class="headerlink" title="kubernetes 插件做了什么"></a>kubernetes 插件做了什么</h3><p>CoreDNS 正是通过 kubernetes 插件实现了解析 k8s 集群内域名的功能。那么我们看下这个插件做了些什么事情。</p><pre><code class="go">// coredns/plugin/kubernetes/setup.go#44func setup(c *caddy.Controller) error &#123;    // 检查了 corefile 中 kubernetes 配置的定义，并配置了一些缺省值    k, err := kubernetesParse(c)    if err != nil &#123;      return plugin.Error(&quot;kubernetes&quot;, err)    &#125;    // 启动了对 pod, service, endpoint 三种资源增、删、改的 watch，并注册了一些回调    // 注意：pod 是否启动 watch 是根据配置文件中 pod 的值来决定的，如果值不是 verified 就不会启动 pod 的 watch    // 这里的 watch 方法观测到变化后，仅仅只改变 dns.modified 这个值，它会将该值设置为当前时间戳    err = k.InitKubeCache()    if err != nil &#123;      return plugin.Error(&quot;kubernetes&quot;, err)    &#125;    // 将插件注册到 Caddy，让 Caddy 启动时能够同时启动该插件    k.RegisterKubeCache(c)    // 注册插件到调用链    dnsserver.GetConfig(c).AddPlugin(func(next plugin.Handler) plugin.Handler &#123;      k.Next = next      return k    &#125;)    return nil&#125;</code></pre><pre><code class="go">// coredns/plugin/kubernetes/controller.go#408// 这三个方法就是 watch 资源时的回调func (dns *dnsControl) Add(obj interface&#123;&#125;)               &#123; dns.detectChanges(nil, obj) &#125;func (dns *dnsControl) Delete(obj interface&#123;&#125;)            &#123; dns.detectChanges(obj, nil) &#125;func (dns *dnsControl) Update(oldObj, newObj interface&#123;&#125;) &#123; dns.detectChanges(oldObj, newObj) &#125;// detectChanges detects changes in objects, and updates the modified timestampfunc (dns *dnsControl) detectChanges(oldObj, newObj interface&#123;&#125;) &#123;    // 判断新老对象的版本    if newObj != nil &amp;&amp; oldObj != nil &amp;&amp; (oldObj.(meta.Object).GetResourceVersion() == newObj.(meta.Object).GetResourceVersion()) &#123;      return    &#125;    obj := newObj    if obj == nil &#123;      obj = oldObj    &#125;    switch ob := obj.(type) &#123;    case *object.Service:      dns.updateModifed()    case *object.Endpoints:      if newObj == nil || oldObj == nil &#123;        dns.updateModifed()        return      &#125;      p := oldObj.(*object.Endpoints)      // endpoint updates can come frequently, make sure it&#39;s a change we care about      if endpointsEquivalent(p, ob) &#123;        return      &#125;      dns.updateModifed()    case *object.Pod:      dns.updateModifed()    default:      log.Warningf(&quot;Updates for %T not supported.&quot;, ob)    &#125;&#125;func (dns *dnsControl) Modified() int64 &#123;    unix := atomic.LoadInt64(&amp;dns.modified)    return unix&#125;// updateModified set dns.modified to the current time.func (dns *dnsControl) updateModifed() &#123;    unix := time.Now().Unix()    atomic.StoreInt64(&amp;dns.modified, unix)&#125;</code></pre><p>上面展示的就是 kubernetes 这个 Plugin Watch 各个资源变化后的回调。可以看到它仅仅只改变 dns.modified 一个值，那么当 Service 发生变化后，kubernetes 插件如何感知，并将它们更新到内存呢。其实并没有或者说并不需要。。。因为这里使用了 <code>client-go</code> 中的 <code>informer</code> 机制，kubernetes 在解析 Service DNS 时会根据直接列出所有 Service（这里其实这么说并不准确，如果查找的是泛域名，那么才会列出所有 Service，如果是正常的 servicename.namespace，那么插件会使用 <code>client-go</code> 的 <code>Indexer</code> 机制，根据索引查找符合的 ServiceList），再进行匹配，直到找到匹配的 Service 再根据它的不同类型，决定返回结果。如果是 ClusterIP 类型，则返回 svc 的 ClusterIP，如果是 Headless 类型，则返回它所有的 Endpoint 的IP，如果是 ExternalName 类型，且 external_name 的值为 CNAME 类型，则返回 external_name 的值。整个操作仍然是在内存中进行的，效率并不会很低。</p><pre><code class="go">// findServices returns the services matching r from the cache.func (k *Kubernetes) findServices(r recordRequest, zone string) (services []msg.Service, err error) &#123;    // 如果 namespace 为 * 或者 为 any，或者该 namespace 在配置文件中没有被 namespace: 这个配置项中配置    // 则返回 NXDOMAIN    if !wildcard(r.namespace) &amp;&amp; !k.namespaceExposed(r.namespace) &#123;      return nil, errNoItems    &#125;    // 如果 lookup 的 service 为空    if r.service == &quot;&quot; &#123;      //  如果 namepace 存在 或者 namespace 是通配符就返回空的 Service 列表      if k.namespaceExposed(r.namespace) || wildcard(r.namespace) &#123;        // NODATA        return nil, nil      &#125;      // 否则返回 NXDOMAIN      return nil, errNoItems    &#125;    err = errNoItems    if wildcard(r.service) &amp;&amp; !wildcard(r.namespace) &#123;      // If namespace exists, err should be nil, so that we return NODATA instead of NXDOMAIN      if k.namespaceExposed(r.namespace) &#123;        err = nil      &#125;    &#125;    var (      endpointsListFunc func() []*object.Endpoints      endpointsList     []*object.Endpoints      serviceList       []*object.Service    )    if wildcard(r.service) || wildcard(r.namespace) &#123;      // 如果 service 或者 namespace 为 * 或者 any，列出当前所有的 Service      serviceList = k.APIConn.ServiceList()      endpointsListFunc = func() []*object.Endpoints &#123; return k.APIConn.EndpointsList() &#125;    &#125; else &#123;      // 根据 service.namespace 获取 index      idx := object.ServiceKey(r.service, r.namespace)      // 通过 client-go 的 indexer 返回 serviceList      serviceList = k.APIConn.SvcIndex(idx)      endpointsListFunc = func() []*object.Endpoints &#123; return k.APIConn.EpIndex(idx) &#125;    &#125;    // 将 zone 转化为 etcd key 的格式    // /c/local/cluster    zonePath := msg.Path(zone, coredns)    for _, svc := range serviceList &#123;      if !(match(r.namespace, svc.Namespace) &amp;&amp; match(r.service, svc.Name)) &#123;        continue      &#125;      // If request namespace is a wildcard, filter results against Corefile namespace list.      // (Namespaces without a wildcard were filtered before the call to this function.)      if wildcard(r.namespace) &amp;&amp; !k.namespaceExposed(svc.Namespace) &#123;        continue      &#125;      // 如果查找的 Service 没有 Endpoint，就返回 NXDOMAIN，除非这个 Service 是 Headless Service 或者 External name      if k.opts.ignoreEmptyService &amp;&amp; svc.ClusterIP != api.ClusterIPNone &amp;&amp; svc.Type != api.ServiceTypeExternalName &#123;        // serve NXDOMAIN if no endpoint is able to answer        podsCount := 0        for _, ep := range endpointsListFunc() &#123;          for _, eps := range ep.Subsets &#123;            podsCount = podsCount + len(eps.Addresses)          &#125;        &#125;        // No Endpoints        if podsCount == 0 &#123;          continue        &#125;      &#125;      // lookup 的 Service 是 headless Service 或者是使用 Endpoint lookup      if svc.ClusterIP == api.ClusterIPNone || r.endpoint != &quot;&quot; &#123;        if endpointsList == nil &#123;          endpointsList = endpointsListFunc()        &#125;        for _, ep := range endpointsList &#123;          if ep.Name != svc.Name || ep.Namespace != svc.Namespace &#123;            continue          &#125;          for _, eps := range ep.Subsets &#123;            for _, addr := range eps.Addresses &#123;              // See comments in parse.go parseRequest about the endpoint handling.              if r.endpoint != &quot;&quot; &#123;                if !match(r.endpoint, endpointHostname(addr, k.endpointNameMode)) &#123;                  continue                &#125;              &#125;              for _, p := range eps.Ports &#123;                if !(match(r.port, p.Name) &amp;&amp; match(r.protocol, string(p.Protocol))) &#123;                  continue                &#125;                s := msg.Service&#123;Host: addr.IP, Port: int(p.Port), TTL: k.ttl&#125;                s.Key = strings.Join([]string&#123;zonePath, Svc, svc.Namespace, svc.Name, endpointHostname(addr, k.endpointNameMode)&#125;, &quot;/&quot;)                err = nil                // 遍历 Endpoints 并将结果添加到返回列表                services = append(services, s)              &#125;            &#125;          &#125;        &#125;        continue      &#125;      // External service      // 如果 svc 是 External Service      if svc.Type == api.ServiceTypeExternalName &#123;        s := msg.Service&#123;Key: strings.Join([]string&#123;zonePath, Svc, svc.Namespace, svc.Name&#125;, &quot;/&quot;), Host: svc.ExternalName, TTL: k.ttl&#125;        // 只有当 External Name 是 CNAME 时，才会添加该 Service 到结果        if t, _ := s.HostType(); t == dns.TypeCNAME &#123;          s.Key = strings.Join([]string&#123;zonePath, Svc, svc.Namespace, svc.Name&#125;, &quot;/&quot;)          services = append(services, s)          err = nil        &#125;        continue      &#125;      // ClusterIP service      // 正常情况，返回的 msg.Service 的 Host 为 ClusterIP      for _, p := range svc.Ports &#123;        if !(match(r.port, p.Name) &amp;&amp; match(r.protocol, string(p.Protocol))) &#123;          continue        &#125;        err = nil        s := msg.Service&#123;Host: svc.ClusterIP, Port: int(p.Port), TTL: k.ttl&#125;        s.Key = strings.Join([]string&#123;zonePath, Svc, svc.Namespace, svc.Name&#125;, &quot;/&quot;)        services = append(services, s)      &#125;    &#125;    return services, err&#125;</code></pre><h3 id="coredns的优缺点"><a href="#coredns的优缺点" class="headerlink" title="coredns的优缺点"></a>coredns的优缺点</h3><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ol><li>非常灵活的配置，可以根据不同的需求给不同的域名配置不同的插件</li><li>k8s 1.9 版本后的默认的 dns 解析</li></ol><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ol><li>缓存的效率不如 dnsmasq，对集群内部域名解析的速度不如 kube-dns （10% 左右）</li></ol><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>在 CoreDNS 的官网中已有详细的性能测试报告，<a href="https://coredns.io/2018/11/27/cluster-dns-coredns-vs-kube-dns/">地址</a></p><ul><li>对于内部域名解析 KubeDNS 要优于 CoreDNS 大约 10%，可能是因为 dnsmasq 对于缓存的优化会比 CoreDNS 要好</li><li>对于外部域名 CoreDNS 要比 KubeDNS 好 3 倍。但这个值大家看看就好，因为 kube-dns 不会缓存 Negative cache。但即使 kubeDNS 使用了 Negative cache，表现仍然也差不多</li><li>CoreDNS 的内存占用情况会优于 KubeDNS</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本篇文章主要围绕两种 DNS Server 的实现方式展开，会比较两种 Server 的优缺点。&lt;/p&gt;
&lt;p&gt;在说两个 Service 之前，我们先来了解一下在k8s中域名是如何被解析的。&lt;/p&gt;
&lt;p&gt;我们都知道，在 k8s 中，一个 Pod 如果要访问同 Namesp</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="https://domc.me/tags/kubernetes/"/>
    
    <category term="dns" scheme="https://domc.me/tags/dns/"/>
    
  </entry>
  
  <entry>
    <title>golang sync.Map源码解析</title>
    <link href="https://domc.me/2019/03/01/golang_sync_map/"/>
    <id>https://domc.me/2019/03/01/golang_sync_map/</id>
    <published>2019-03-01T14:04:44.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>golang的内建类型map是非线程安全的，当我们使用并发操作去写map时会引发panic</p><pre><code class="go">package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() &#123; var m = make(map[string]string,100) var wait  sync.WaitGroup for i := 0 ;i &lt; 100 ; i ++ &#123;  wait.Add(1)  go func() &#123;   m[fmt.Sprintf(&quot;%d&quot;,i)] = &quot;test&quot;   wait.Done()  &#125;() &#125; wait.Wait()&#125;</code></pre><pre><code class="plaintext">fatal error: concurrent map writesfatal error: concurrent map writesgoroutine 11 [running]:runtime.throw(0x10c6370, 0x15) /usr/local/go/src/runtime/panic.go:608 +0x72 fp=0xc00002cf08 sp=0xc00002ced8 pc=0x1026e52runtime.mapassign_faststr(0x10a9860, 0xc00006e120, 0xc0000a0010, 0x2, 0x1) /usr/local/go/src/runtime/map_faststr.go:199 +0x3da fp=0xc00002cf70 sp=0xc00002cf08 pc=0x100fadamain.main.func1(0xc00006e120, 0xc000016078, 0xc000016080)...</code></pre><p>因此当我们需要并发修改map时，第一种方法是在每次修改map值时使用互斥锁，第二种则是直接使用golang内置的线程安全的map，也就是sync.Map。</p><p>本文则从sync.Map的源码层面介绍，sync.Map是如何实现线程安全的。</p><p>首先我们看下sync.Map的数据结构</p><pre><code class="go">// Map 类似于map[interface&#123;&#125;]interface&#123;&#125;，但是对于多个goroutine并发使用是安全的，不需要额外的lock// // 该类型是专门为并发操作提供的，你应该使用该类型来替代需要对原生map加锁的场景，// 以此获得更好地安全性，并使得操作更加简便//// Map针对了两个常见的场景进行了优化（1）对一个key只写入以此，但是读很多次。（2）在并发场景下，goroutine不会// 同时去操作一个key（也就是不会发生竞态）。// 在这两种情况下使用Map会比使用RWMutex加锁的map效率更高//// 你可以直接var x sync.Map就能使用，不需要去执行New...,Map不能被copytype Map struct &#123; mu Mutex    // read中保存了Map中部分内容，这些内容是只读的，所以是线程安全的    // 其中保存的数据类型是readOnly read atomic.Value // readOnly    // 所有对dirty的操作都是需要加锁的    // 如果dirty为空，下一次写操作会复制read中没被删除的数据到dirty dirty map[interface&#123;&#125;]*entry    // 当从Map中读取entry时，会先去read中读取，如果read中读不到则到dirty中读取，这是    // 会将该值+1，当该值到达一定大小后就会将read中所有值更新为dirty中保存的值 misses int&#125;</code></pre><p>从Map的数据结构看，就不难发现Map是如何保证线程安全的</p><p>当往Map中添加新的值时，不会往read中插入数据，而是直接将数据保存在dirty中。当需要从Map中读取数据时，<br>会先从read中读取数据，如果读到直接返回，如果读不到那么就会从dirty中读取，并更新misses的值，当misses值到达一定数值之后，<br>就会将dirty的值赋给read。(所有对dirty的操作都是加锁的，这就保证了这个类是线程安全的，同时因为read-only的存在，提升了并发读取的效率)</p><pre><code class="go">// readOnly is an immutable struct stored atomically in the Map.read field.type readOnly struct &#123; m       map[interface&#123;&#125;]*entry amended bool // 如果dirty中存在一些m中没有的key，该值则为true&#125;</code></pre><pre><code class="go">// An entry is a slot in the map corresponding to a particular key.type entry struct &#123; // p points to the interface&#123;&#125; value stored for the entry. //    // p有三种情况    // p == nil: entry已经被删除，且m.dirty为nil    // expunged: entry已经被删除，但是m.dirty不是nil，并且这个entry不在m.dirty中    // 其他: entry是个正常值 p unsafe.Pointer // *interface&#123;&#125;&#125;</code></pre><h2 id="Load"><a href="#Load" class="headerlink" title="Load"></a>Load</h2><p>Load方法用于根据Key读取Map中的值</p><pre><code class="go">// Load 根据所给的key返回Map中的值，如果不存在返回nil//// ok 表示Map中是否包含keyfunc (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123;    // 直接从readonly中读取，如果读到直接返回，因为是readonly所以不用加锁 read, _ := m.read.Load().(readOnly) e, ok := read.m[key]    // 如果readonly中没有值，并且dirty中存在read中不存在的值时 if !ok &amp;&amp; read.amended &#123;  m.mu.Lock()        // 加锁，双检查  read, _ = m.read.Load().(readOnly)  e, ok = read.m[key]        // 如果read中仍然不存在该key，且dirty中有read中不存在的值  if !ok &amp;&amp; read.amended &#123;            // 从dirty中检查是否有该key   e, ok = m.dirty[key]            // 不管dirty中是否有改key，都将misses+1            // 当misses到达一定值之后，m.dirty会被提升为read   m.missLocked()  &#125;  m.mu.Unlock() &#125; if !ok &#123;  return nil, false &#125; return e.load()&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;golang的内建类型map是非线程安全的，当我们使用并发操作去写map时会引发panic&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;go&quot;&gt;package main

import (
 &amp;quot;fmt&amp;quot;
 &amp;quot;sync&amp;quot;
)

func</summary>
      
    
    
    
    
    <category term="golang" scheme="https://domc.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>如何用Redis实现一个指定时间的限速器</title>
    <link href="https://domc.me/2019/02/28/redis_limit/"/>
    <id>https://domc.me/2019/02/28/redis_limit/</id>
    <published>2019-02-28T14:53:58.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>使用Redis的<code>Incr</code>可以很容易的实现一个限速器</p><p>在redis的官方文档中也有详细的示例</p><pre><code class="c">FUNCTION LIMIT_API_CALL(ip)ts = CURRENT_UNIX_TIME()keyname = ip+&quot;:&quot;+tscurrent = GET(keyname)IF current != NULL AND current &gt; 10 THEN    ERROR &quot;too many requests per second&quot;ENDIF current == NULL THEN    MULTI        INCR(keyname, 1)        EXPIRE(keyname, 1)    EXECELSE    INCR(keyname, 1)ENDPERFORM_API_CALL()</code></pre><h3 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h3><p>现有一个服务，1分钟内只能接受一个用户不超过10次的请求，这时我们可以将用户的ip地址设置为key，用户每次时程序去redis中获取该key的值，如果大于等于10则返回错误，否则给key对应的value+1即可，如果value为0，那么再将该key设置1分钟的过期时间。</p><hr><p><strong>但是现在有一个需求，我们可以在一个指定的时间内给用户推送一条消息，但是要求用户每分钟内只能接受1条消息，每小时内接受的消息不超过5条，一天内接受的消息不超过10条。</strong></p><p>比如，我现在向这个接口提交了一条数据，要求在2019-11-11 11:11:11时向一个用户发送一条数据，那么当我再提交一条数据，要求在2019-11-11 11:11:12是向同样的用户发送一条数据，那么接口就会返回错误。</p><p>此时，仅使用<code>Incr</code>是无法满足该需求的。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>使用<code>set</code>或者<code>zset</code>将用户的<code>ip+发送日期</code>作为key,发送时间转换为当天的秒数作为value，插入到<code>set</code>或者<code>zset</code>中，每次向用户提交信息时，可以获取到<code>set</code>中的所有发送时间，然后再一一比对，如果不满足条件就返回错误。</p><p>以下是伪代码实现</p><pre><code>function RateLimit(ip,sendtime)&#123;  // 根据发送时间得到发送的日期  sendDate = getSendDate(sendtime)  // 获取发送时间和当天0点之间的秒数差值  second = getSendSecond(sendtime)  // 列出该发送日期中的所有发送时间  secondList = listSeconds(ip + sendDate)  // 检验成功  if check(second,secondList) &#123;    // 将发送时间添加到缓存    return addToList(ip + sendDate,second)  &#125;  return false&#125;</code></pre><p>以下是golang的实现</p><pre><code class="go">func RateLimit(ctx context.Context, ip string, sendTime time.Time) error &#123;    sendTime = sendTime.UTC() if sendTime.Before(time.Now()) &#123;  return nil &#125; zeroStr := sendTime.Format(&quot;2006-01-02&quot;) key := ip + zeroStr zero, _ := time.Parse(&quot;2006-01-02&quot;, zeroStr)    // 获取发送时间距当天时间的秒数 second := int(sendTime.Sub(zero).Seconds()) ress, err := cache.SMembers(ctx, key) if err != nil &#123;  return err &#125;    // 处理返回参数，将[]string转换为[]int    var sends []int for _, v := range ress &#123;  if vv, err := strconv.Atoi(v); err == nil &#123;   sends = append(sends, vv)  &#125; &#125; if len(sends) &gt;= 10 &#123;  return errors.ErrMsg1DayLimit &#125; var expire bool if len(sends) == 0 &#123;  expire = true &#125; var hourCount int for _, s := range sends &#123;  if math.Abs(float64(second-s)) &lt; 60 &#123;   return errors.ErrMsg1MinuteLimit  &#125;  if math.Abs(float64(second-s)) &lt; 3600 &#123;   hourCount++  &#125; &#125; if hourCount &gt; 5 &#123;  return errors.ErrMsg1HourLimit &#125;    // 异步更新    go func() &#123;        // 开启事务  t := cache.Pipeline()  defer t.Close()  t.SAdd(context.Background(), key, []byte(strconv.Itoa(second)))  if expire &#123;   ttl := zero.Add(time.Hour*24).Sub(time.Now()).Seconds()   if ttl &lt;= 0 &#123;    t.Rollback()    return   &#125;   t.Expire( key, int64(ttl))  &#125;  t.Commit() &#125;()    return nil&#125;</code></pre><p>以上就可以实现一个指定时间的限速器。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;使用Redis的&lt;code&gt;Incr&lt;/code&gt;可以很容易的实现一个限速器&lt;/p&gt;
&lt;p&gt;在redis的官方文档中也有详细的示例&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;c&quot;&gt;FUNCTION LIMIT_API_CALL(ip)
ts = CURRENT_UNIX_</summary>
      
    
    
    
    
    <category term="redis" scheme="https://domc.me/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>解决golang使用elastic连接elasticsearch时自动转换连接地址</title>
    <link href="https://domc.me/2019/02/28/golang_elastic_url/"/>
    <id>https://domc.me/2019/02/28/golang_elastic_url/</id>
    <published>2019-02-28T14:53:27.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>使用<a href="https://github.com/olivere/elastic">olivere&#x2F;<em>elastic</em></a>连接elasticsearch时，发现连接地址明明输入的时候是公网地址，但是连接时会自动转换成内网地址或者docker中的ip地址，导致服务连接不上。</p><pre><code>// 自动转换成docker中的ip导致无法连接服务time=&quot;2019-02-15T20:04:26+08:00&quot; level=error msg=&quot;Head http://172.17.0.2:9200: context deadline exceeded&quot;</code></pre><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><pre><code class="golang">client, _ := elastic.NewClient(  // ...  // 将sniff设置为false后，便不会自动转换地址   elastic.SetSniff(false),)</code></pre><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><pre><code class="golang">// sniff 会请求http://ip:port/_nodes/http，将其返回的url list作为新的url list。// 如果snifferEnabled被设置为false，那么则不启动该功能。func (c *Client) sniff(parentCtx context.Context, timeout time.Duration) error &#123; c.mu.RLock() if !c.snifferEnabled &#123;  c.mu.RUnlock()  return nil &#125; // Use all available URLs provided to sniff the cluster. var urls []string urlsMap := make(map[string]bool) // Add all URLs provided on startup for _, url := range c.urls &#123;  urlsMap[url] = true  urls = append(urls, url) &#125; c.mu.RUnlock() // Add all URLs found by sniffing c.connsMu.RLock() for _, conn := range c.conns &#123;  if !conn.IsDead() &#123;   url := conn.URL()   if _, found := urlsMap[url]; !found &#123;    urls = append(urls, url)   &#125;  &#125; &#125; c.connsMu.RUnlock() if len(urls) == 0 &#123;  return errors.Wrap(ErrNoClient, &quot;no URLs found&quot;) &#125; // Start sniffing on all found URLs ch := make(chan []*conn, len(urls)) ctx, cancel := context.WithTimeout(parentCtx, timeout) defer cancel() for _, url := range urls &#123;        // sniffNode 方法使用http，请求了对应的url，将结果封装后返回  go func(url string) &#123; ch &lt;- c.sniffNode(ctx, url) &#125;(url) &#125; // Wait for the results to come back, or the process times out. for &#123;  select &#123;  case conns := &lt;-ch:   if len(conns) &gt; 0 &#123;    c.updateConns(conns)    return nil   &#125;  case &lt;-ctx.Done():   if err := ctx.Err(); err != nil &#123;    switch &#123;    case IsContextErr(err):     return err    &#125;    return errors.Wrapf(ErrNoClient, &quot;sniff timeout: %v&quot;, err)   &#125;   // We get here if no cluster responds in time   return errors.Wrap(ErrNoClient, &quot;sniff timeout&quot;)  &#125; &#125;&#125;</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;使用&lt;a href=&quot;https://github.com/olivere/elastic&quot;&gt;olivere&amp;#x2F;&lt;em&gt;elastic&lt;/em&gt;&lt;/a&gt;连接elasticsearch时，发现连接地址明明输入的时候是公网地址，但是连接时会自动转换成内网地址或者dock</summary>
      
    
    
    
    
    <category term="golang" scheme="https://domc.me/tags/golang/"/>
    
    <category term="elasticsearch" scheme="https://domc.me/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>github 移除敏感文件提交记录</title>
    <link href="https://domc.me/2019/02/28/github_sensitive/"/>
    <id>https://domc.me/2019/02/28/github_sensitive/</id>
    <published>2019-02-28T14:52:51.000Z</published>
    <updated>2025-04-11T02:56:57.522Z</updated>
    
    <content type="html"><![CDATA[<p>在往github提交代码时，有可能会将一些私密信息提交到仓库，即使后续删除该文件，该文件的内容依然可以在github的提交记录中被找到。</p><p>因此如果需要数据从history中删除，可以使用<code>git filter-branch</code>命令或BFG Repo-Cleaner开源工具。</p><p>本文主要介绍如何使用<code>git filter-branch</code>命令清除history中指定文件的内容。</p><p>值得注意的是，如果我们清除了history中的记录，后续我们将无法使用其他命令查看该文件的提交记录和变化。</p><p>下面开始使用git filter-branch清除文件提交记录</p><p>1、进入项目根路径</p><pre><code class="bash">cd YOUR-REPOSITORY</code></pre><p>2、执行<code>git filter-branch</code></p><p><code>注意执行该命令后相应的文件也会被删除，如果不希望文件被删除可以先做好备份，后续再恢复</code></p><pre><code class="bash">git filter-branch --force --index-filter \&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39; \--prune-empty --tag-name-filter cat -- --all</code></pre><p>将<code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code>替换成文件相对于项目的路径</p><p>3、将文件添加到<code>.gitignore</code>防止文件再次被提交</p><pre><code class="bash">echo &quot;YOUR-FILE-WITH-SENSITIVE-DATA&quot; &gt;&gt; .gitignoregit add .gitignoregit commit -m &quot;Add YOUR-FILE-WITH-SENSITIVE-DATA to .gitignore&quot;</code></pre><p>4、如果你确保不会<code>push</code>不会发生冲突，你可以使用</p><pre><code class="bash">git push origin --force --all</code></pre><p>来更新你的所有分支</p><p>5、如果需要从标记的版本删除敏感信息你可以使用</p><pre><code class="bash">git push origin --force --tags</code></pre><p>至此即可将敏感文件移除</p><p><a href="https://help.github.com/articles/removing-sensitive-data-from-a-repository/">官方指导</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在往github提交代码时，有可能会将一些私密信息提交到仓库，即使后续删除该文件，该文件的内容依然可以在github的提交记录中被找到。&lt;/p&gt;
&lt;p&gt;因此如果需要数据从history中删除，可以使用&lt;code&gt;git filter-branch&lt;/code&gt;命令或BFG </summary>
      
    
    
    
    
    <category term="git" scheme="https://domc.me/tags/git/"/>
    
    <category term="github" scheme="https://domc.me/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>[Golang] http.Request复用</title>
    <link href="https://domc.me/2019/02/21/golang_http_request/"/>
    <id>https://domc.me/2019/02/21/golang_http_request/</id>
    <published>2019-02-21T14:49:45.000Z</published>
    <updated>2025-04-11T02:56:57.521Z</updated>
    
    <content type="html"><![CDATA[<h3 id="复用针对除了-Get-以外的请求"><a href="#复用针对除了-Get-以外的请求" class="headerlink" title="复用针对除了 Get 以外的请求"></a>复用针对除了 Get 以外的请求</h3><pre><code class="go">package mainimport ( &quot;net/http&quot; &quot;strings&quot; )func main()&#123;     reader := strings.NewReader(&quot;hello&quot;)     req,_ := http.NewRequest(&quot;POST&quot;,&quot;http://www.abc.com&quot;,reader)     client := http.Client&#123;&#125;     client.Do(req) // 第一次会请求成功     client.Do(req) // 请求失败&#125;</code></pre><p>第二次请求会出错</p><p><code>http: ContentLength=5 with Body length 0</code></p><p>原因是第一次请求后 req.Body 已经读取到结束位置，所以第二次请求时无法再读取 body，<br>解决方法：重新定义一个 ReadCloser 的实现类替换 req.Body</p><pre><code class="go">package readerimport ( &quot;io&quot; &quot;net/http&quot; &quot;strings&quot; &quot;sync/atomic&quot;)type Repeat struct&#123; reader io.ReaderAt offset int64&#125;// Read 重写读方法，使每次读取request.Body时能从指定位置读取func (p *Repeat) Read(val []byte) (n int, err error) &#123; n, err = p.reader.ReadAt(val, p.offset) atomic.AddInt64(&amp;p.offset, int64(n)) return&#125;// Reset 重置偏移量func (p *Repeat) Reset()&#123;        atomic.StoreInt64(&amp;p.offset,0)&#125;func (p *Repeat) Close() error &#123;    // 因为req.Body实现了readcloser接口，所以要实现close方法    // 但是repeat中的值有可能是只读的，所以这里只是尝试关闭一下。 if p.reader != nil &#123;      if rc, ok := p.reader.(io.Closer); ok &#123;       return rc.Close()      &#125;     &#125; return nil&#125;func doPost()  &#123;    client := http.Client&#123;&#125;    reader := strings.NewReader(&quot;hello&quot;)    req , _ := http.NewRequest(&quot;POST&quot;,&quot;http://www.abc.com&quot;,reader)    req.Body = &amp;Repeat&#123;reader:reader,offset:0&#125;    client.Do(req)    // 将偏移量重置为0    req.Body.(*Repeat).Reset()    client.Do(req)&#125;</code></pre><p>这样就不会报错了，因为也重写了 Close()方法，所以同时也解决了 request 重用时，req.Body 自动关闭的问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;复用针对除了-Get-以外的请求&quot;&gt;&lt;a href=&quot;#复用针对除了-Get-以外的请求&quot; class=&quot;headerlink&quot; title=&quot;复用针对除了 Get 以外的请求&quot;&gt;&lt;/a&gt;复用针对除了 Get 以外的请求&lt;/h3&gt;&lt;pre&gt;&lt;code class=</summary>
      
    
    
    
    
    <category term="golang" scheme="https://domc.me/tags/golang/"/>
    
    <category term="http" scheme="https://domc.me/tags/http/"/>
    
  </entry>
  
</feed>
